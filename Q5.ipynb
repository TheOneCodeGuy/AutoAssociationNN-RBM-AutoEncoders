{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Q5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw9tMESAZD44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN9XtcSDLQJs",
        "colab_type": "text"
      },
      "source": [
        "## Group 3:\n",
        "Classes: 3, 1, 4, 6, 8\n",
        "    \n",
        "    Open Country - 3\n",
        "    Tall Building - 1\n",
        "    Mountain - 4\n",
        "    Highway - 6\n",
        "    Coast - 8\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC9BSjni-wIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZLxugJ7LQJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from io import StringIO\n",
        "import pdb\n",
        "from math import sqrt, log\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmaPR3JkLQJx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d440cfc0-97a3-4f76-e798-38ed45c915a4"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t_ogzkSLQJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DatasetClass(Dataset):\n",
        "    \n",
        "    def __init__(self, folder, filename, label_dict):\n",
        "        \n",
        "        self.data = []\n",
        "        self.filename = filename\n",
        "        tar = tarfile.open(folder + '/' + filename)\n",
        "        for file in tar.getmembers():\n",
        "            f = tar.extractfile(file)\n",
        "            if f != None:\n",
        "                content = pd.read_csv(StringIO(f.read().decode()), sep=' ', header=None).values.ravel()\n",
        "                self.data.append(content)\n",
        "            \n",
        "        self.y = torch.tensor(label_dict[self.filename[:-7]], dtype=torch.long)\n",
        "    \n",
        "    def __getitem__(self, idx):     \n",
        "        \n",
        "        return torch.tensor(self.data[idx], dtype=torch.float), self.y\n",
        "      \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.data)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5kQMX5vLQJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_loader(directory, label_dict, train_fraction=0.8, num_workers=2, bs_fraction = 0.2):\n",
        "\n",
        "    all_files = list(filter(lambda x: x.endswith('.tar.gz'), os.listdir(directory)))\n",
        "    files = [file for file in all_files if file[:-7] in label_dict.keys()]\n",
        "    \n",
        "    datasets = list(map(lambda x : DatasetClass(directory, x, label_dict), files))\n",
        "    dataset = ConcatDataset(datasets)\n",
        "    N = dataset.cumulative_sizes[-1]\n",
        "    \n",
        "    train_size = int(N*train_fraction)\n",
        "    test_size = N - train_size\n",
        "\n",
        "    train_data, test_data = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    trainloader = DataLoader(train_data, batch_size=int(bs_fraction*N), shuffle=True, num_workers=num_workers)\n",
        "    testloader = DataLoader(test_data, batch_size=int(bs_fraction*N), shuffle=True, num_workers=num_workers)\n",
        "    \n",
        "    return trainloader, testloader"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afya0RZFLQJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GBRBM:\n",
        "    \n",
        "    def __init__(self, visible_nodes, h_len, lr_W=0.1, lr_bias=0.001):  \n",
        "        # set lower lr for bias than for the weights\n",
        "        self.N = visible_nodes.shape[0]\n",
        "        v_len = visible_nodes.shape[1]\n",
        "        self.V = visible_nodes.to(device)\n",
        "        self.sigma2 = torch.var(self.V, 0)[0].to(device)  \n",
        "        self.lr_W = lr_W\n",
        "        self.lr_bias = lr_bias\n",
        "        \n",
        "        # Initialisation done based on the methods mentioned in the paper\n",
        "        self.W = torch.empty(v_len, h_len).uniform_(-sqrt(6/(v_len+h_len)), sqrt(6/(v_len+h_len))).to(device)\n",
        "        self.b = torch.mean(visible_nodes, axis=0).view(1,-1).to(device)\n",
        "        self.c = torch.tensor([((torch.norm(self.b + self.W[:, i])**2 - torch.norm(self.b)**2)/(2*self.sigma2) +  log(0.01)).item() for i in range(h_len)]).view(1,-1).to(device)\n",
        "\n",
        "    def get_h(self, v):\n",
        "        \n",
        "        a = torch.mm((v/self.sigma2).view(1,-1), self.W) + self.c\n",
        "        f = torch.nn.Sigmoid()\n",
        "        p_h_v = f(a)\n",
        "        return p_h_v, torch.bernoulli(p_h_v)\n",
        "    \n",
        "    def get_v(self, h):\n",
        "        a = torch.mm(h.view(1,-1), self.W.T) + self.b # mean of normal dist\n",
        "        if (torch.isnan(a)).any().item():\n",
        "            pdb.set_trace()\n",
        "        else:\n",
        "            pass\n",
        "        v_h = torch.normal(mean=a, std=torch.sqrt(self.sigma2)).to(device)\n",
        "        return v_h\n",
        "    \n",
        "    def params_update(self, p_h_v0, p_h_vk, v0, vk):\n",
        "        self.W += self.lr_W*(torch.mm((v0/self.sigma2).view(-1,1), p_h_v0) - torch.mm((vk/self.sigma2).view(-1,1), p_h_vk))/self.N\n",
        "        self.b += self.lr_bias*(v0 - vk)/self.N\n",
        "        self.c += self.lr_bias*(p_h_v0 - p_h_vk)/self.N\n",
        "\n",
        "        \n",
        "    def one_epoch(self, k):\n",
        "        for v0 in self.V:\n",
        "            v_t = v0\n",
        "            for t in range(k):  \n",
        "                p_h_vt, h_t = self.get_h(v_t)\n",
        "                if t==0:\n",
        "                    p_h_v0 = p_h_vt                    \n",
        "                v_t1 = self.get_v(h_t)\n",
        "                v_t = v_t1\n",
        "\n",
        "            try:\n",
        "                V_k = torch.cat((V_k, v_t.view(1,-1)), dim=0)\n",
        "                H_k = torch.cat((H_k, h_t.view(1,-1)), dim=0)\n",
        "            except:\n",
        "                V_k = v_t.view(1,-1)\n",
        "                H_k = h_t.view(1,-1)\n",
        "            self.params_update(p_h_v0, p_h_vt, v0, v_t)\n",
        "        return V_k, H_k\n",
        "        \n",
        "    def train(self, k):\n",
        "        ep = 0\n",
        "        error_old = np.inf\n",
        "        max_ep = 100\n",
        "        while True: #ep<=max_ep:\n",
        "            ep += 1\n",
        "            ## Check if error should be SSE?\n",
        "            V_k, H_k = self.one_epoch(k)\n",
        "\n",
        "            error_new = torch.sum((V_k - self.V)**2) \n",
        "            error_new = error_new/V_k.shape[0]\n",
        "            print('Epoch: {0}, Error: {1}, Error diff :{2}'.format(ep, error_new, abs((error_old-error_new)/error_new)))\n",
        "            \n",
        "            if abs(error_new - error_old)/error_new <= 1e-3:\n",
        "                print('Converged!')\n",
        "                break\n",
        "            error_old = error_new\n",
        "\n",
        "        self.V_train = V_k\n",
        "        self.H_train = H_k      \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC96PFW5LQJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BBRBM:\n",
        "    \n",
        "    def __init__(self, visible_nodes, h_len, lr_W=0.01, lr_bias=0.001):\n",
        "        \n",
        "        # set lower lr for bias than for the weights\n",
        "        \n",
        "        self.N = visible_nodes.shape[0]        \n",
        "        v_len = visible_nodes.shape[1]\n",
        "        self.W = torch.randn(v_len, h_len).to(device)\n",
        "        self.b = torch.randn(1, v_len).to(device)\n",
        "        self.c = torch.randn(1, h_len).to(device)\n",
        "        self.V = visible_nodes.to(device)\n",
        "        self.lr_W = lr_W\n",
        "        self.lr_bias = lr_bias        \n",
        "        \n",
        "    def get_h(self, v):\n",
        "        \n",
        "        a = torch.mm(v.view(1,-1), self.W) + self.c\n",
        "        f = torch.nn.Sigmoid()\n",
        "        p_h_v = f(a)\n",
        "        return p_h_v, torch.bernoulli(p_h_v)\n",
        "    \n",
        "    def get_v(self, h):\n",
        "        a = torch.mm(h.view(1,-1), self.W.T) + self.b\n",
        "        f = torch.nn.Sigmoid()\n",
        "        p_v_h = f(a)\n",
        "        return p_v_h, torch.bernoulli(p_v_h)\n",
        "    \n",
        "    def params_update(self, p_h_v0, p_h_vk, v0, vk):\n",
        "        self.W += self.lr_W*(torch.mm(v0.view(-1,1), p_h_v0) - torch.mm(vk.view(-1,1), p_h_vk))/self.N\n",
        "        self.b += self.lr_bias*(v0 - vk)/self.N\n",
        "        self.c += self.lr_bias*(p_h_v0 - p_h_vk)/self.N\n",
        "        \n",
        "    def one_epoch(self, k):\n",
        "\n",
        "        for v0 in self.V:\n",
        "            v_t = v0\n",
        "            for t in range(k):  \n",
        "                p_h_vt, h_t = self.get_h(v_t)\n",
        "                if t==0:\n",
        "                    p_h_v0 = p_h_vt                    \n",
        "                p_v_ht, v_t1 = self.get_v(h_t)\n",
        "                v_t = v_t1\n",
        "\n",
        "            try:\n",
        "                V_k = torch.cat((V_k, v_t.view(1,-1)), dim=0)\n",
        "                H_k = torch.cat((H_k, h_t.view(1,-1)), dim=0)\n",
        "            except:\n",
        "                V_k = v_t.view(1,-1)\n",
        "                H_k = h_t.view(1,-1)\n",
        "\n",
        "            self.params_update(p_h_v0, p_h_vt, v0, v_t)\n",
        "\n",
        "        return V_k, H_k\n",
        "        \n",
        "    def train(self, k):\n",
        "        ep = 0\n",
        "        error_old = np.inf\n",
        "\n",
        "        max_ep = 100\n",
        "        while True: #ep <= max_ep:\n",
        "            ep += 1\n",
        "            ## Check if error should be SSE?\n",
        "            V_k, H_k = self.one_epoch(k)\n",
        "            error_new = torch.sum((V_k - self.V)**2) \n",
        "            error_new = error_new/V_k.shape[0]\n",
        "            print('Epoch: {0}, Error: {1}, Error diff :{2}'.format(ep, error_new, abs((error_old-error_new)/error_new)))\n",
        "            \n",
        "            if abs(error_new - error_old)/error_new <= 1e-3:\n",
        "                print('Converged!')               \n",
        "                break\n",
        "            error_old = error_new\n",
        "        self.V_train = V_k\n",
        "        self.H_train = H_k "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0gjstgMLQKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FinalNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
        "        super(FinalNet, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "        self.out = nn.Linear(hidden_sizes[2], num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            y_score = self.forward(X)\n",
        "            y_pred = torch.argmax(y_score, axis=1)\n",
        "            \n",
        "        return y_pred   "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPghIPVKLQKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Gaussian_stacked_RBM(v, n_stacks, h_layers_len, learning_rates, k_list):\n",
        "    '''\n",
        "    Parameters:\n",
        "    ------------\n",
        "    v              - Input to RBM (visible nodes). Must be continuous valued, whitened.\n",
        "    n_stacks       - No. of RMBs to be stacked\n",
        "    h_layers_len   - List of no. of nodes in the hidden layer of each RMB\n",
        "    learning_rates - List of list of learning rates (for Weights, bias) for each RBM\n",
        "    k_list         - List of k values for each RBM\n",
        "     '''\n",
        "    \n",
        "    weights = []\n",
        "    biases = []\n",
        "    \n",
        "    print('------Gaussian Binary RBM------')\n",
        "    gaussain = GBRBM(v_whitened.to(device), h_layers_len[0], learning_rates[0][0], learning_rates[0][1])\n",
        "    gaussain.train(k_list[0])\n",
        "    weights.append(gaussain.W)\n",
        "    biases.append(gaussain.c)\n",
        "    v_new = gaussain.H_train\n",
        "    \n",
        "    for i in range(1, n_stacks):\n",
        "        print('------Binary Binary RBM {0}------'.format(i))\n",
        "        binary = BBRBM(v_new.to(device), h_layers_len[i], learning_rates[i][0], learning_rates[i][1])\n",
        "        binary.train(k_list[i])\n",
        "        weights.append(binary.W)\n",
        "        biases.append(binary.c)\n",
        "        v_new = binary.H_train\n",
        "        \n",
        "    return weights, biases"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvVXx3E8LQKF",
        "colab_type": "text"
      },
      "source": [
        "1. Finding $\\sigma^{2}$ for visible nodes\n",
        "2. Whitening of data for GBRMB?\n",
        "3. Choice of initial conditions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95qP365qLQKG",
        "colab_type": "text"
      },
      "source": [
        "### Data pre-processing - Whitening the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ofj9NA6Xz__",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "07aab7d0-2cda-4127-b78c-19c0acf9b068"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49DYjAOeLQKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# label_dict = {\n",
        "#     'tallbuilding': 1,\n",
        "#     'mountain': 4,\n",
        "#     'highway': 6,\n",
        "#     'coast': 8,\n",
        "#     'opencountry' : 3    \n",
        "# }\n",
        "\n",
        "\n",
        "label_dict = {\n",
        "    'tallbuilding': 0,\n",
        "    'mountain': 1,\n",
        "    'highway': 2,\n",
        "    'coast': 3, \n",
        "    'opencountry': 4}\n",
        "\n",
        "trainloader, testloader = train_test_loader('/content/drive/My Drive/Data_Set_1(Colored_Images)', label_dict, train_fraction=0.8, num_workers=0, bs_fraction = 1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4Bh2WlcLQKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v = list(trainloader)[0][0]\n",
        "v_centered = v - torch.stack([torch.mean(v, 0)]*v.shape[0], dim=0)\n",
        "cov = torch.mm(v_centered.T, v_centered)/v_centered.shape[0]\n",
        "U, S, V = torch.svd(cov)\n",
        "v_whitened = torch.mm(v_centered, U)/torch.sqrt(S)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk5pvn_KXjAF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b0704bb-2dbf-4b5e-8a93-dafeb53ea82c"
      },
      "source": [
        "v.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1408, 828])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT3cfgcZGt58",
        "colab_type": "text"
      },
      "source": [
        "PCA:\n",
        "FinalNet(reduced_dimension, [150, 75, 50], 5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_olJ81QeZGHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gaussian = GBRBM(v_whitened.to(device), 150, 0.001, 0.01)\n",
        "# gaussian.train(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Y373tXvfLQKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "385b1e51-a8ac-4c4a-cff3-898e6c621316"
      },
      "source": [
        "n_stacks = 3\n",
        "h_layers_len = [150, 75, 50]\n",
        "learning_rates = [[0.001, 0.001], [0.001, 0.001], [0.001, 0.001]]\n",
        "k_list = [1000]*3\n",
        "\n",
        "weights_pre_trained, biases_pre_trained = Gaussian_stacked_RBM(v_whitened, n_stacks, h_layers_len, learning_rates, k_list)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------Gaussian Binary RBM------\n",
            "Epoch: 1, Error: 1400.6890869140625, Error diff :inf\n",
            "Epoch: 2, Error: 1400.926025390625, Error diff :0.00016912989667616785\n",
            "Converged!\n",
            "------Binary Binary RBM 1------\n",
            "Epoch: 1, Error: 86.59659576416016, Error diff :inf\n",
            "Epoch: 2, Error: 86.4772720336914, Error diff :0.0013798276195302606\n",
            "Epoch: 3, Error: 86.42826843261719, Error diff :0.0005669858073815703\n",
            "Converged!\n",
            "------Binary Binary RBM 2------\n",
            "Epoch: 1, Error: 39.97514343261719, Error diff :inf\n",
            "Epoch: 2, Error: 39.8082389831543, Error diff :0.004192711319774389\n",
            "Epoch: 3, Error: 39.825286865234375, Error diff :0.000428066763561219\n",
            "Converged!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-PXOPrgLQKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = FinalNet(v.shape[1], h_layers_len, len(np.unique(np.array(list(trainloader)[0][1]))))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sjnB7v_LQKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    classifier.fc1.weight.data = nn.Parameter(weights_pre_trained[0].t())\n",
        "    classifier.fc1.bias.data = nn.Parameter(biases_pre_trained[0].squeeze(0))\n",
        "    \n",
        "    classifier.fc2.weight = nn.Parameter(weights_pre_trained[1].t())\n",
        "    classifier.fc2.bias = nn.Parameter(biases_pre_trained[1].squeeze(0))\n",
        "    \n",
        "    classifier.fc3.weight = nn.Parameter(weights_pre_trained[2].t())\n",
        "    classifier.fc3.bias = nn.Parameter(biases_pre_trained[2].squeeze(0))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4urK_3bWLQKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
        "classifier = classifier.to(device)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEVCDLfwLQKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainloader, testloader = train_test_loader('/content/drive/My Drive/Data_Set_1(Colored_Images)', label_dict, train_fraction=0.8, num_workers=0, bs_fraction = 0.2)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MdBkNmCLQKX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e0d6770-fc2d-4836-bf64-e65efb351750"
      },
      "source": [
        "old_loss = np.inf\n",
        "\n",
        "max_epoch = 500\n",
        "losses = []\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for data in trainloader:\n",
        "\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward\n",
        "        y_hat = classifier(X)\n",
        "\n",
        "        # Calculate Loss (Cross Entropy)\n",
        "        loss = criterion(y_hat, y)\n",
        "        \n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update Parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
        "    losses.append(running_loss)\n",
        "    \n",
        "    if abs(running_loss-old_loss)/running_loss < 1e-6:\n",
        "        print('Converged')\n",
        "        break\n",
        "    \n",
        "    old_loss = running_loss\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Loss = 4.279282093048096\n",
            "Epoch 2 : Loss = 4.1563884019851685\n",
            "Epoch 3 : Loss = 4.172834634780884\n",
            "Epoch 4 : Loss = 4.12682718038559\n",
            "Epoch 5 : Loss = 4.208453059196472\n",
            "Epoch 6 : Loss = 4.0969648361206055\n",
            "Epoch 7 : Loss = 4.007447600364685\n",
            "Epoch 8 : Loss = 3.9938154816627502\n",
            "Epoch 9 : Loss = 4.016039729118347\n",
            "Epoch 10 : Loss = 3.9854941964149475\n",
            "Epoch 11 : Loss = 3.902457594871521\n",
            "Epoch 12 : Loss = 3.8902514576911926\n",
            "Epoch 13 : Loss = 3.912411868572235\n",
            "Epoch 14 : Loss = 3.9602906703948975\n",
            "Epoch 15 : Loss = 3.966278314590454\n",
            "Epoch 16 : Loss = 3.8940932154655457\n",
            "Epoch 17 : Loss = 3.8590981364250183\n",
            "Epoch 18 : Loss = 3.925403654575348\n",
            "Epoch 19 : Loss = 3.8896188139915466\n",
            "Epoch 20 : Loss = 4.013372898101807\n",
            "Epoch 21 : Loss = 4.058172762393951\n",
            "Epoch 22 : Loss = 4.163473725318909\n",
            "Epoch 23 : Loss = 4.30550479888916\n",
            "Epoch 24 : Loss = 4.114674746990204\n",
            "Epoch 25 : Loss = 3.914518356323242\n",
            "Epoch 26 : Loss = 3.960690498352051\n",
            "Epoch 27 : Loss = 3.924605369567871\n",
            "Epoch 28 : Loss = 3.781192123889923\n",
            "Epoch 29 : Loss = 3.6703301668167114\n",
            "Epoch 30 : Loss = 3.623623013496399\n",
            "Epoch 31 : Loss = 3.5546266436576843\n",
            "Epoch 32 : Loss = 3.5236979126930237\n",
            "Epoch 33 : Loss = 3.49302738904953\n",
            "Epoch 34 : Loss = 3.446080446243286\n",
            "Epoch 35 : Loss = 3.4234586358070374\n",
            "Epoch 36 : Loss = 3.4102181792259216\n",
            "Epoch 37 : Loss = 3.4041154384613037\n",
            "Epoch 38 : Loss = 3.479250907897949\n",
            "Epoch 39 : Loss = 3.551498055458069\n",
            "Epoch 40 : Loss = 3.4662429690361023\n",
            "Epoch 41 : Loss = 3.325324296951294\n",
            "Epoch 42 : Loss = 3.326675772666931\n",
            "Epoch 43 : Loss = 3.340444028377533\n",
            "Epoch 44 : Loss = 3.356109380722046\n",
            "Epoch 45 : Loss = 3.425847589969635\n",
            "Epoch 46 : Loss = 3.4116768836975098\n",
            "Epoch 47 : Loss = 3.278919517993927\n",
            "Epoch 48 : Loss = 3.2247875928878784\n",
            "Epoch 49 : Loss = 3.2213420271873474\n",
            "Epoch 50 : Loss = 3.153554916381836\n",
            "Epoch 51 : Loss = 3.206191599369049\n",
            "Epoch 52 : Loss = 3.2345982789993286\n",
            "Epoch 53 : Loss = 3.1941638588905334\n",
            "Epoch 54 : Loss = 3.249928057193756\n",
            "Epoch 55 : Loss = 3.1947959661483765\n",
            "Epoch 56 : Loss = 3.196492612361908\n",
            "Epoch 57 : Loss = 3.1103373765945435\n",
            "Epoch 58 : Loss = 3.157433867454529\n",
            "Epoch 59 : Loss = 3.283965051174164\n",
            "Epoch 60 : Loss = 3.2786627411842346\n",
            "Epoch 61 : Loss = 3.0300461053848267\n",
            "Epoch 62 : Loss = 3.077412962913513\n",
            "Epoch 63 : Loss = 3.165197193622589\n",
            "Epoch 64 : Loss = 3.096514880657196\n",
            "Epoch 65 : Loss = 3.108551859855652\n",
            "Epoch 66 : Loss = 2.9635923504829407\n",
            "Epoch 67 : Loss = 2.941065728664398\n",
            "Epoch 68 : Loss = 2.9502498507499695\n",
            "Epoch 69 : Loss = 2.845039129257202\n",
            "Epoch 70 : Loss = 2.808909833431244\n",
            "Epoch 71 : Loss = 2.7811853885650635\n",
            "Epoch 72 : Loss = 2.7780805230140686\n",
            "Epoch 73 : Loss = 2.8283379673957825\n",
            "Epoch 74 : Loss = 2.834783971309662\n",
            "Epoch 75 : Loss = 2.888925552368164\n",
            "Epoch 76 : Loss = 2.7849303483963013\n",
            "Epoch 77 : Loss = 2.7314462661743164\n",
            "Epoch 78 : Loss = 2.7074705362319946\n",
            "Epoch 79 : Loss = 2.7016862630844116\n",
            "Epoch 80 : Loss = 2.689623713493347\n",
            "Epoch 81 : Loss = 2.6491329669952393\n",
            "Epoch 82 : Loss = 2.623861789703369\n",
            "Epoch 83 : Loss = 2.673462390899658\n",
            "Epoch 84 : Loss = 2.6268712878227234\n",
            "Epoch 85 : Loss = 2.5579091906547546\n",
            "Epoch 86 : Loss = 2.5759361386299133\n",
            "Epoch 87 : Loss = 2.559325933456421\n",
            "Epoch 88 : Loss = 2.657567083835602\n",
            "Epoch 89 : Loss = 2.8743462562561035\n",
            "Epoch 90 : Loss = 3.1876733899116516\n",
            "Epoch 91 : Loss = 3.247851610183716\n",
            "Epoch 92 : Loss = 2.9778420329093933\n",
            "Epoch 93 : Loss = 2.824146091938019\n",
            "Epoch 94 : Loss = 2.898617446422577\n",
            "Epoch 95 : Loss = 2.589271903038025\n",
            "Epoch 96 : Loss = 2.468231678009033\n",
            "Epoch 97 : Loss = 2.4553791880607605\n",
            "Epoch 98 : Loss = 2.4369770288467407\n",
            "Epoch 99 : Loss = 2.4352049231529236\n",
            "Epoch 100 : Loss = 2.4675374031066895\n",
            "Epoch 101 : Loss = 2.4258898496627808\n",
            "Epoch 102 : Loss = 2.398690164089203\n",
            "Epoch 103 : Loss = 2.3409230709075928\n",
            "Epoch 104 : Loss = 2.3203564882278442\n",
            "Epoch 105 : Loss = 2.329828381538391\n",
            "Epoch 106 : Loss = 2.4277262687683105\n",
            "Epoch 107 : Loss = 2.4111194610595703\n",
            "Epoch 108 : Loss = 2.3655465245246887\n",
            "Epoch 109 : Loss = 2.3235984444618225\n",
            "Epoch 110 : Loss = 2.478000044822693\n",
            "Epoch 111 : Loss = 2.4532719254493713\n",
            "Epoch 112 : Loss = 2.5404768586158752\n",
            "Epoch 113 : Loss = 2.310964345932007\n",
            "Epoch 114 : Loss = 2.208810567855835\n",
            "Epoch 115 : Loss = 2.1857995986938477\n",
            "Epoch 116 : Loss = 2.2164440155029297\n",
            "Epoch 117 : Loss = 2.2001760005950928\n",
            "Epoch 118 : Loss = 2.224012017250061\n",
            "Epoch 119 : Loss = 2.1565328240394592\n",
            "Epoch 120 : Loss = 2.275719463825226\n",
            "Epoch 121 : Loss = 2.2458083033561707\n",
            "Epoch 122 : Loss = 2.2492191195487976\n",
            "Epoch 123 : Loss = 2.1640180945396423\n",
            "Epoch 124 : Loss = 2.176894724369049\n",
            "Epoch 125 : Loss = 2.3895463943481445\n",
            "Epoch 126 : Loss = 2.2263468205928802\n",
            "Epoch 127 : Loss = 2.181437373161316\n",
            "Epoch 128 : Loss = 2.219196617603302\n",
            "Epoch 129 : Loss = 2.116697609424591\n",
            "Epoch 130 : Loss = 2.0954457223415375\n",
            "Epoch 131 : Loss = 2.0148182809352875\n",
            "Epoch 132 : Loss = 2.0050321221351624\n",
            "Epoch 133 : Loss = 1.9916020035743713\n",
            "Epoch 134 : Loss = 2.038179248571396\n",
            "Epoch 135 : Loss = 1.946958988904953\n",
            "Epoch 136 : Loss = 1.9474234879016876\n",
            "Epoch 137 : Loss = 1.9673528373241425\n",
            "Epoch 138 : Loss = 1.9196668863296509\n",
            "Epoch 139 : Loss = 2.0050122141838074\n",
            "Epoch 140 : Loss = 1.9622935056686401\n",
            "Epoch 141 : Loss = 1.9966842234134674\n",
            "Epoch 142 : Loss = 2.0761645138263702\n",
            "Epoch 143 : Loss = 1.9701656699180603\n",
            "Epoch 144 : Loss = 2.0427304208278656\n",
            "Epoch 145 : Loss = 1.9339694380760193\n",
            "Epoch 146 : Loss = 2.091344803571701\n",
            "Epoch 147 : Loss = 1.9467220306396484\n",
            "Epoch 148 : Loss = 2.098000943660736\n",
            "Epoch 149 : Loss = 2.9807897806167603\n",
            "Epoch 150 : Loss = 4.311826586723328\n",
            "Epoch 151 : Loss = 5.928536415100098\n",
            "Epoch 152 : Loss = 5.22058892250061\n",
            "Epoch 153 : Loss = 5.032732009887695\n",
            "Epoch 154 : Loss = 4.487071871757507\n",
            "Epoch 155 : Loss = 4.084290444850922\n",
            "Epoch 156 : Loss = 3.8649153113365173\n",
            "Epoch 157 : Loss = 3.7355198860168457\n",
            "Epoch 158 : Loss = 3.6028941869735718\n",
            "Epoch 159 : Loss = 3.5121570229530334\n",
            "Epoch 160 : Loss = 3.4231740832328796\n",
            "Epoch 161 : Loss = 3.324187994003296\n",
            "Epoch 162 : Loss = 3.2245715260505676\n",
            "Epoch 163 : Loss = 3.210386633872986\n",
            "Epoch 164 : Loss = 3.1387537717819214\n",
            "Epoch 165 : Loss = 3.1042473316192627\n",
            "Epoch 166 : Loss = 3.134790062904358\n",
            "Epoch 167 : Loss = 2.8727030158042908\n",
            "Epoch 168 : Loss = 2.7103832364082336\n",
            "Epoch 169 : Loss = 2.773192524909973\n",
            "Epoch 170 : Loss = 2.629827082157135\n",
            "Epoch 171 : Loss = 2.5387893319129944\n",
            "Epoch 172 : Loss = 2.4844507575035095\n",
            "Epoch 173 : Loss = 2.3919628262519836\n",
            "Epoch 174 : Loss = 2.305568814277649\n",
            "Epoch 175 : Loss = 2.2589622139930725\n",
            "Epoch 176 : Loss = 2.165297508239746\n",
            "Epoch 177 : Loss = 2.1177388429641724\n",
            "Epoch 178 : Loss = 2.0898262560367584\n",
            "Epoch 179 : Loss = 2.111150622367859\n",
            "Epoch 180 : Loss = 2.0981891751289368\n",
            "Epoch 181 : Loss = 2.0093235075473785\n",
            "Epoch 182 : Loss = 1.9942831099033356\n",
            "Epoch 183 : Loss = 2.1962838172912598\n",
            "Epoch 184 : Loss = 2.0871198177337646\n",
            "Epoch 185 : Loss = 2.412070870399475\n",
            "Epoch 186 : Loss = 2.246823012828827\n",
            "Epoch 187 : Loss = 2.0154917240142822\n",
            "Epoch 188 : Loss = 2.1375914812088013\n",
            "Epoch 189 : Loss = 1.9621435105800629\n",
            "Epoch 190 : Loss = 1.8486522436141968\n",
            "Epoch 191 : Loss = 1.7879353165626526\n",
            "Epoch 192 : Loss = 1.7886058986186981\n",
            "Epoch 193 : Loss = 1.74777752161026\n",
            "Epoch 194 : Loss = 1.7279481291770935\n",
            "Epoch 195 : Loss = 1.7661353647708893\n",
            "Epoch 196 : Loss = 1.7129096388816833\n",
            "Epoch 197 : Loss = 1.6912673711776733\n",
            "Epoch 198 : Loss = 1.7015010118484497\n",
            "Epoch 199 : Loss = 1.6844300627708435\n",
            "Epoch 200 : Loss = 1.6397033631801605\n",
            "Epoch 201 : Loss = 1.653040587902069\n",
            "Epoch 202 : Loss = 1.6286471784114838\n",
            "Epoch 203 : Loss = 1.697753369808197\n",
            "Epoch 204 : Loss = 1.6069015860557556\n",
            "Epoch 205 : Loss = 1.6025914251804352\n",
            "Epoch 206 : Loss = 1.5734509229660034\n",
            "Epoch 207 : Loss = 1.5358172357082367\n",
            "Epoch 208 : Loss = 1.516323208808899\n",
            "Epoch 209 : Loss = 1.510704666376114\n",
            "Epoch 210 : Loss = 1.4855084419250488\n",
            "Epoch 211 : Loss = 1.4775230884552002\n",
            "Epoch 212 : Loss = 1.460801750421524\n",
            "Epoch 213 : Loss = 1.4524968564510345\n",
            "Epoch 214 : Loss = 1.4328411221504211\n",
            "Epoch 215 : Loss = 1.4509575963020325\n",
            "Epoch 216 : Loss = 1.4282937943935394\n",
            "Epoch 217 : Loss = 1.4180436730384827\n",
            "Epoch 218 : Loss = 1.4322926104068756\n",
            "Epoch 219 : Loss = 1.4097171127796173\n",
            "Epoch 220 : Loss = 1.3677450716495514\n",
            "Epoch 221 : Loss = 1.3870899379253387\n",
            "Epoch 222 : Loss = 1.3815979361534119\n",
            "Epoch 223 : Loss = 1.3622250258922577\n",
            "Epoch 224 : Loss = 1.3372626304626465\n",
            "Epoch 225 : Loss = 1.321212112903595\n",
            "Epoch 226 : Loss = 1.3072815537452698\n",
            "Epoch 227 : Loss = 1.3048455715179443\n",
            "Epoch 228 : Loss = 1.3071047961711884\n",
            "Epoch 229 : Loss = 1.3085907399654388\n",
            "Epoch 230 : Loss = 1.3355296850204468\n",
            "Epoch 231 : Loss = 1.3628412783145905\n",
            "Epoch 232 : Loss = 1.2865988612174988\n",
            "Epoch 233 : Loss = 1.2705025374889374\n",
            "Epoch 234 : Loss = 1.2602673768997192\n",
            "Epoch 235 : Loss = 1.2706844806671143\n",
            "Epoch 236 : Loss = 1.2771124243736267\n",
            "Epoch 237 : Loss = 1.2465484738349915\n",
            "Epoch 238 : Loss = 1.2848384380340576\n",
            "Epoch 239 : Loss = 1.2936748266220093\n",
            "Epoch 240 : Loss = 1.3024044334888458\n",
            "Epoch 241 : Loss = 1.4713926911354065\n",
            "Epoch 242 : Loss = 1.4797845780849457\n",
            "Epoch 243 : Loss = 1.9756659269332886\n",
            "Epoch 244 : Loss = 2.649178773164749\n",
            "Epoch 245 : Loss = 2.1516707837581635\n",
            "Epoch 246 : Loss = 1.7204008400440216\n",
            "Epoch 247 : Loss = 1.3651390373706818\n",
            "Epoch 248 : Loss = 1.2925150096416473\n",
            "Epoch 249 : Loss = 1.2858421504497528\n",
            "Epoch 250 : Loss = 1.2140617966651917\n",
            "Epoch 251 : Loss = 1.1756711602210999\n",
            "Epoch 252 : Loss = 1.1510499715805054\n",
            "Epoch 253 : Loss = 1.1305584013462067\n",
            "Epoch 254 : Loss = 1.1159143447875977\n",
            "Epoch 255 : Loss = 1.108987271785736\n",
            "Epoch 256 : Loss = 1.102354258298874\n",
            "Epoch 257 : Loss = 1.088627353310585\n",
            "Epoch 258 : Loss = 1.0978409051895142\n",
            "Epoch 259 : Loss = 1.0829835385084152\n",
            "Epoch 260 : Loss = 1.0812814235687256\n",
            "Epoch 261 : Loss = 1.0623815953731537\n",
            "Epoch 262 : Loss = 1.056004911661148\n",
            "Epoch 263 : Loss = 1.0558722615242004\n",
            "Epoch 264 : Loss = 1.0789902955293655\n",
            "Epoch 265 : Loss = 1.047889083623886\n",
            "Epoch 266 : Loss = 1.0324276238679886\n",
            "Epoch 267 : Loss = 1.069113314151764\n",
            "Epoch 268 : Loss = 1.120292067527771\n",
            "Epoch 269 : Loss = 1.2887615710496902\n",
            "Epoch 270 : Loss = 1.1794264614582062\n",
            "Epoch 271 : Loss = 1.6948496103286743\n",
            "Epoch 272 : Loss = 1.7022688388824463\n",
            "Epoch 273 : Loss = 1.670316755771637\n",
            "Epoch 274 : Loss = 1.6403241753578186\n",
            "Epoch 275 : Loss = 1.8761582374572754\n",
            "Epoch 276 : Loss = 1.484226793050766\n",
            "Epoch 277 : Loss = 2.0135921835899353\n",
            "Epoch 278 : Loss = 2.05965119600296\n",
            "Epoch 279 : Loss = 1.7469753623008728\n",
            "Epoch 280 : Loss = 2.237790584564209\n",
            "Epoch 281 : Loss = 2.0446862876415253\n",
            "Epoch 282 : Loss = 1.7046469449996948\n",
            "Epoch 283 : Loss = 1.510618805885315\n",
            "Epoch 284 : Loss = 1.2732888162136078\n",
            "Epoch 285 : Loss = 1.1187458634376526\n",
            "Epoch 286 : Loss = 1.0532316118478775\n",
            "Epoch 287 : Loss = 1.0338760018348694\n",
            "Epoch 288 : Loss = 0.9877232909202576\n",
            "Epoch 289 : Loss = 0.9722755253314972\n",
            "Epoch 290 : Loss = 0.9530499279499054\n",
            "Epoch 291 : Loss = 0.9363882094621658\n",
            "Epoch 292 : Loss = 0.9258637875318527\n",
            "Epoch 293 : Loss = 0.9321179240942001\n",
            "Epoch 294 : Loss = 0.916390910744667\n",
            "Epoch 295 : Loss = 0.9030352234840393\n",
            "Epoch 296 : Loss = 0.9015788286924362\n",
            "Epoch 297 : Loss = 0.8917170614004135\n",
            "Epoch 298 : Loss = 0.8884120285511017\n",
            "Epoch 299 : Loss = 0.8903223872184753\n",
            "Epoch 300 : Loss = 0.8744400441646576\n",
            "Epoch 301 : Loss = 0.8654430657625198\n",
            "Epoch 302 : Loss = 0.8591272979974747\n",
            "Epoch 303 : Loss = 0.8646475225687027\n",
            "Epoch 304 : Loss = 0.8556573539972305\n",
            "Epoch 305 : Loss = 0.8516567051410675\n",
            "Epoch 306 : Loss = 0.8615347295999527\n",
            "Epoch 307 : Loss = 0.8498035669326782\n",
            "Epoch 308 : Loss = 0.842506930232048\n",
            "Epoch 309 : Loss = 0.8377699702978134\n",
            "Epoch 310 : Loss = 0.8365614414215088\n",
            "Epoch 311 : Loss = 0.8243289589881897\n",
            "Epoch 312 : Loss = 0.816348060965538\n",
            "Epoch 313 : Loss = 0.8146359324455261\n",
            "Epoch 314 : Loss = 0.8101997971534729\n",
            "Epoch 315 : Loss = 0.8030913174152374\n",
            "Epoch 316 : Loss = 0.8058398067951202\n",
            "Epoch 317 : Loss = 0.7936558723449707\n",
            "Epoch 318 : Loss = 0.7863049507141113\n",
            "Epoch 319 : Loss = 0.7848173230886459\n",
            "Epoch 320 : Loss = 0.785345658659935\n",
            "Epoch 321 : Loss = 0.7759147584438324\n",
            "Epoch 322 : Loss = 0.7740457952022552\n",
            "Epoch 323 : Loss = 0.7697509378194809\n",
            "Epoch 324 : Loss = 0.76540108025074\n",
            "Epoch 325 : Loss = 0.7579113841056824\n",
            "Epoch 326 : Loss = 0.7610241174697876\n",
            "Epoch 327 : Loss = 0.75164894759655\n",
            "Epoch 328 : Loss = 0.7474251687526703\n",
            "Epoch 329 : Loss = 0.7510192096233368\n",
            "Epoch 330 : Loss = 0.7421691566705704\n",
            "Epoch 331 : Loss = 0.7367064356803894\n",
            "Epoch 332 : Loss = 0.7315032184123993\n",
            "Epoch 333 : Loss = 0.728439062833786\n",
            "Epoch 334 : Loss = 0.7249691337347031\n",
            "Epoch 335 : Loss = 0.7208951711654663\n",
            "Epoch 336 : Loss = 0.7218944430351257\n",
            "Epoch 337 : Loss = 0.7173679918050766\n",
            "Epoch 338 : Loss = 0.7124312669038773\n",
            "Epoch 339 : Loss = 0.7111101746559143\n",
            "Epoch 340 : Loss = 0.7032398879528046\n",
            "Epoch 341 : Loss = 0.6977275609970093\n",
            "Epoch 342 : Loss = 0.6963785737752914\n",
            "Epoch 343 : Loss = 0.6936600059270859\n",
            "Epoch 344 : Loss = 0.690186619758606\n",
            "Epoch 345 : Loss = 0.684683308005333\n",
            "Epoch 346 : Loss = 0.6849365234375\n",
            "Epoch 347 : Loss = 0.6768624037504196\n",
            "Epoch 348 : Loss = 0.6757684648036957\n",
            "Epoch 349 : Loss = 0.6724375784397125\n",
            "Epoch 350 : Loss = 0.6720618605613708\n",
            "Epoch 351 : Loss = 0.674799233675003\n",
            "Epoch 352 : Loss = 0.6764461249113083\n",
            "Epoch 353 : Loss = 0.6712640076875687\n",
            "Epoch 354 : Loss = 0.6745177656412125\n",
            "Epoch 355 : Loss = 0.671756699681282\n",
            "Epoch 356 : Loss = 0.6795405298471451\n",
            "Epoch 357 : Loss = 0.677520826458931\n",
            "Epoch 358 : Loss = 0.6591520607471466\n",
            "Epoch 359 : Loss = 0.6550110280513763\n",
            "Epoch 360 : Loss = 0.6472961157560349\n",
            "Epoch 361 : Loss = 0.6419790238142014\n",
            "Epoch 362 : Loss = 0.6385814547538757\n",
            "Epoch 363 : Loss = 0.6368782669305801\n",
            "Epoch 364 : Loss = 0.6368214339017868\n",
            "Epoch 365 : Loss = 0.6327134370803833\n",
            "Epoch 366 : Loss = 0.629222184419632\n",
            "Epoch 367 : Loss = 0.6244986653327942\n",
            "Epoch 368 : Loss = 0.6228765398263931\n",
            "Epoch 369 : Loss = 0.6185203939676285\n",
            "Epoch 370 : Loss = 0.6158629357814789\n",
            "Epoch 371 : Loss = 0.6202068626880646\n",
            "Epoch 372 : Loss = 0.6200169920921326\n",
            "Epoch 373 : Loss = 0.6241473853588104\n",
            "Epoch 374 : Loss = 0.6137630492448807\n",
            "Epoch 375 : Loss = 0.6211454123258591\n",
            "Epoch 376 : Loss = 0.6171285510063171\n",
            "Epoch 377 : Loss = 0.6038955599069595\n",
            "Epoch 378 : Loss = 0.5996997132897377\n",
            "Epoch 379 : Loss = 0.5972762554883957\n",
            "Epoch 380 : Loss = 0.5967492908239365\n",
            "Epoch 381 : Loss = 0.5901150479912758\n",
            "Epoch 382 : Loss = 0.5874626934528351\n",
            "Epoch 383 : Loss = 0.5838680565357208\n",
            "Epoch 384 : Loss = 0.5804157257080078\n",
            "Epoch 385 : Loss = 0.5810509324073792\n",
            "Epoch 386 : Loss = 0.5777366608381271\n",
            "Epoch 387 : Loss = 0.5789470300078392\n",
            "Epoch 388 : Loss = 0.5753837078809738\n",
            "Epoch 389 : Loss = 0.5728651434183121\n",
            "Epoch 390 : Loss = 0.5730679556727409\n",
            "Epoch 391 : Loss = 0.5681630223989487\n",
            "Epoch 392 : Loss = 0.5641086846590042\n",
            "Epoch 393 : Loss = 0.5664275735616684\n",
            "Epoch 394 : Loss = 0.5605622231960297\n",
            "Epoch 395 : Loss = 0.5567811578512192\n",
            "Epoch 396 : Loss = 0.5533934086561203\n",
            "Epoch 397 : Loss = 0.5503073558211327\n",
            "Epoch 398 : Loss = 0.5491944700479507\n",
            "Epoch 399 : Loss = 0.5463227853178978\n",
            "Epoch 400 : Loss = 0.5436233654618263\n",
            "Epoch 401 : Loss = 0.540470726788044\n",
            "Epoch 402 : Loss = 0.5394834652543068\n",
            "Epoch 403 : Loss = 0.5392294973134995\n",
            "Epoch 404 : Loss = 0.5388186722993851\n",
            "Epoch 405 : Loss = 0.5354504510760307\n",
            "Epoch 406 : Loss = 0.5342338755726814\n",
            "Epoch 407 : Loss = 0.5327492952346802\n",
            "Epoch 408 : Loss = 0.5327421724796295\n",
            "Epoch 409 : Loss = 0.5327318236231804\n",
            "Epoch 410 : Loss = 0.5340462625026703\n",
            "Epoch 411 : Loss = 0.5314181819558144\n",
            "Epoch 412 : Loss = 0.5402692183852196\n",
            "Epoch 413 : Loss = 0.5412042587995529\n",
            "Epoch 414 : Loss = 0.5311540886759758\n",
            "Epoch 415 : Loss = 0.5246244668960571\n",
            "Epoch 416 : Loss = 0.5231934860348701\n",
            "Epoch 417 : Loss = 0.5170160830020905\n",
            "Epoch 418 : Loss = 0.5131324231624603\n",
            "Epoch 419 : Loss = 0.5128786563873291\n",
            "Epoch 420 : Loss = 0.507856585085392\n",
            "Epoch 421 : Loss = 0.5094821229577065\n",
            "Epoch 422 : Loss = 0.5064239948987961\n",
            "Epoch 423 : Loss = 0.5087511986494064\n",
            "Epoch 424 : Loss = 0.5043495520949364\n",
            "Epoch 425 : Loss = 0.4986192211508751\n",
            "Epoch 426 : Loss = 0.4945088252425194\n",
            "Epoch 427 : Loss = 0.49259568750858307\n",
            "Epoch 428 : Loss = 0.4924415349960327\n",
            "Epoch 429 : Loss = 0.4915498122572899\n",
            "Epoch 430 : Loss = 0.4859190359711647\n",
            "Epoch 431 : Loss = 0.4861180931329727\n",
            "Epoch 432 : Loss = 0.4843484088778496\n",
            "Epoch 433 : Loss = 0.48209165036678314\n",
            "Epoch 434 : Loss = 0.4850412905216217\n",
            "Epoch 435 : Loss = 0.4839050844311714\n",
            "Epoch 436 : Loss = 0.4853458032011986\n",
            "Epoch 437 : Loss = 0.48764966428279877\n",
            "Epoch 438 : Loss = 0.48284727334976196\n",
            "Epoch 439 : Loss = 0.4830673113465309\n",
            "Epoch 440 : Loss = 0.4749401733279228\n",
            "Epoch 441 : Loss = 0.47135987877845764\n",
            "Epoch 442 : Loss = 0.46868015080690384\n",
            "Epoch 443 : Loss = 0.46536071598529816\n",
            "Epoch 444 : Loss = 0.46518952399492264\n",
            "Epoch 445 : Loss = 0.4638953283429146\n",
            "Epoch 446 : Loss = 0.4607146456837654\n",
            "Epoch 447 : Loss = 0.4598337560892105\n",
            "Epoch 448 : Loss = 0.45663000643253326\n",
            "Epoch 449 : Loss = 0.45814181864261627\n",
            "Epoch 450 : Loss = 0.4549335911870003\n",
            "Epoch 451 : Loss = 0.45347653329372406\n",
            "Epoch 452 : Loss = 0.45130762457847595\n",
            "Epoch 453 : Loss = 0.4480404555797577\n",
            "Epoch 454 : Loss = 0.4461604878306389\n",
            "Epoch 455 : Loss = 0.44629964232444763\n",
            "Epoch 456 : Loss = 0.44378551095724106\n",
            "Epoch 457 : Loss = 0.4418949633836746\n",
            "Epoch 458 : Loss = 0.4397996515035629\n",
            "Epoch 459 : Loss = 0.43722862750291824\n",
            "Epoch 460 : Loss = 0.4370109736919403\n",
            "Epoch 461 : Loss = 0.437897264957428\n",
            "Epoch 462 : Loss = 0.4362116903066635\n",
            "Epoch 463 : Loss = 0.4320336729288101\n",
            "Epoch 464 : Loss = 0.42963414639234543\n",
            "Epoch 465 : Loss = 0.4314109906554222\n",
            "Epoch 466 : Loss = 0.4285004511475563\n",
            "Epoch 467 : Loss = 0.42656752467155457\n",
            "Epoch 468 : Loss = 0.4252341687679291\n",
            "Epoch 469 : Loss = 0.4235115945339203\n",
            "Epoch 470 : Loss = 0.42223304510116577\n",
            "Epoch 471 : Loss = 0.41952141374349594\n",
            "Epoch 472 : Loss = 0.41958580911159515\n",
            "Epoch 473 : Loss = 0.4172825440764427\n",
            "Epoch 474 : Loss = 0.41702696681022644\n",
            "Epoch 475 : Loss = 0.41527554392814636\n",
            "Epoch 476 : Loss = 0.41248369961977005\n",
            "Epoch 477 : Loss = 0.4114258587360382\n",
            "Epoch 478 : Loss = 0.4115222617983818\n",
            "Epoch 479 : Loss = 0.40967439860105515\n",
            "Epoch 480 : Loss = 0.4074248895049095\n",
            "Epoch 481 : Loss = 0.409582756459713\n",
            "Epoch 482 : Loss = 0.4055917412042618\n",
            "Epoch 483 : Loss = 0.404599592089653\n",
            "Epoch 484 : Loss = 0.40362293273210526\n",
            "Epoch 485 : Loss = 0.4042814150452614\n",
            "Epoch 486 : Loss = 0.4031737595796585\n",
            "Epoch 487 : Loss = 0.3985987976193428\n",
            "Epoch 488 : Loss = 0.39886604249477386\n",
            "Epoch 489 : Loss = 0.39653951674699783\n",
            "Epoch 490 : Loss = 0.39533475041389465\n",
            "Epoch 491 : Loss = 0.3950040638446808\n",
            "Epoch 492 : Loss = 0.3937194272875786\n",
            "Epoch 493 : Loss = 0.39145126938819885\n",
            "Epoch 494 : Loss = 0.39081813395023346\n",
            "Epoch 495 : Loss = 0.38851846754550934\n",
            "Epoch 496 : Loss = 0.3895829766988754\n",
            "Epoch 497 : Loss = 0.38683976978063583\n",
            "Epoch 498 : Loss = 0.3842812404036522\n",
            "Epoch 499 : Loss = 0.3832850679755211\n",
            "Epoch 500 : Loss = 0.3815769702196121\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrDlkBooU5ns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "0f8641d4-16d9-47d9-84c8-76bb5f079408"
      },
      "source": [
        "plt.plot(list(range(1,len(losses)+1)), losses)\n",
        "plt.title('Convergence Monitor')\n",
        "plt.xlabel('Iter Number')\n",
        "plt.ylabel('Loss')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcTlhXV9QUtq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "09a060eb-110b-4bb7-d64a-3416ccca02b2"
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    train_loss = 0.0\n",
        "    y_train = []\n",
        "    y_train_pred = []\n",
        "\n",
        "    for data in trainloader:\n",
        "\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        y_hat = classifier(X)      \n",
        "        train_loss += criterion(y_hat, y)\n",
        "        \n",
        "        y_train.extend(list(y.cpu().detach().numpy()))\n",
        "        y_train_pred.extend(list(torch.argmax(y_hat, axis=1).cpu().detach().numpy()))\n",
        "\n",
        "print('Train Loss =', train_loss.item())\n",
        "pd.DataFrame(confusion_matrix(y_train, y_train_pred))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.3769224286079407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>268</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>289</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>198</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>296</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>338</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2    3    4\n",
              "0  268    1    0    1    1\n",
              "1    3  289    3    1    3\n",
              "2    0    0  198    1    0\n",
              "3    0    1    1  296    1\n",
              "4    0    0    0    2  338"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXJSHlD2pgbl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5a4b213-f933-4875-9025-46b7cfe058f7"
      },
      "source": [
        "acc = accuracy_score(y_train, y_train_pred)\n",
        "prec = precision_score(y_train, y_train_pred, average='macro')\n",
        "f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "\n",
        "print('Train Accuracy =', acc, 'Train Precision =', prec, 'Train F1 =', f1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy = 0.9865056818181818 Train Precision = 0.9862133003421828 Train F1 = 0.9865079686162341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C_sLteiQQIC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "d9c97d9f-e3e8-4c84-8917-796cbdc804ab"
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    test_loss = 0.0\n",
        "    y_test = []\n",
        "    y_test_pred = []\n",
        "\n",
        "    for data in testloader:\n",
        "\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        y_hat = classifier(X)      \n",
        "        test_loss += criterion(y_hat, y)\n",
        "        \n",
        "        y_test.extend(list(y.cpu().detach().numpy()))\n",
        "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).cpu().detach().numpy()))\n",
        "\n",
        "print('Test Loss =', test_loss.item())\n",
        "pd.DataFrame(confusion_matrix(y_test, y_test_pred))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss = 1.2461928129196167\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>53</td>\n",
              "      <td>16</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15</td>\n",
              "      <td>41</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>38</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>35</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    0   1   2   3   4\n",
              "0  53  16   7   5   4\n",
              "1  15  41   4   6   9\n",
              "2   4   5  38  13   1\n",
              "3   4   5   4  35  13\n",
              "4   1   5   0  18  46"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqN0D-Mipi_K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d70a4ae1-e424-47ee-a78a-f5692c3daaf0"
      },
      "source": [
        "acc = accuracy_score(y_test, y_test_pred)\n",
        "prec = precision_score(y_test, y_test_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "print('Test Accuracy =', acc, 'Test Precision =', prec, 'Test F1 =', f1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy = 0.6051136363636364 Test Precision = 0.6118839411356858 Test F1 = 0.6058827607481851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl99XPXGpjik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the model\n",
        "\n",
        "model_state = {'state_dict': classifier.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "model_path = '/content/drive/My Drive/Q5model.pt'\n",
        "torch.save(model_state, model_path)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WIcFFDipi1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}