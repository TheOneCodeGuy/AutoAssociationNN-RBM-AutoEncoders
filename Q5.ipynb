{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Q5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw9tMESAZD44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN9XtcSDLQJs",
        "colab_type": "text"
      },
      "source": [
        "## Group 3:\n",
        "Classes: 3, 1, 4, 6, 8\n",
        "    \n",
        "    Open Country - 3\n",
        "    Tall Building - 1\n",
        "    Mountain - 4\n",
        "    Highway - 6\n",
        "    Coast - 8\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZLxugJ7LQJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from io import StringIO\n",
        "import pdb\n",
        "from math import sqrt, log"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmaPR3JkLQJx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6d4e402-c6cb-4905-86bb-d1eacd12c627"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t_ogzkSLQJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DatasetClass(Dataset):\n",
        "    \n",
        "    def __init__(self, folder, filename, label_dict):\n",
        "        \n",
        "        self.data = []\n",
        "        self.filename = filename\n",
        "        tar = tarfile.open(folder + '/' + filename)\n",
        "        for file in tar.getmembers():\n",
        "            f = tar.extractfile(file)\n",
        "            if f != None:\n",
        "                content = pd.read_csv(StringIO(f.read().decode()), sep=' ', header=None).values.ravel()\n",
        "                self.data.append(content)\n",
        "            \n",
        "        self.y = torch.tensor(label_dict[self.filename[:-7]], dtype=torch.long)\n",
        "    \n",
        "    def __getitem__(self, idx):     \n",
        "        \n",
        "        return torch.tensor(self.data[idx], dtype=torch.float), self.y\n",
        "      \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.data)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5kQMX5vLQJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_loader(directory, label_dict, train_fraction=0.8, num_workers=2, bs_fraction = 0.2):\n",
        "\n",
        "    all_files = list(filter(lambda x: x.endswith('.tar.gz'), os.listdir(directory)))\n",
        "    files = [file for file in all_files if file[:-7] in label_dict.keys()]\n",
        "    \n",
        "    datasets = list(map(lambda x : DatasetClass(directory, x, label_dict), files))\n",
        "    dataset = ConcatDataset(datasets)\n",
        "    N = dataset.cumulative_sizes[-1]\n",
        "    \n",
        "    train_size = int(N*train_fraction)\n",
        "    test_size = N - train_size\n",
        "\n",
        "    train_data, test_data = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    trainloader = DataLoader(train_data, batch_size=bs_fraction*N, shuffle=True, num_workers=num_workers)\n",
        "    testloader = DataLoader(test_data, batch_size=bs_fraction*N, shuffle=True, num_workers=num_workers)\n",
        "    \n",
        "    return trainloader, testloader"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afya0RZFLQJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GBRBM:\n",
        "    \n",
        "    def __init__(self, visible_nodes, h_len, lr_W=0.1, lr_bias=0.001):  \n",
        "        # set lower lr for bias than for the weights\n",
        "        self.N = visible_nodes.shape[0]\n",
        "        v_len = visible_nodes.shape[1]\n",
        "        self.V = visible_nodes.to(device)\n",
        "        self.sigma2 = torch.var(self.V, 0)[0].to(device)  \n",
        "        self.lr_W = lr_W\n",
        "        self.lr_bias = lr_bias\n",
        "        \n",
        "        # Initialisation done based on the methods mentioned in the paper\n",
        "        self.W = torch.empty(v_len, h_len).uniform_(-sqrt(6/(v_len+h_len)), sqrt(6/(v_len+h_len))).to(device)\n",
        "        self.b = torch.mean(visible_nodes, axis=0).view(1,-1).to(device)\n",
        "        self.c = torch.tensor([((torch.norm(self.b + self.W[:, i])**2 - torch.norm(self.b)**2)/(2*self.sigma2) +  log(0.01)).item() for i in range(h_len)]).view(1,-1).to(device)\n",
        "\n",
        "    def get_h(self, v):\n",
        "        \n",
        "        a = torch.mm((v/self.sigma2).view(1,-1), self.W) + self.c\n",
        "        f = torch.nn.Sigmoid()\n",
        "        p_h_v = f(a)\n",
        "        return p_h_v, torch.bernoulli(p_h_v)\n",
        "    \n",
        "    def get_v(self, h):\n",
        "        a = torch.mm(h.view(1,-1), self.W.T) + self.b # mean of normal dist\n",
        "        if (torch.isnan(a)).any().item():\n",
        "            pdb.set_trace()\n",
        "        else:\n",
        "            pass\n",
        "        v_h = torch.normal(mean=a, std=torch.sqrt(self.sigma2)).to(device)\n",
        "        return v_h\n",
        "    \n",
        "    def params_update(self, p_h_v0, p_h_vk, v0, vk):\n",
        "        self.W += self.lr_W*(torch.mm((v0/self.sigma2).view(-1,1), p_h_v0) - torch.mm((vk/self.sigma2).view(-1,1), p_h_vk))/self.N\n",
        "        self.b += self.lr_bias*(v0 - vk)/self.N\n",
        "        self.c += self.lr_bias*(p_h_v0 - p_h_vk)/self.N\n",
        "\n",
        "        \n",
        "    def one_epoch(self, k):\n",
        "        for v0 in self.V:\n",
        "            v_t = v0\n",
        "            for t in range(k):  \n",
        "                p_h_vt, h_t = self.get_h(v_t)\n",
        "                if t==0:\n",
        "                    p_h_v0 = p_h_vt                    \n",
        "                v_t1 = self.get_v(h_t)\n",
        "                v_t = v_t1\n",
        "\n",
        "            try:\n",
        "                V_k = torch.cat((V_k, v_t.view(1,-1)), dim=0)\n",
        "                H_k = torch.cat((H_k, h_t.view(1,-1)), dim=0)\n",
        "            except:\n",
        "                V_k = v_t.view(1,-1)\n",
        "                H_k = h_t.view(1,-1)\n",
        "            self.params_update(p_h_v0, p_h_vt, v0, v_t)\n",
        "        return V_k, H_k\n",
        "        \n",
        "    def train(self, k):\n",
        "        ep = 0\n",
        "        error_old = np.inf\n",
        "        max_ep = 100\n",
        "        while ep<=max_ep:\n",
        "            ep += 1\n",
        "            ## Check if error should be SSE?\n",
        "            V_k, H_k = self.one_epoch(k)\n",
        "\n",
        "            error_new = torch.sum((V_k - self.V)**2) \n",
        "            error_new = error_new/V_k.shape[0]\n",
        "            print('Epoch: {0}, Error: {1}, Error diff :{2}'.format(ep, error_new, (error_old-error_new)/error_new))\n",
        "            \n",
        "            if abs(error_new - error_old)/error_new <= 1e-6:\n",
        "                print('Converged!')\n",
        "                self.V_train = V_k\n",
        "                self.H_train = H_k\n",
        "                break\n",
        "            error_old = error_new\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC96PFW5LQJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BBRBM:\n",
        "    \n",
        "    def __init__(self, visible_nodes, h_len, lr_W=0.01, lr_bias=0.001):\n",
        "        \n",
        "        # set lower lr for bias than for the weights\n",
        "        \n",
        "        self.N = visible_nodes.shape[0]        \n",
        "        v_len = visible_nodes.shape[1]\n",
        "        self.W = torch.randn(v_len, h_len).to(device)\n",
        "        self.b = torch.randn(1, v_len).to(device)\n",
        "        self.c = torch.randn(1, h_len).to(device)\n",
        "        self.V = visible_nodes.to(device)\n",
        "        self.lr_W = lr_W\n",
        "        self.lr_bias = lr_bias        \n",
        "        \n",
        "    def get_h(self, v):\n",
        "        \n",
        "        a = torch.mm(v.view(1,-1), self.W) + self.c\n",
        "        f = torch.nn.Sigmoid()\n",
        "        p_h_v = f(a)\n",
        "        return p_h_v, torch.bernoulli(p_h_v)\n",
        "    \n",
        "    def get_v(self, h):\n",
        "        a = torch.mm(h.view(1,-1), self.W.T) + self.b\n",
        "        f = torch.nn.Sigmoid()\n",
        "        p_v_h = f(a)\n",
        "        return p_v_h, torch.bernoulli(p_v_h)\n",
        "    \n",
        "    def params_update(self, p_h_v0, p_h_vk, v0, vk):\n",
        "        self.W += self.lr_W*(torch.mm(v0.view(-1,1), p_h_v0) - torch.mm(vk.view(-1,1), p_h_vk))/self.N\n",
        "        self.b += self.lr_bias*(v0 - vk)/self.N\n",
        "        self.c += self.lr_bias*(p_h_v0 - p_h_vk)/self.N\n",
        "        \n",
        "    def one_epoch(self, k):\n",
        "\n",
        "        for v0 in self.V:\n",
        "            v_t = v0\n",
        "            for t in range(k):  \n",
        "                p_h_vt, h_t = self.get_h(v_t)\n",
        "                if t==0:\n",
        "                    p_h_v0 = p_h_vt                    \n",
        "                p_v_ht, v_t1 = self.get_v(h_t)\n",
        "                v_t = v_t1\n",
        "\n",
        "            try:\n",
        "                V_k = torch.cat((V_k, v_t.view(1,-1)), dim=0)\n",
        "                H_k = torch.cat((H_k, h_t.view(1,-1)), dim=0)\n",
        "            except:\n",
        "                V_k = v_t.view(1,-1)\n",
        "                H_k = h_t.view(1,-1)\n",
        "\n",
        "            self.params_update(p_h_v0, p_h_vt, v0, v_t)\n",
        "\n",
        "        return V_k\n",
        "        \n",
        "    def train(self, k):\n",
        "        ep = 0\n",
        "        error_old = np.inf\n",
        "\n",
        "        max_ep = 100\n",
        "        while True: #ep <= max_ep:\n",
        "            ep += 1\n",
        "            ## Check if error should be SSE?\n",
        "            V_k, H_h = self.one_epoch(k)\n",
        "            error_new = torch.sum((V_k - self.V)**2) \n",
        "            error_new = error_new/V_k.shape[0]\n",
        "            print('Epoch: {0}, Error: {1}, Error diff :{2}'.format(ep, error_new, (error_old-error_new)/error_new))\n",
        "            \n",
        "            if abs(error_new - error_old)/error_new <= 1e-6:\n",
        "                print('Converged!')\n",
        "                self.V_train = V_k\n",
        "                self.H_train = H_k                \n",
        "                break\n",
        "            error_old = error_new"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0gjstgMLQKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FinalNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
        "        super(FinalNet, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "        self.out = nn.Linear(hidden_sizes[2], num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            y_score = self.forward(X)\n",
        "            y_pred = torch.argmax(y_score, axis=1)\n",
        "            \n",
        "        return y_pred   "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPghIPVKLQKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Gaussian_stacked_RBM(v, n_stacks, h_layers_len, learning_rates, k_list):\n",
        "    '''\n",
        "    Parameters:\n",
        "    ------------\n",
        "    v              - Input to RBM (visible nodes). Must be continuous valued, whitened.\n",
        "    n_stacks       - No. of RMBs to be stacked\n",
        "    h_layers_len   - List of no. of nodes in the hidden layer of each RMB\n",
        "    learning_rates - List of list of learning rates (for Weights, bias) for each RBM\n",
        "    k_list         - List of k values for each RBM\n",
        "     '''\n",
        "    \n",
        "    weights = []\n",
        "    biases = []\n",
        "    \n",
        "    print('------Gaussian Binary RBM------')\n",
        "    gaussain = GBRBM(v_whitened.to(device), h_layers_len[0], learning_rates[0][0], learning_rates[0][1])\n",
        "    gaussain.train(k_list[0])\n",
        "    weights.append(gaussain.W)\n",
        "    biases.append(gaussain.c)\n",
        "    v_new = gaussain.H_train\n",
        "    \n",
        "    for i in range(1, n_stacks):\n",
        "        print('------Binary Binary RBM {0}------'.format(i))\n",
        "        binary = BBRBM(v_new.to(device), h_layers_len[i], learning_rates[i][0], learning_rates[i][1])\n",
        "        binary.train(k[i])\n",
        "        weights.append(binary.W)\n",
        "        biases.append(binary.c)\n",
        "        v_new = binary.H_train\n",
        "        \n",
        "    return weights, biases"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvVXx3E8LQKF",
        "colab_type": "text"
      },
      "source": [
        "1. Finding $\\sigma^{2}$ for visible nodes\n",
        "2. Whitening of data for GBRMB?\n",
        "3. Choice of initial conditions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95qP365qLQKG",
        "colab_type": "text"
      },
      "source": [
        "### Data pre-processing - Whitening the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ofj9NA6Xz__",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67b07a0f-1aa6-4fc6-8d21-1d418b98f782"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49DYjAOeLQKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_dict = {\n",
        "    'tallbuilding': 1,\n",
        "    'mountain': 4,\n",
        "    'highway': 6,\n",
        "    'coast': 8,\n",
        "    'opencountry' : 3    \n",
        "}\n",
        "\n",
        "trainloader, testloader = train_test_loader('/content/drive/My Drive/Data_Set_1(Colored_Images)', label_dict, train_fraction=0.8, num_workers=0, bs_fraction = 1)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4Bh2WlcLQKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v = list(trainloader)[0][0]\n",
        "v_centered = v - torch.stack([torch.mean(v, 0)]*v.shape[0], dim=0)\n",
        "cov = torch.mm(v_centered.T, v_centered)/v_centered.shape[0]\n",
        "U, S, V = torch.svd(cov)\n",
        "v_whitened = torch.mm(v_centered, U)/torch.sqrt(S)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk5pvn_KXjAF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "768eae3e-f2a0-491c-e24c-47be9c62069a"
      },
      "source": [
        "v.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1408, 828])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_olJ81QeZGHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gaussian = GBRBM(v_whitened.to(device), 400, 0.001, 0.01)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7T-1OxNZWKa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "5ffcaf91-f615-46b8-969e-1b455767b681"
      },
      "source": [
        "gaussian.train(1500)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Error: 1405.65625, Error diff :inf\n",
            "Epoch: 2, Error: 1407.399169921875, Error diff :-0.001238397671841085\n",
            "Epoch: 3, Error: 1403.4114990234375, Error diff :0.002841412555426359\n",
            "Epoch: 4, Error: 1406.5238037109375, Error diff :-0.0022127635311335325\n",
            "Epoch: 5, Error: 1402.617431640625, Error diff :0.0027850589249283075\n",
            "Epoch: 6, Error: 1404.1947021484375, Error diff :-0.0011232562828809023\n",
            "Epoch: 7, Error: 1404.80908203125, Error diff :-0.000437340495409444\n",
            "Epoch: 8, Error: 1403.479736328125, Error diff :0.000947178399655968\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-586466e568a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgaussian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-bd93dbf53dfc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m## Check if error should be SSE?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mV_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0merror_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV_k\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-bd93dbf53dfc>\u001b[0m in \u001b[0;36mone_epoch\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0mp_h_v0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_h_vt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mv_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mv_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_t1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-bd93dbf53dfc>\u001b[0m in \u001b[0;36mget_v\u001b[0;34m(self, h)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mv_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mv_h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Y373tXvfLQKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "890cad00-045a-43ea-9ddb-6f953b91031b"
      },
      "source": [
        "n_stacks = 3\n",
        "h_layers_len = [800, 800, 600]\n",
        "learning_rates = [[0.0001, 0.001]*3]\n",
        "k_list = [100]*3\n",
        "\n",
        "weights_pre_trained, biases_pre_trained = Gaussian_stacked_RBM(v_whitened, n_stacks, h_layers_len, learning_rates, k_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------Gaussian Binary RBM------\n",
            "Epoch: 1, Error: 1985519.5, Error diff :inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ebe7aa6f1165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mk_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mweights_pre_trained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases_pre_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussian_stacked_RBM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_whitened\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_stacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_layers_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-4d6770787983>\u001b[0m in \u001b[0;36mGaussian_stacked_RBM\u001b[0;34m(v, n_stacks, h_layers_len, learning_rates, k_list)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'------Gaussian Binary RBM------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mgaussain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGBRBM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_whitened\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_layers_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mgaussain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgaussain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mbiases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgaussain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-87dc88dab440>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m## Check if error should be SSE?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mV_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0merror_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV_k\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-87dc88dab440>\u001b[0m in \u001b[0;36mone_epoch\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0mp_h_v0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_h_vt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mv_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mv_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_t1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-87dc88dab440>\u001b[0m in \u001b[0;36mget_v\u001b[0;34m(self, h)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;31m# mean of normal dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-PXOPrgLQKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = FinalNet(v.shape[1], h_layers_len, len(np.unique(np.array(trainloader)[0][1]))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sjnB7v_LQKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    classifier.fc1.weight.data = nn.Parameter(weights_pre_trained[0].t())\n",
        "    classifier.fc1.bias.data = nn.Parameter(biases_pre_trained[0].squeeze(0))\n",
        "    \n",
        "    classifier.fc2.weight = nn.Parameter(weights_pre_trained[1].t())\n",
        "    classifier.fc2.bias = nn.Parameter(biases_pre_trained[1].squeeze(0))\n",
        "    \n",
        "    classifier.fc3.weight = nn.Parameter(weights_pre_trained[2].t())\n",
        "    classifier.fc3.bias = nn.Parameter(biases_pre_trained[2].squeeze(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4urK_3bWLQKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
        "classifier = classifier.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEVCDLfwLQKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainloader, testloader = train_test_loader('Data_Set_1(Colored_Images)', label_dict, train_fraction=0.8, num_workers=0, bs_fraction = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MdBkNmCLQKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "old_loss = np.inf\n",
        "\n",
        "max_epoch = 500\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for data in trainloader:\n",
        "        \n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward\n",
        "        y_hat = classifier(X)\n",
        "        \n",
        "        # Calculate Loss (Cross Entropy)\n",
        "        loss = criterion(y_hat, y)\n",
        "        \n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update Parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
        "    \n",
        "    if abs(running_loss-old_loss)/running_loss < 1e-3:\n",
        "        print('Converged')\n",
        "        break\n",
        "    \n",
        "    old_loss = running_loss\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcTlhXV9QUtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    train_loss = 0.0\n",
        "    y_train = []\n",
        "    y_train_pred = []\n",
        "\n",
        "    for data in trainloader:\n",
        "\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        y_hat = classifier(X)      \n",
        "        train_loss += criterion(y_hat, y)\n",
        "        \n",
        "        y_train.extend(list(y.cpu().detach().numpy()))\n",
        "        y_train_pred.extend(list(torch.argmax(y_hat, axis=1).cpu().detach().numpy()))\n",
        "\n",
        "print('Train Loss =', train_loss.item())\n",
        "pd.DataFrame(confusion_matrix(y_train, y_train_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C_sLteiQQIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    test_loss = 0.0\n",
        "    y_test = []\n",
        "    y_test_pred = []\n",
        "\n",
        "    for data in testloader:\n",
        "\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        y_hat = classifier(X)      \n",
        "        test_loss += criterion(y_hat, y)\n",
        "        \n",
        "        y_test.extend(list(y.cpu().detach().numpy()))\n",
        "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).cpu().detach().numpy()))\n",
        "\n",
        "print('Test Loss =', test_loss.item())\n",
        "pd.DataFrame(confusion_matrix(y_test, y_test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-FgwrVMLQKa",
        "colab_type": "text"
      },
      "source": [
        "## ---------------------- End of code ---------------------- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c839JofLqesO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi73JCIBLQKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vk = gaussian.train(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INKq4_IfLQKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vk = gaussian.train(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjthMlyCLQKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vk = gaussian.train(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM8g_WASLQKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# v_len = 828\n",
        "# h_len = 1000\n",
        "# sigma2 = torch.var(v_whitened, 0)\n",
        "# W = torch.empty(v_len, h_len).uniform_(-sqrt(6/(v_len+h_len)), sqrt(6/(v_len+h_len))).to(device)\n",
        "# b = torch.mean(v_whitened, axis=0).view(1,-1).to(device)\n",
        "# c = torch.tensor([((torch.norm(b + W[:, i])**2 - torch.norm(b)**2)/(2*torch.var(v_whitened)) +  log(0.01)).item() for i in range(h_len)]).view(1,-1).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}