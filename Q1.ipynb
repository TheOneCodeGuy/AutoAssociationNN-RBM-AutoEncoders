{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from io import StringIO\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetClass(Dataset):\n",
    "    \n",
    "    def __init__(self, folder, filename, label_dict):\n",
    "        \n",
    "        self.data = []\n",
    "        self.filename = filename\n",
    "        tar = tarfile.open(folder + '\\\\' + filename)\n",
    "        for file in tar.getmembers():\n",
    "            f = tar.extractfile(file)\n",
    "            if f != None:\n",
    "                content = pd.read_csv(StringIO(f.read().decode()), sep=' ', header=None).values.ravel()\n",
    "                self.data.append(content)\n",
    "            \n",
    "        self.y = torch.tensor(label_dict[self.filename[:-7]], dtype=torch.long)\n",
    "    \n",
    "    def __getitem__(self, idx):     \n",
    "        \n",
    "        return torch.tensor(self.data[idx], dtype=torch.float), self.y\n",
    "      \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_loader(directory, label_dict, train_fraction=0.8, num_workers=2, batch_size=32):\n",
    "\n",
    "    all_files = list(filter(lambda x: x.endswith('.tar.gz'), os.listdir(directory)))\n",
    "    files = [file for file in all_files if file[:-7] in label_dict.keys()]\n",
    "    \n",
    "    datasets = list(map(lambda x : DatasetClass(directory, x, label_dict), files))\n",
    "    dataset = ConcatDataset(datasets)\n",
    "    N = dataset.cumulative_sizes[-1]\n",
    "    \n",
    "    train_size = int(N*train_fraction)\n",
    "    test_size = N - train_size\n",
    "\n",
    "    train_data, test_data = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'tallbuilding': 0, 'opencountry':1, 'mountain': 2, 'highway': 3, 'coast': 4}\n",
    "trainloader, testloader = train_test_loader('Data_Set_1(Colored_Images)', label_dict, train_fraction=0.8, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, h_layer_sizes):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_features, h_layer_sizes[0])\n",
    "        self.fc2 = nn.Linear(h_layer_sizes[0], h_layer_sizes[1])\n",
    "        self.fc3 = nn.Linear(h_layer_sizes[1], h_layer_sizes[2])\n",
    "        self.out = nn.Linear(h_layer_sizes[2], n_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.tanh(self.fc1(x)) # Hidden Layer 1 (Tanh)\n",
    "        x = self.fc2(x)    # Hidden Layer 2 (Linear)\n",
    "        x = torch.tanh(self.fc3(x)) # Hidden Layer 3 (Tanh)\n",
    "        x = self.out(x) # Output Layer (Linear)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_z(self, x):\n",
    "        \n",
    "        z = torch.tanh(self.fc1(x))\n",
    "        z = self.fc2(z)\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae1 = AutoEncoder(828, [500, 300, 500])\n",
    "ae1 = ae1.to(device)\n",
    "optimizer1 = optim.SGD(ae1.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss = 82.11686182022095\n",
      "Epoch 2 : Loss = 72.09533536434174\n",
      "Epoch 3 : Loss = 55.275643706321716\n",
      "Epoch 4 : Loss = 35.821474611759186\n",
      "Epoch 5 : Loss = 21.7523130774498\n",
      "Epoch 6 : Loss = 14.383622944355011\n",
      "Epoch 7 : Loss = 11.11868867278099\n",
      "Epoch 8 : Loss = 9.768427357077599\n",
      "Epoch 9 : Loss = 9.233877211809158\n",
      "Epoch 10 : Loss = 9.020542934536934\n",
      "Epoch 11 : Loss = 8.934777528047562\n",
      "Epoch 12 : Loss = 8.899174153804779\n",
      "Epoch 13 : Loss = 8.883565053343773\n",
      "Epoch 14 : Loss = 8.875580057501793\n",
      "Converged\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "old_loss = np.inf\n",
    "\n",
    "max_epoch = 200\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, _ = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer1.zero_grad()\n",
    "        \n",
    "        # Reconstructed Representation of X (forward)\n",
    "        X_hat = ae1(X)\n",
    "        \n",
    "        # Calculate Loss (MSE)\n",
    "        loss = criterion(X_hat, X)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer1.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    \n",
    "    \n",
    "    if abs(running_loss-old_loss)/running_loss < 1e-3:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
    "        super(FinalNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "        #self.fc4 = nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
    "        self.out = nn.Linear(hidden_sizes[2], num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        #x = torch.tanh(self.fc4(x))\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_score = self.forward(X)\n",
    "            y_pred = torch.argmax(y_score, axis=1)\n",
    "            \n",
    "        return y_pred\n",
    "            \n",
    "    \n",
    "classifier = FinalNet(300, [150, 75, 50], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss = 70.59424698352814\n",
      "Epoch 2 : Loss = 70.35061419010162\n",
      "Epoch 3 : Loss = 70.36256313323975\n",
      "Epoch 4 : Loss = 70.31681597232819\n",
      "Epoch 5 : Loss = 70.32313930988312\n",
      "Converged\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "old_loss = np.inf\n",
    "\n",
    "max_epoch = 500\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        # extracting encoder features from AE1 to use as input to the MLFFNN\n",
    "        with torch.no_grad():\n",
    "            Z = ae1.get_z(X)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_hat = classifier(Z)\n",
    "        \n",
    "        # Calculate Loss (Cross Entropy)\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    \n",
    "    if abs(running_loss-old_loss)/running_loss < 1e-4:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 17.60600471496582\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1  2  3  4\n",
       "0  0  71  0  0  0\n",
       "1  0  85  0  0  0\n",
       "2  0  68  0  0  0\n",
       "3  0  55  0  0  0\n",
       "4  0  73  0  0  0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        Z = ae1.get_z(X)\n",
    "        y_hat = classifier(Z)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.cpu().detach().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).cpu().detach().numpy()))\n",
    "\n",
    "print('Test Loss =', test_loss.item())\n",
    "pd.DataFrame(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5950, 0.6597, 0.4282,  ..., 2.9600, 2.7177, 2.9838],\n",
      "        [0.5868, 0.4174, 0.8629,  ..., 3.3219, 3.1987, 3.3914],\n",
      "        [0.5960, 0.5398, 0.4418,  ..., 3.0622, 3.0685, 2.7189],\n",
      "        ...,\n",
      "        [0.6232, 0.6077, 0.6365,  ..., 3.1719, 2.7474, 2.9302],\n",
      "        [0.6060, 0.0340, 0.8770,  ..., 1.9181, 2.1452, 2.0354],\n",
      "        [0.5327, 0.5327, 0.4959,  ..., 2.1170, 2.1116, 2.0841]])\n",
      "torch.Size([1408, 828])\n"
     ]
    }
   ],
   "source": [
    "# To fit the PCA model we load all the training points in a single batch\n",
    "train, test = train_test_loader('Data_Set_1(Colored_Images)', label_dict, train_fraction=0.8, batch_size=2000, num_workers=0)\n",
    "\n",
    "for i in train:\n",
    "    print(i[0])\n",
    "    temp = i[0]\n",
    "    print(i[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_pca(temp):\n",
    "    pca = PCA(n_components=0.99)\n",
    "    pca.fit(temp)\n",
    "    return pca\n",
    "\n",
    "PCA_model = return_pca(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_dimension = PCA_model.transform(temp).shape[1]\n",
    "pca_clf = FinalNet(reduced_dimension, [150, 75, 50], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(pca_clf.parameters(), lr=0.001, momentum=0.9)\n",
    "pca_clf = pca_clf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss = 70.95451056957245\n",
      "Epoch 2 : Loss = 69.7358535528183\n",
      "Epoch 3 : Loss = 68.4542692899704\n",
      "Epoch 4 : Loss = 67.03552746772766\n",
      "Epoch 5 : Loss = 65.47527801990509\n",
      "Epoch 6 : Loss = 63.835047245025635\n",
      "Epoch 7 : Loss = 62.26478052139282\n",
      "Epoch 8 : Loss = 60.82287073135376\n",
      "Epoch 9 : Loss = 59.50989067554474\n",
      "Epoch 10 : Loss = 58.211318373680115\n",
      "Epoch 11 : Loss = 56.8797789812088\n",
      "Epoch 12 : Loss = 55.472567319869995\n",
      "Epoch 13 : Loss = 53.97001278400421\n",
      "Epoch 14 : Loss = 52.46879464387894\n",
      "Epoch 15 : Loss = 50.976709485054016\n",
      "Epoch 16 : Loss = 49.48734086751938\n",
      "Epoch 17 : Loss = 48.13438868522644\n",
      "Epoch 18 : Loss = 46.83879017829895\n",
      "Epoch 19 : Loss = 45.588482558727264\n",
      "Epoch 20 : Loss = 44.41742658615112\n",
      "Epoch 21 : Loss = 43.343338429927826\n",
      "Epoch 22 : Loss = 42.2998605966568\n",
      "Epoch 23 : Loss = 41.31622505187988\n",
      "Epoch 24 : Loss = 40.40190762281418\n",
      "Epoch 25 : Loss = 39.50726789236069\n",
      "Epoch 26 : Loss = 38.62200117111206\n",
      "Epoch 27 : Loss = 37.84256637096405\n",
      "Epoch 28 : Loss = 37.05637961626053\n",
      "Epoch 29 : Loss = 36.28230905532837\n",
      "Epoch 30 : Loss = 35.5137745141983\n",
      "Epoch 31 : Loss = 34.82827228307724\n",
      "Epoch 32 : Loss = 34.085165083408356\n",
      "Epoch 33 : Loss = 33.37642019987106\n",
      "Epoch 34 : Loss = 32.67411780357361\n",
      "Epoch 35 : Loss = 32.02209788560867\n",
      "Epoch 36 : Loss = 31.369804710149765\n",
      "Epoch 37 : Loss = 30.742548048496246\n",
      "Epoch 38 : Loss = 30.204615473747253\n",
      "Epoch 39 : Loss = 29.510759592056274\n",
      "Epoch 40 : Loss = 28.932008653879166\n",
      "Epoch 41 : Loss = 28.379074037075043\n",
      "Epoch 42 : Loss = 27.773566901683807\n",
      "Epoch 43 : Loss = 27.20142924785614\n",
      "Epoch 44 : Loss = 26.66573703289032\n",
      "Epoch 45 : Loss = 26.200702041387558\n",
      "Epoch 46 : Loss = 25.697425991296768\n",
      "Epoch 47 : Loss = 25.133202880620956\n",
      "Epoch 48 : Loss = 24.701250284910202\n",
      "Epoch 49 : Loss = 24.13628414273262\n",
      "Epoch 50 : Loss = 23.780732810497284\n",
      "Epoch 51 : Loss = 23.2531541287899\n",
      "Epoch 52 : Loss = 22.757634073495865\n",
      "Epoch 53 : Loss = 22.2140451669693\n",
      "Epoch 54 : Loss = 21.758029609918594\n",
      "Epoch 55 : Loss = 21.360473960638046\n",
      "Epoch 56 : Loss = 20.88480305671692\n",
      "Epoch 57 : Loss = 20.430402785539627\n",
      "Epoch 58 : Loss = 20.007402807474136\n",
      "Epoch 59 : Loss = 19.712262704968452\n",
      "Epoch 60 : Loss = 19.24570032954216\n",
      "Epoch 61 : Loss = 18.798228919506073\n",
      "Epoch 62 : Loss = 18.32073649764061\n",
      "Epoch 63 : Loss = 18.015824511647224\n",
      "Epoch 64 : Loss = 17.53254798054695\n",
      "Epoch 65 : Loss = 17.20697683095932\n",
      "Epoch 66 : Loss = 16.822635740041733\n",
      "Epoch 67 : Loss = 16.431872189044952\n",
      "Epoch 68 : Loss = 16.044832780957222\n",
      "Epoch 69 : Loss = 15.594757571816444\n",
      "Epoch 70 : Loss = 15.262297928333282\n",
      "Epoch 71 : Loss = 14.825395256280899\n",
      "Epoch 72 : Loss = 14.456331983208656\n",
      "Epoch 73 : Loss = 14.11329174041748\n",
      "Epoch 74 : Loss = 13.755029425024986\n",
      "Epoch 75 : Loss = 13.349999725818634\n",
      "Epoch 76 : Loss = 12.948819905519485\n",
      "Epoch 77 : Loss = 12.711758747696877\n",
      "Epoch 78 : Loss = 12.383322432637215\n",
      "Epoch 79 : Loss = 11.970160275697708\n",
      "Epoch 80 : Loss = 11.634594671428204\n",
      "Epoch 81 : Loss = 11.27897061407566\n",
      "Epoch 82 : Loss = 10.951985403895378\n",
      "Epoch 83 : Loss = 10.624330170452595\n",
      "Epoch 84 : Loss = 10.259473770856857\n",
      "Epoch 85 : Loss = 9.945656843483448\n",
      "Epoch 86 : Loss = 9.721420593559742\n",
      "Epoch 87 : Loss = 9.358795627951622\n",
      "Epoch 88 : Loss = 9.030800573527813\n",
      "Epoch 89 : Loss = 8.740098245441914\n",
      "Epoch 90 : Loss = 8.442839808762074\n",
      "Epoch 91 : Loss = 8.161537155508995\n",
      "Epoch 92 : Loss = 7.9676220044493675\n",
      "Epoch 93 : Loss = 7.646916382014751\n",
      "Epoch 94 : Loss = 7.392300330102444\n",
      "Epoch 95 : Loss = 7.0596636682748795\n",
      "Epoch 96 : Loss = 6.832881689071655\n",
      "Epoch 97 : Loss = 6.5803932920098305\n",
      "Epoch 98 : Loss = 6.324029266834259\n",
      "Epoch 99 : Loss = 6.115430653095245\n",
      "Epoch 100 : Loss = 5.896960757672787\n",
      "Epoch 101 : Loss = 5.669733241200447\n",
      "Epoch 102 : Loss = 5.487360179424286\n",
      "Epoch 103 : Loss = 5.251052163541317\n",
      "Epoch 104 : Loss = 5.006873480975628\n",
      "Epoch 105 : Loss = 4.792461015284061\n",
      "Epoch 106 : Loss = 4.656745217740536\n",
      "Epoch 107 : Loss = 4.463711217045784\n",
      "Epoch 108 : Loss = 4.288905434310436\n",
      "Epoch 109 : Loss = 4.096862010657787\n",
      "Epoch 110 : Loss = 3.949668161571026\n",
      "Epoch 111 : Loss = 3.8033916354179382\n",
      "Epoch 112 : Loss = 3.651132218539715\n",
      "Epoch 113 : Loss = 3.4987905025482178\n",
      "Epoch 114 : Loss = 3.37489253282547\n",
      "Epoch 115 : Loss = 3.23533383756876\n",
      "Epoch 116 : Loss = 3.136144295334816\n",
      "Epoch 117 : Loss = 2.9871192947030067\n",
      "Epoch 118 : Loss = 2.88593203574419\n",
      "Epoch 119 : Loss = 2.794778861105442\n",
      "Epoch 120 : Loss = 2.693739026784897\n",
      "Epoch 121 : Loss = 2.6035828217864037\n",
      "Epoch 122 : Loss = 2.5018910244107246\n",
      "Epoch 123 : Loss = 2.420481599867344\n",
      "Epoch 124 : Loss = 2.3296222910284996\n",
      "Epoch 125 : Loss = 2.2523137480020523\n",
      "Epoch 126 : Loss = 2.179325632750988\n",
      "Epoch 127 : Loss = 2.123035781085491\n",
      "Epoch 128 : Loss = 2.041593924164772\n",
      "Epoch 129 : Loss = 1.961446188390255\n",
      "Epoch 130 : Loss = 1.9123723730444908\n",
      "Epoch 131 : Loss = 1.8529371917247772\n",
      "Epoch 132 : Loss = 1.8058286905288696\n",
      "Epoch 133 : Loss = 1.7542391493916512\n",
      "Epoch 134 : Loss = 1.687801606953144\n",
      "Epoch 135 : Loss = 1.637828953564167\n",
      "Epoch 136 : Loss = 1.595469243824482\n",
      "Epoch 137 : Loss = 1.5472303852438927\n",
      "Epoch 138 : Loss = 1.5005038157105446\n",
      "Epoch 139 : Loss = 1.4677014574408531\n",
      "Epoch 140 : Loss = 1.4282401278614998\n",
      "Epoch 141 : Loss = 1.384096808731556\n",
      "Epoch 142 : Loss = 1.3615679070353508\n",
      "Epoch 143 : Loss = 1.3181700631976128\n",
      "Epoch 144 : Loss = 1.2856667563319206\n",
      "Epoch 145 : Loss = 1.251700222492218\n",
      "Epoch 146 : Loss = 1.2174582779407501\n",
      "Epoch 147 : Loss = 1.1911209598183632\n",
      "Epoch 148 : Loss = 1.1614343971014023\n",
      "Epoch 149 : Loss = 1.1344573721289635\n",
      "Epoch 150 : Loss = 1.1122102662920952\n",
      "Epoch 151 : Loss = 1.0873822271823883\n",
      "Epoch 152 : Loss = 1.0598224997520447\n",
      "Epoch 153 : Loss = 1.0343241095542908\n",
      "Epoch 154 : Loss = 1.0126738101243973\n",
      "Epoch 155 : Loss = 0.9906025230884552\n",
      "Epoch 156 : Loss = 0.9699446335434914\n",
      "Epoch 157 : Loss = 0.9499623626470566\n",
      "Epoch 158 : Loss = 0.9308697134256363\n",
      "Epoch 159 : Loss = 0.9105068370699883\n",
      "Epoch 160 : Loss = 0.8916716873645782\n",
      "Epoch 161 : Loss = 0.8739906921982765\n",
      "Epoch 162 : Loss = 0.8589896857738495\n",
      "Epoch 163 : Loss = 0.8423609063029289\n",
      "Epoch 164 : Loss = 0.8264977186918259\n",
      "Epoch 165 : Loss = 0.8108161836862564\n",
      "Epoch 166 : Loss = 0.796076811850071\n",
      "Epoch 167 : Loss = 0.7797002345323563\n",
      "Epoch 168 : Loss = 0.7660726085305214\n",
      "Epoch 169 : Loss = 0.7507349029183388\n",
      "Epoch 170 : Loss = 0.7407377362251282\n",
      "Epoch 171 : Loss = 0.7271143421530724\n",
      "Epoch 172 : Loss = 0.7145466953516006\n",
      "Epoch 173 : Loss = 0.7029622942209244\n",
      "Epoch 174 : Loss = 0.690142922103405\n",
      "Epoch 175 : Loss = 0.6773826777935028\n",
      "Epoch 176 : Loss = 0.6648865714669228\n",
      "Epoch 177 : Loss = 0.6552950665354729\n",
      "Epoch 178 : Loss = 0.6438339799642563\n",
      "Epoch 179 : Loss = 0.6336801871657372\n",
      "Epoch 180 : Loss = 0.6237619891762733\n",
      "Epoch 181 : Loss = 0.6146716922521591\n",
      "Epoch 182 : Loss = 0.6050546914339066\n",
      "Epoch 183 : Loss = 0.5956095829606056\n",
      "Epoch 184 : Loss = 0.5871144160628319\n",
      "Epoch 185 : Loss = 0.5768454745411873\n",
      "Epoch 186 : Loss = 0.569532573223114\n",
      "Epoch 187 : Loss = 0.5608156621456146\n",
      "Epoch 188 : Loss = 0.5526875033974648\n",
      "Epoch 189 : Loss = 0.5447044521570206\n",
      "Epoch 190 : Loss = 0.5368265360593796\n",
      "Epoch 191 : Loss = 0.5293911918997765\n",
      "Epoch 192 : Loss = 0.5214450135827065\n",
      "Epoch 193 : Loss = 0.5145349130034447\n",
      "Epoch 194 : Loss = 0.5075175389647484\n",
      "Epoch 195 : Loss = 0.5016360804438591\n",
      "Epoch 196 : Loss = 0.4937436878681183\n",
      "Epoch 197 : Loss = 0.48839980363845825\n",
      "Epoch 198 : Loss = 0.4816862791776657\n",
      "Epoch 199 : Loss = 0.4760023057460785\n",
      "Epoch 200 : Loss = 0.46969254314899445\n",
      "Epoch 201 : Loss = 0.4636422246694565\n",
      "Epoch 202 : Loss = 0.45718473196029663\n",
      "Epoch 203 : Loss = 0.4521501362323761\n",
      "Epoch 204 : Loss = 0.4461566209793091\n",
      "Epoch 205 : Loss = 0.4411657750606537\n",
      "Epoch 206 : Loss = 0.4349626749753952\n",
      "Epoch 207 : Loss = 0.4297499656677246\n",
      "Epoch 208 : Loss = 0.42422550916671753\n",
      "Epoch 209 : Loss = 0.4200242608785629\n",
      "Epoch 210 : Loss = 0.41468557715415955\n",
      "Epoch 211 : Loss = 0.4101172089576721\n",
      "Epoch 212 : Loss = 0.4052865207195282\n",
      "Epoch 213 : Loss = 0.40083639323711395\n",
      "Epoch 214 : Loss = 0.39592859148979187\n",
      "Epoch 215 : Loss = 0.3919030874967575\n",
      "Epoch 216 : Loss = 0.3864884525537491\n",
      "Epoch 217 : Loss = 0.38306769728660583\n",
      "Epoch 218 : Loss = 0.3785717189311981\n",
      "Epoch 219 : Loss = 0.37457309663295746\n",
      "Epoch 220 : Loss = 0.37087276577949524\n",
      "Epoch 221 : Loss = 0.36646410822868347\n",
      "Epoch 222 : Loss = 0.3626435250043869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223 : Loss = 0.35886241495609283\n",
      "Epoch 224 : Loss = 0.3549400568008423\n",
      "Epoch 225 : Loss = 0.35141371190547943\n",
      "Epoch 226 : Loss = 0.3475446403026581\n",
      "Epoch 227 : Loss = 0.34422467648983\n",
      "Epoch 228 : Loss = 0.3403140902519226\n",
      "Epoch 229 : Loss = 0.33684442937374115\n",
      "Epoch 230 : Loss = 0.3338111937046051\n",
      "Epoch 231 : Loss = 0.3304840326309204\n",
      "Epoch 232 : Loss = 0.327055424451828\n",
      "Epoch 233 : Loss = 0.3238407224416733\n",
      "Epoch 234 : Loss = 0.32101453840732574\n",
      "Epoch 235 : Loss = 0.3179997205734253\n",
      "Epoch 236 : Loss = 0.3152262568473816\n",
      "Epoch 237 : Loss = 0.31195494532585144\n",
      "Epoch 238 : Loss = 0.30859918892383575\n",
      "Epoch 239 : Loss = 0.3060655891895294\n",
      "Epoch 240 : Loss = 0.3031214028596878\n",
      "Epoch 241 : Loss = 0.30054140090942383\n",
      "Epoch 242 : Loss = 0.2977461963891983\n",
      "Epoch 243 : Loss = 0.29483582079410553\n",
      "Epoch 244 : Loss = 0.2920280396938324\n",
      "Epoch 245 : Loss = 0.28970593214035034\n",
      "Epoch 246 : Loss = 0.28703485429286957\n",
      "Epoch 247 : Loss = 0.284223273396492\n",
      "Epoch 248 : Loss = 0.2820495516061783\n",
      "Epoch 249 : Loss = 0.27943339943885803\n",
      "Epoch 250 : Loss = 0.27732063829898834\n",
      "Epoch 251 : Loss = 0.27477870881557465\n",
      "Epoch 252 : Loss = 0.2724025547504425\n",
      "Epoch 253 : Loss = 0.27031126618385315\n",
      "Epoch 254 : Loss = 0.26791518926620483\n",
      "Epoch 255 : Loss = 0.26549509167671204\n",
      "Epoch 256 : Loss = 0.2636376917362213\n",
      "Epoch 257 : Loss = 0.2611231654882431\n",
      "Epoch 258 : Loss = 0.2588217556476593\n",
      "Epoch 259 : Loss = 0.25696517527103424\n",
      "Epoch 260 : Loss = 0.2547735571861267\n",
      "Epoch 261 : Loss = 0.25254829227924347\n",
      "Epoch 262 : Loss = 0.2508189380168915\n",
      "Epoch 263 : Loss = 0.24853451550006866\n",
      "Epoch 264 : Loss = 0.24660585820674896\n",
      "Epoch 265 : Loss = 0.24465839564800262\n",
      "Epoch 266 : Loss = 0.24303142726421356\n",
      "Epoch 267 : Loss = 0.24097347259521484\n",
      "Epoch 268 : Loss = 0.2391662746667862\n",
      "Epoch 269 : Loss = 0.23734013736248016\n",
      "Epoch 270 : Loss = 0.23528516292572021\n",
      "Epoch 271 : Loss = 0.23368002474308014\n",
      "Epoch 272 : Loss = 0.2318711280822754\n",
      "Epoch 273 : Loss = 0.22998301684856415\n",
      "Epoch 274 : Loss = 0.22850416600704193\n",
      "Epoch 275 : Loss = 0.22702990472316742\n",
      "Epoch 276 : Loss = 0.2250329852104187\n",
      "Epoch 277 : Loss = 0.22342348098754883\n",
      "Epoch 278 : Loss = 0.22177663445472717\n",
      "Epoch 279 : Loss = 0.2204163819551468\n",
      "Epoch 280 : Loss = 0.21838246285915375\n",
      "Epoch 281 : Loss = 0.21707308292388916\n",
      "Epoch 282 : Loss = 0.21551638841629028\n",
      "Epoch 283 : Loss = 0.21398702263832092\n",
      "Epoch 284 : Loss = 0.21246585249900818\n",
      "Epoch 285 : Loss = 0.2111215591430664\n",
      "Epoch 286 : Loss = 0.20938341319561005\n",
      "Epoch 287 : Loss = 0.20801907777786255\n",
      "Epoch 288 : Loss = 0.20686094462871552\n",
      "Epoch 289 : Loss = 0.20513421297073364\n",
      "Epoch 290 : Loss = 0.20371407270431519\n",
      "Epoch 291 : Loss = 0.20231279730796814\n",
      "Epoch 292 : Loss = 0.20103250443935394\n",
      "Epoch 293 : Loss = 0.19983667135238647\n",
      "Epoch 294 : Loss = 0.19836723804473877\n",
      "Epoch 295 : Loss = 0.19701598584651947\n",
      "Epoch 296 : Loss = 0.19580255448818207\n",
      "Epoch 297 : Loss = 0.19441503286361694\n",
      "Epoch 298 : Loss = 0.19314727187156677\n",
      "Epoch 299 : Loss = 0.19194717705249786\n",
      "Epoch 300 : Loss = 0.1907774955034256\n",
      "Epoch 301 : Loss = 0.18935757875442505\n",
      "Epoch 302 : Loss = 0.18820953369140625\n",
      "Epoch 303 : Loss = 0.18720898032188416\n",
      "Epoch 304 : Loss = 0.18599563837051392\n",
      "Epoch 305 : Loss = 0.18464334309101105\n",
      "Epoch 306 : Loss = 0.18355268239974976\n",
      "Epoch 307 : Loss = 0.182416170835495\n",
      "Epoch 308 : Loss = 0.1812998205423355\n",
      "Epoch 309 : Loss = 0.18009842932224274\n",
      "Epoch 310 : Loss = 0.17903223633766174\n",
      "Epoch 311 : Loss = 0.17798209190368652\n",
      "Epoch 312 : Loss = 0.17685171961784363\n",
      "Epoch 313 : Loss = 0.17585964500904083\n",
      "Epoch 314 : Loss = 0.17480528354644775\n",
      "Epoch 315 : Loss = 0.17361962795257568\n",
      "Epoch 316 : Loss = 0.17265252768993378\n",
      "Epoch 317 : Loss = 0.17169128358364105\n",
      "Epoch 318 : Loss = 0.17065590620040894\n",
      "Epoch 319 : Loss = 0.16969314217567444\n",
      "Epoch 320 : Loss = 0.1686248928308487\n",
      "Epoch 321 : Loss = 0.16765888035297394\n",
      "Epoch 322 : Loss = 0.16662847995758057\n",
      "Epoch 323 : Loss = 0.16568511724472046\n",
      "Epoch 324 : Loss = 0.16476809978485107\n",
      "Epoch 325 : Loss = 0.16379033029079437\n",
      "Epoch 326 : Loss = 0.1628778874874115\n",
      "Epoch 327 : Loss = 0.16189329326152802\n",
      "Epoch 328 : Loss = 0.16101747751235962\n",
      "Epoch 329 : Loss = 0.1601373702287674\n",
      "Epoch 330 : Loss = 0.15919849276542664\n",
      "Epoch 331 : Loss = 0.15844231843948364\n",
      "Epoch 332 : Loss = 0.157477468252182\n",
      "Epoch 333 : Loss = 0.15661805868148804\n",
      "Epoch 334 : Loss = 0.15568119287490845\n",
      "Epoch 335 : Loss = 0.15489745140075684\n",
      "Epoch 336 : Loss = 0.15414473414421082\n",
      "Epoch 337 : Loss = 0.15323340892791748\n",
      "Epoch 338 : Loss = 0.1524336189031601\n",
      "Epoch 339 : Loss = 0.15160514414310455\n",
      "Epoch 340 : Loss = 0.15072959661483765\n",
      "Epoch 341 : Loss = 0.14989040791988373\n",
      "Epoch 342 : Loss = 0.14918257296085358\n",
      "Epoch 343 : Loss = 0.14843519032001495\n",
      "Epoch 344 : Loss = 0.14759132266044617\n",
      "Epoch 345 : Loss = 0.1468416154384613\n",
      "Epoch 346 : Loss = 0.1460486203432083\n",
      "Epoch 347 : Loss = 0.14530602097511292\n",
      "Epoch 348 : Loss = 0.14451344311237335\n",
      "Epoch 349 : Loss = 0.14373458921909332\n",
      "Epoch 350 : Loss = 0.1431836038827896\n",
      "Epoch 351 : Loss = 0.1423196643590927\n",
      "Epoch 352 : Loss = 0.1415998488664627\n",
      "Epoch 353 : Loss = 0.1408725380897522\n",
      "Epoch 354 : Loss = 0.14020486176013947\n",
      "Epoch 355 : Loss = 0.13943015038967133\n",
      "Epoch 356 : Loss = 0.1387849897146225\n",
      "Epoch 357 : Loss = 0.1380559504032135\n",
      "Epoch 358 : Loss = 0.13747072219848633\n",
      "Epoch 359 : Loss = 0.13673678040504456\n",
      "Epoch 360 : Loss = 0.13608476519584656\n",
      "Epoch 361 : Loss = 0.1353568136692047\n",
      "Epoch 362 : Loss = 0.13472521305084229\n",
      "Epoch 363 : Loss = 0.13408251106739044\n",
      "Epoch 364 : Loss = 0.13332721590995789\n",
      "Epoch 365 : Loss = 0.13276879489421844\n",
      "Epoch 366 : Loss = 0.1320991963148117\n",
      "Epoch 367 : Loss = 0.13152694702148438\n",
      "Epoch 368 : Loss = 0.13093450665473938\n",
      "Epoch 369 : Loss = 0.13019293546676636\n",
      "Epoch 370 : Loss = 0.12961313128471375\n",
      "Epoch 371 : Loss = 0.12901829183101654\n",
      "Epoch 372 : Loss = 0.12836748361587524\n",
      "Epoch 373 : Loss = 0.12776510417461395\n",
      "Epoch 374 : Loss = 0.12719106674194336\n",
      "Epoch 375 : Loss = 0.1266639679670334\n",
      "Epoch 376 : Loss = 0.12601101398468018\n",
      "Epoch 377 : Loss = 0.1254647672176361\n",
      "Epoch 378 : Loss = 0.12485407292842865\n",
      "Epoch 379 : Loss = 0.12433488667011261\n",
      "Epoch 380 : Loss = 0.12379750609397888\n",
      "Epoch 381 : Loss = 0.12316487729549408\n",
      "Epoch 382 : Loss = 0.12264737486839294\n",
      "Epoch 383 : Loss = 0.1220821738243103\n",
      "Epoch 384 : Loss = 0.12153187394142151\n",
      "Epoch 385 : Loss = 0.12105067074298859\n",
      "Epoch 386 : Loss = 0.12046515941619873\n",
      "Epoch 387 : Loss = 0.11992011964321136\n",
      "Epoch 388 : Loss = 0.1193937212228775\n",
      "Epoch 389 : Loss = 0.1188930869102478\n",
      "Epoch 390 : Loss = 0.11831720173358917\n",
      "Epoch 391 : Loss = 0.11781527101993561\n",
      "Epoch 392 : Loss = 0.11729401350021362\n",
      "Epoch 393 : Loss = 0.11677871644496918\n",
      "Epoch 394 : Loss = 0.11626757681369781\n",
      "Epoch 395 : Loss = 0.11581842601299286\n",
      "Epoch 396 : Loss = 0.11527428030967712\n",
      "Epoch 397 : Loss = 0.11479800939559937\n",
      "Epoch 398 : Loss = 0.1142934262752533\n",
      "Epoch 399 : Loss = 0.11382962763309479\n",
      "Epoch 400 : Loss = 0.11329902708530426\n",
      "Epoch 401 : Loss = 0.11282321810722351\n",
      "Epoch 402 : Loss = 0.11245445907115936\n",
      "Epoch 403 : Loss = 0.11189620196819305\n",
      "Epoch 404 : Loss = 0.11144836246967316\n",
      "Epoch 405 : Loss = 0.11097714304924011\n",
      "Epoch 406 : Loss = 0.11050258576869965\n",
      "Epoch 407 : Loss = 0.11008453369140625\n",
      "Epoch 408 : Loss = 0.10960268974304199\n",
      "Epoch 409 : Loss = 0.10914532840251923\n",
      "Epoch 410 : Loss = 0.10867592692375183\n",
      "Epoch 411 : Loss = 0.10826167464256287\n",
      "Epoch 412 : Loss = 0.10785016417503357\n",
      "Epoch 413 : Loss = 0.10740293562412262\n",
      "Epoch 414 : Loss = 0.10695803165435791\n",
      "Epoch 415 : Loss = 0.10649055242538452\n",
      "Epoch 416 : Loss = 0.1060924232006073\n",
      "Epoch 417 : Loss = 0.1056530624628067\n",
      "Epoch 418 : Loss = 0.10525371134281158\n",
      "Epoch 419 : Loss = 0.10486696660518646\n",
      "Epoch 420 : Loss = 0.10441438853740692\n",
      "Epoch 421 : Loss = 0.10398712754249573\n",
      "Epoch 422 : Loss = 0.10357104241847992\n",
      "Epoch 423 : Loss = 0.10313959419727325\n",
      "Epoch 424 : Loss = 0.10278475284576416\n",
      "Epoch 425 : Loss = 0.10235489904880524\n",
      "Epoch 426 : Loss = 0.10196134448051453\n",
      "Epoch 427 : Loss = 0.10158480703830719\n",
      "Epoch 428 : Loss = 0.10115595161914825\n",
      "Epoch 429 : Loss = 0.10081371665000916\n",
      "Epoch 430 : Loss = 0.10042701661586761\n",
      "Epoch 431 : Loss = 0.10003465414047241\n",
      "Epoch 432 : Loss = 0.09965907037258148\n",
      "Epoch 433 : Loss = 0.0993199348449707\n",
      "Epoch 434 : Loss = 0.09895002841949463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 435 : Loss = 0.09854684770107269\n",
      "Epoch 436 : Loss = 0.09817448258399963\n",
      "Epoch 437 : Loss = 0.09780050814151764\n",
      "Epoch 438 : Loss = 0.09739133715629578\n",
      "Epoch 439 : Loss = 0.0970243364572525\n",
      "Epoch 440 : Loss = 0.09668996930122375\n",
      "Epoch 441 : Loss = 0.09632909297943115\n",
      "Epoch 442 : Loss = 0.09596574306488037\n",
      "Epoch 443 : Loss = 0.09563899040222168\n",
      "Epoch 444 : Loss = 0.09527324140071869\n",
      "Epoch 445 : Loss = 0.09491516649723053\n",
      "Epoch 446 : Loss = 0.09455963969230652\n",
      "Epoch 447 : Loss = 0.09425465762615204\n",
      "Epoch 448 : Loss = 0.09386946260929108\n",
      "Epoch 449 : Loss = 0.09354498982429504\n",
      "Epoch 450 : Loss = 0.09320688247680664\n",
      "Epoch 451 : Loss = 0.09290160238742828\n",
      "Epoch 452 : Loss = 0.09254403412342072\n",
      "Epoch 453 : Loss = 0.09219805896282196\n",
      "Epoch 454 : Loss = 0.09190163016319275\n",
      "Epoch 455 : Loss = 0.09154480695724487\n",
      "Epoch 456 : Loss = 0.09124806523323059\n",
      "Epoch 457 : Loss = 0.09090901911258698\n",
      "Epoch 458 : Loss = 0.09062895178794861\n",
      "Epoch 459 : Loss = 0.09028442203998566\n",
      "Epoch 460 : Loss = 0.08995677530765533\n",
      "Epoch 461 : Loss = 0.08967554569244385\n",
      "Epoch 462 : Loss = 0.08932258188724518\n",
      "Epoch 463 : Loss = 0.0890330970287323\n",
      "Epoch 464 : Loss = 0.08870278298854828\n",
      "Epoch 465 : Loss = 0.08840207755565643\n",
      "Epoch 466 : Loss = 0.08810465037822723\n",
      "Epoch 467 : Loss = 0.0878085196018219\n",
      "Epoch 468 : Loss = 0.08749376237392426\n",
      "Epoch 469 : Loss = 0.08719021081924438\n",
      "Epoch 470 : Loss = 0.08689865469932556\n",
      "Epoch 471 : Loss = 0.08664809167385101\n",
      "Epoch 472 : Loss = 0.08637265861034393\n",
      "Epoch 473 : Loss = 0.08602967858314514\n",
      "Epoch 474 : Loss = 0.08574151992797852\n",
      "Epoch 475 : Loss = 0.08544334769248962\n",
      "Epoch 476 : Loss = 0.08515143394470215\n",
      "Epoch 477 : Loss = 0.08486732840538025\n",
      "Epoch 478 : Loss = 0.08459421992301941\n",
      "Epoch 479 : Loss = 0.08429165184497833\n",
      "Epoch 480 : Loss = 0.08403107523918152\n",
      "Epoch 481 : Loss = 0.08373592793941498\n",
      "Epoch 482 : Loss = 0.08345809578895569\n",
      "Epoch 483 : Loss = 0.0832080990076065\n",
      "Epoch 484 : Loss = 0.08291861414909363\n",
      "Epoch 485 : Loss = 0.08267723023891449\n",
      "Epoch 486 : Loss = 0.08237393200397491\n",
      "Epoch 487 : Loss = 0.0821213573217392\n",
      "Epoch 488 : Loss = 0.08185930550098419\n",
      "Epoch 489 : Loss = 0.08161148428916931\n",
      "Epoch 490 : Loss = 0.08133198320865631\n",
      "Epoch 491 : Loss = 0.08110438287258148\n",
      "Epoch 492 : Loss = 0.08080188930034637\n",
      "Epoch 493 : Loss = 0.08054980635643005\n",
      "Epoch 494 : Loss = 0.08032558858394623\n",
      "Epoch 495 : Loss = 0.08003045618534088\n",
      "Epoch 496 : Loss = 0.0797945111989975\n",
      "Epoch 497 : Loss = 0.07954476773738861\n",
      "Epoch 498 : Loss = 0.07928580045700073\n",
      "Epoch 499 : Loss = 0.07906250655651093\n",
      "Epoch 500 : Loss = 0.07879507541656494\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "old_loss = np.inf\n",
    "\n",
    "max_epoch = 500\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, y = data[0], data[1].to(device)\n",
    "        \n",
    "        # applying PCA on the data to use as input to the MLFFNN\n",
    "        Z = torch.Tensor(PCA_model.transform(X)).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_hat = pca_clf(Z)\n",
    "        \n",
    "        # Calculate Loss (Cross Entropy)\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    \n",
    "    if abs(running_loss-old_loss)/running_loss < 1e-4:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 27.340171813964844\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4\n",
       "0  44   4  16   4   3\n",
       "1   4  55  12   2  12\n",
       "2   9  13  36   4   6\n",
       "3   6   4   7  30   8\n",
       "4   5  12   8   7  41"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        X, y = data[0], data[1].to(device)\n",
    "        Z = torch.Tensor(PCA_model.transform(X)).to(device)\n",
    "        y_hat = pca_clf(Z)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.cpu().detach().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).cpu().detach().numpy()))\n",
    "\n",
    "print('Test Loss =', test_loss.item())\n",
    "pd.DataFrame(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
