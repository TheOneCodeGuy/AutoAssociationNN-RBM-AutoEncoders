{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from io import StringIO\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetClass(Dataset):\n",
    "    \n",
    "    def __init__(self, folder, filename, label_dict):\n",
    "        \n",
    "        self.data = []\n",
    "        self.filename = filename\n",
    "        tar = tarfile.open(folder + '\\\\' + filename)\n",
    "        for file in tar.getmembers():\n",
    "            f = tar.extractfile(file)\n",
    "            if f != None:\n",
    "                content = pd.read_csv(StringIO(f.read().decode()), sep=' ', header=None).values.ravel()\n",
    "                self.data.append(content)\n",
    "            \n",
    "        self.y = torch.tensor(label_dict[self.filename[:-7]], dtype=torch.long)\n",
    "    \n",
    "    def __getitem__(self, idx):     \n",
    "        \n",
    "        return torch.tensor(self.data[idx], dtype=torch.float), self.y\n",
    "      \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_loader(directory, label_dict, train_fraction=0.8, num_workers=2, batch_size=32):\n",
    "\n",
    "    all_files = list(filter(lambda x: x.endswith('.tar.gz'), os.listdir(directory)))\n",
    "    files = [file for file in all_files if file[:-7] in label_dict.keys()]\n",
    "    \n",
    "    datasets = list(map(lambda x : DatasetClass(directory, x, label_dict), files))\n",
    "    dataset = ConcatDataset(datasets)\n",
    "    N = dataset.cumulative_sizes[-1]\n",
    "    \n",
    "    train_size = int(N*train_fraction)\n",
    "    test_size = N - train_size\n",
    "\n",
    "    train_data, test_data = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'tallbuilding': 0, 'opencountry':1, 'mountain': 2, 'highway': 3, 'coast': 4}\n",
    "trainloader, testloader = train_test_loader('Data_Set_1(Colored_Images)', label_dict, train_fraction=0.8, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, h_layer_sizes):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_features, h_layer_sizes[0])\n",
    "        self.fc2 = nn.Linear(h_layer_sizes[0], h_layer_sizes[1])\n",
    "        self.fc3 = nn.Linear(h_layer_sizes[1], h_layer_sizes[2])\n",
    "        self.out = nn.Linear(h_layer_sizes[2], n_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.tanh(self.fc1(x)) # Hidden Layer 1 (Tanh)\n",
    "        x = self.fc2(x)    # Hidden Layer 2 (Linear)\n",
    "        x = torch.tanh(self.fc3(x)) # Hidden Layer 3 (Tanh)\n",
    "        x = self.out(x) # Output Layer (Linear)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_z(self, x):\n",
    "        \n",
    "        z = torch.tanh(self.fc1(x))\n",
    "        z = self.fc2(z)\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae1 = AutoEncoder(828, [500, 300, 500])\n",
    "ae1 = ae1.to(device)\n",
    "optimizer1 = optim.SGD(ae1.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss = 84.81388974189758\n",
      "Epoch 2 : Loss = 83.72193026542664\n",
      "Epoch 3 : Loss = 82.63125085830688\n",
      "Epoch 4 : Loss = 81.56800734996796\n",
      "Epoch 5 : Loss = 80.50918519496918\n",
      "Epoch 6 : Loss = 79.4323400259018\n",
      "Epoch 7 : Loss = 78.31987202167511\n",
      "Epoch 8 : Loss = 77.15656733512878\n",
      "Epoch 9 : Loss = 75.92866683006287\n",
      "Epoch 10 : Loss = 74.62778151035309\n",
      "Epoch 11 : Loss = 73.24534940719604\n",
      "Epoch 12 : Loss = 71.7760180234909\n",
      "Epoch 13 : Loss = 70.21788251399994\n",
      "Epoch 14 : Loss = 68.57295835018158\n",
      "Epoch 15 : Loss = 66.8405749797821\n",
      "Epoch 16 : Loss = 65.02913665771484\n",
      "Epoch 17 : Loss = 63.14270734786987\n",
      "Epoch 18 : Loss = 61.19146478176117\n",
      "Epoch 19 : Loss = 59.187352657318115\n",
      "Epoch 20 : Loss = 57.136245012283325\n",
      "Epoch 21 : Loss = 55.05608332157135\n",
      "Epoch 22 : Loss = 52.95425021648407\n",
      "Epoch 23 : Loss = 50.848626494407654\n",
      "Epoch 24 : Loss = 48.74791431427002\n",
      "Epoch 25 : Loss = 46.66660034656525\n",
      "Epoch 26 : Loss = 44.615538418293\n",
      "Epoch 27 : Loss = 42.6039200425148\n",
      "Epoch 28 : Loss = 40.6421115398407\n",
      "Epoch 29 : Loss = 38.738252103328705\n",
      "Epoch 30 : Loss = 36.89907377958298\n",
      "Epoch 31 : Loss = 35.128575801849365\n",
      "Epoch 32 : Loss = 33.43088936805725\n",
      "Epoch 33 : Loss = 31.811816453933716\n",
      "Epoch 34 : Loss = 30.266928672790527\n",
      "Epoch 35 : Loss = 28.804174602031708\n",
      "Epoch 36 : Loss = 27.418662428855896\n",
      "Epoch 37 : Loss = 26.10967093706131\n",
      "Epoch 38 : Loss = 24.877749621868134\n",
      "Epoch 39 : Loss = 23.720289409160614\n",
      "Epoch 40 : Loss = 22.63381353020668\n",
      "Epoch 41 : Loss = 21.617330759763718\n",
      "Epoch 42 : Loss = 20.666698098182678\n",
      "Epoch 43 : Loss = 19.779427140951157\n",
      "Epoch 44 : Loss = 18.953237026929855\n",
      "Epoch 45 : Loss = 18.183523774147034\n",
      "Epoch 46 : Loss = 17.468628853559494\n",
      "Epoch 47 : Loss = 16.803282111883163\n",
      "Epoch 48 : Loss = 16.187449425458908\n",
      "Epoch 49 : Loss = 15.615326136350632\n",
      "Epoch 50 : Loss = 15.08507975935936\n",
      "Epoch 51 : Loss = 14.595959842205048\n",
      "Epoch 52 : Loss = 14.14325487613678\n",
      "Epoch 53 : Loss = 13.72302520275116\n",
      "Epoch 54 : Loss = 13.335813164710999\n",
      "Epoch 55 : Loss = 12.978629738092422\n",
      "Epoch 56 : Loss = 12.648713827133179\n",
      "Epoch 57 : Loss = 12.34462058544159\n",
      "Epoch 58 : Loss = 12.06354235112667\n",
      "Epoch 59 : Loss = 11.804911777377129\n",
      "Epoch 60 : Loss = 11.566797196865082\n",
      "Epoch 61 : Loss = 11.347006186842918\n",
      "Epoch 62 : Loss = 11.144840195775032\n",
      "Epoch 63 : Loss = 10.958556681871414\n",
      "Epoch 64 : Loss = 10.78727701306343\n",
      "Epoch 65 : Loss = 10.62977783381939\n",
      "Epoch 66 : Loss = 10.48455061018467\n",
      "Epoch 67 : Loss = 10.350840061903\n",
      "Epoch 68 : Loss = 10.228509947657585\n",
      "Epoch 69 : Loss = 10.115261167287827\n",
      "Epoch 70 : Loss = 10.011662855744362\n",
      "Epoch 71 : Loss = 9.916290923953056\n",
      "Epoch 72 : Loss = 9.828328892588615\n",
      "Epoch 73 : Loss = 9.747814774513245\n",
      "Epoch 74 : Loss = 9.673692435026169\n",
      "Epoch 75 : Loss = 9.605619430541992\n",
      "Epoch 76 : Loss = 9.542793989181519\n",
      "Epoch 77 : Loss = 9.485510125756264\n",
      "Epoch 78 : Loss = 9.432414934039116\n",
      "Epoch 79 : Loss = 9.383802101016045\n",
      "Epoch 80 : Loss = 9.339312583208084\n",
      "Epoch 81 : Loss = 9.297927349805832\n",
      "Epoch 82 : Loss = 9.260280966758728\n",
      "Epoch 83 : Loss = 9.225734487175941\n",
      "Epoch 84 : Loss = 9.193665459752083\n",
      "Epoch 85 : Loss = 9.164425104856491\n",
      "Epoch 86 : Loss = 9.13739350438118\n",
      "Epoch 87 : Loss = 9.112581863999367\n",
      "Epoch 88 : Loss = 9.089822679758072\n",
      "Epoch 89 : Loss = 9.068876758217812\n",
      "Epoch 90 : Loss = 9.049553409218788\n",
      "Epoch 91 : Loss = 9.03186585009098\n",
      "Epoch 92 : Loss = 9.015558421611786\n",
      "Epoch 93 : Loss = 9.000543639063835\n",
      "Epoch 94 : Loss = 8.986675962805748\n",
      "Epoch 95 : Loss = 8.974027752876282\n",
      "Epoch 96 : Loss = 8.962338700890541\n",
      "Epoch 97 : Loss = 8.951548621058464\n",
      "Epoch 98 : Loss = 8.941639974713326\n",
      "Epoch 99 : Loss = 8.932482942938805\n",
      "Epoch 100 : Loss = 8.924065202474594\n",
      "Epoch 101 : Loss = 8.916333928704262\n",
      "Epoch 102 : Loss = 8.909126460552216\n",
      "Epoch 103 : Loss = 8.90258014202118\n",
      "Epoch 104 : Loss = 8.896509394049644\n",
      "Epoch 105 : Loss = 8.890893787145615\n",
      "Epoch 106 : Loss = 8.885692656040192\n",
      "Epoch 107 : Loss = 8.880892142653465\n",
      "Epoch 108 : Loss = 8.876502886414528\n",
      "Epoch 109 : Loss = 8.87238384783268\n",
      "Epoch 110 : Loss = 8.868663296103477\n",
      "Epoch 111 : Loss = 8.865128263831139\n",
      "Epoch 112 : Loss = 8.861883714795113\n",
      "Epoch 113 : Loss = 8.858939871191978\n",
      "Epoch 114 : Loss = 8.856134921312332\n",
      "Epoch 115 : Loss = 8.85357515513897\n",
      "Epoch 116 : Loss = 8.85115784406662\n",
      "Epoch 117 : Loss = 8.848938673734665\n",
      "Epoch 118 : Loss = 8.84689685702324\n",
      "Epoch 119 : Loss = 8.844979971647263\n",
      "Epoch 120 : Loss = 8.84319207072258\n",
      "Epoch 121 : Loss = 8.841534063220024\n",
      "Epoch 122 : Loss = 8.839951619505882\n",
      "Epoch 123 : Loss = 8.838547021150589\n",
      "Epoch 124 : Loss = 8.837179109454155\n",
      "Epoch 125 : Loss = 8.835927516222\n",
      "Epoch 126 : Loss = 8.834705248475075\n",
      "Epoch 127 : Loss = 8.833599343895912\n",
      "Epoch 128 : Loss = 8.832575976848602\n",
      "Epoch 129 : Loss = 8.831540793180466\n",
      "Epoch 130 : Loss = 8.830633789300919\n",
      "Epoch 131 : Loss = 8.82976958155632\n",
      "Converged\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "old_loss = np.inf\n",
    "\n",
    "max_epoch = 500\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, _ = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer1.zero_grad()\n",
    "        \n",
    "        # Reconstructed Representation of X (forward)\n",
    "        X_hat = ae1(X)\n",
    "        \n",
    "        # Calculate Loss (MSE)\n",
    "        loss = criterion(X_hat, X)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer1.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    \n",
    "    \n",
    "    if abs(running_loss-old_loss)/running_loss < 1e-4:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
    "        super(FinalNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "        #self.fc4 = nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
    "        self.out = nn.Linear(hidden_sizes[2], num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        #x = torch.tanh(self.fc4(x))\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_score = self.forward(X)\n",
    "            y_pred = torch.argmax(y_score, axis=1)\n",
    "            \n",
    "        return y_pred\n",
    "            \n",
    "    \n",
    "classifier = FinalNet(300, [150, 75, 50], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.00001, momentum=0.9)\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss = 3.2300708070397377\n",
      "Epoch 2 : Loss = 3.1674853786826134\n",
      "Epoch 3 : Loss = 3.1596838608384132\n",
      "Epoch 4 : Loss = 3.160363093018532\n",
      "Epoch 5 : Loss = 3.156294383108616\n",
      "Epoch 6 : Loss = 3.1562704667448997\n",
      "Epoch 7 : Loss = 3.1523110792040825\n",
      "Epoch 8 : Loss = 3.1593799591064453\n",
      "Epoch 9 : Loss = 3.1622075363993645\n",
      "Epoch 10 : Loss = 3.1569722816348076\n",
      "Epoch 11 : Loss = 3.1562914103269577\n",
      "Epoch 12 : Loss = 3.154237687587738\n",
      "Epoch 13 : Loss = 3.1552461609244347\n",
      "Epoch 14 : Loss = 3.1530437394976616\n",
      "Epoch 15 : Loss = 3.1548278108239174\n",
      "Epoch 16 : Loss = 3.155103400349617\n",
      "Epoch 17 : Loss = 3.155000627040863\n",
      "Epoch 18 : Loss = 3.156525269150734\n",
      "Epoch 19 : Loss = 3.1561090648174286\n",
      "Epoch 20 : Loss = 3.151229351758957\n",
      "Epoch 21 : Loss = 3.1527416929602623\n",
      "Epoch 22 : Loss = 3.1551481038331985\n",
      "Epoch 23 : Loss = 3.152328483760357\n",
      "Epoch 24 : Loss = 3.1599630266427994\n",
      "Epoch 25 : Loss = 3.1577599570155144\n",
      "Epoch 26 : Loss = 3.1541131362318993\n",
      "Epoch 27 : Loss = 3.1522035598754883\n",
      "Epoch 28 : Loss = 3.154135689139366\n",
      "Epoch 29 : Loss = 3.1558631658554077\n",
      "Epoch 30 : Loss = 3.150197707116604\n",
      "Epoch 31 : Loss = 3.1539879515767097\n",
      "Epoch 32 : Loss = 3.1500452756881714\n",
      "Epoch 33 : Loss = 3.157523527741432\n",
      "Epoch 34 : Loss = 3.152755483984947\n",
      "Epoch 35 : Loss = 3.1498182862997055\n",
      "Epoch 36 : Loss = 3.1518088281154633\n",
      "Epoch 37 : Loss = 3.1541629284620285\n",
      "Epoch 38 : Loss = 3.1507255360484123\n",
      "Epoch 39 : Loss = 3.1539467573165894\n",
      "Epoch 40 : Loss = 3.1515195667743683\n",
      "Epoch 41 : Loss = 3.1516387462615967\n",
      "Epoch 42 : Loss = 3.151118718087673\n",
      "Epoch 43 : Loss = 3.1521075293421745\n",
      "Epoch 44 : Loss = 3.1565384045243263\n",
      "Epoch 45 : Loss = 3.1494167000055313\n",
      "Epoch 46 : Loss = 3.1517923399806023\n",
      "Epoch 47 : Loss = 3.1519870087504387\n",
      "Epoch 48 : Loss = 3.1502637043595314\n",
      "Epoch 49 : Loss = 3.151083655655384\n",
      "Epoch 50 : Loss = 3.155845418572426\n",
      "Epoch 51 : Loss = 3.154185563325882\n",
      "Epoch 52 : Loss = 3.150601103901863\n",
      "Epoch 53 : Loss = 3.1513223946094513\n",
      "Epoch 54 : Loss = 3.148695297539234\n",
      "Epoch 55 : Loss = 3.145354136824608\n",
      "Epoch 56 : Loss = 3.1483411863446236\n",
      "Epoch 57 : Loss = 3.14789430052042\n",
      "Epoch 58 : Loss = 3.150316335260868\n",
      "Epoch 59 : Loss = 3.1570567712187767\n",
      "Epoch 60 : Loss = 3.1541587561368942\n",
      "Epoch 61 : Loss = 3.149888962507248\n",
      "Epoch 62 : Loss = 3.154237799346447\n",
      "Epoch 63 : Loss = 3.1476328521966934\n",
      "Epoch 64 : Loss = 3.1490859910845757\n",
      "Epoch 65 : Loss = 3.1599095836281776\n",
      "Epoch 66 : Loss = 3.1484439745545387\n",
      "Epoch 67 : Loss = 3.1493019834160805\n",
      "Epoch 68 : Loss = 3.150923989713192\n",
      "Epoch 69 : Loss = 3.152024395763874\n",
      "Epoch 70 : Loss = 3.150751382112503\n",
      "Epoch 71 : Loss = 3.148311421275139\n",
      "Epoch 72 : Loss = 3.154223032295704\n",
      "Epoch 73 : Loss = 3.147674411535263\n",
      "Epoch 74 : Loss = 3.1492183208465576\n",
      "Epoch 75 : Loss = 3.147587537765503\n",
      "Epoch 76 : Loss = 3.146245203912258\n",
      "Epoch 77 : Loss = 3.1462802290916443\n",
      "Epoch 78 : Loss = 3.14532158523798\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-cc4e6736c900>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Update Parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vasistha singhal\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vasistha singhal\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "old_loss = np.inf\n",
    "\n",
    "max_epoch = 500\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        # extracting encoder features from AE1 to use as input to the MLFFNN\n",
    "        with torch.no_grad():\n",
    "            Z = ae1.get_z(X)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_hat = classifier(Z)\n",
    "        \n",
    "        # Calculate Loss (Cross Entropy)\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    \n",
    "    if abs(running_loss-old_loss)/running_loss < 1e-6:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 3.288172483444214\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>329</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>281</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4\n",
       "0  289    0    1    0    0\n",
       "1    0  329    2    0    0\n",
       "2    1    6  281    0    0\n",
       "3    0    0    0  209    0\n",
       "4    0    3    2    0  285"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in trainloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        Z = ae1.get_z(X)\n",
    "        y_hat = classifier(Z)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.cpu().detach().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).cpu().detach().numpy()))\n",
    "\n",
    "print('Train Loss =', test_loss.item())\n",
    "pd.DataFrame(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 30.625680923461914\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4\n",
       "0  26   8  20   8   4\n",
       "1   3  44  12   5  15\n",
       "2  17  17  36   6  10\n",
       "3   1   6   6  26  12\n",
       "4   6  16  12   8  28"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        Z = ae1.get_z(X)\n",
    "        y_hat = classifier(Z)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.cpu().detach().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).cpu().detach().numpy()))\n",
    "\n",
    "print('Test Loss =', test_loss.item())\n",
    "pd.DataFrame(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.45454545454545453 Test Precision = 0.45781006232004956 Test F1 = 0.45522083954880266\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "prec = precision_score(y_test, y_test_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "print('Test Accuracy =', acc, 'Test Precision =', prec, 'Test F1 =', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), 'a2_q1_wts.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1624, 0.4426, 0.6126,  ..., 3.2938, 3.3324, 3.2227],\n",
      "        [0.5676, 0.5882, 0.5283,  ..., 0.4174, 0.4174, 0.4144],\n",
      "        [0.5766, 0.1428, 0.9406,  ..., 2.4045, 2.5507, 2.7237],\n",
      "        ...,\n",
      "        [0.5540, 0.3268, 0.7248,  ..., 3.1994, 3.3497, 3.1775],\n",
      "        [0.6029, 0.3759, 0.8112,  ..., 2.7578, 2.8919, 2.9956],\n",
      "        [0.5877, 0.4436, 0.7396,  ..., 3.6991, 3.4472, 3.2400]])\n",
      "torch.Size([1408, 828])\n"
     ]
    }
   ],
   "source": [
    "# To fit the PCA model we load all the training points in a single batch\n",
    "train, test = train_test_loader('Data_Set_1(Colored_Images)', label_dict, train_fraction=0.8, batch_size=2000, num_workers=0)\n",
    "\n",
    "for i in train:\n",
    "    print(i[0])\n",
    "    temp = i[0]\n",
    "    print(i[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_pca(temp):\n",
    "    pca = PCA(n_components=0.99)\n",
    "    pca.fit(temp)\n",
    "    return pca\n",
    "\n",
    "PCA_model = return_pca(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_dimension = PCA_model.transform(temp).shape[1]\n",
    "pca_clf = FinalNet(reduced_dimension, [150, 75, 50], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(pca_clf.parameters(), lr=0.001, momentum=0.9)\n",
    "pca_clf = pca_clf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss = 70.05836939811707\n",
      "Epoch 2 : Loss = 68.88779950141907\n",
      "Epoch 3 : Loss = 67.606889128685\n",
      "Epoch 4 : Loss = 66.15462255477905\n",
      "Epoch 5 : Loss = 64.5316731929779\n",
      "Epoch 6 : Loss = 62.834609031677246\n",
      "Epoch 7 : Loss = 61.232816100120544\n",
      "Epoch 8 : Loss = 59.77930772304535\n",
      "Epoch 9 : Loss = 58.445438265800476\n",
      "Epoch 10 : Loss = 57.12319362163544\n",
      "Epoch 11 : Loss = 55.81054937839508\n",
      "Epoch 12 : Loss = 54.393919229507446\n",
      "Epoch 13 : Loss = 52.95768928527832\n",
      "Epoch 14 : Loss = 51.49075174331665\n",
      "Epoch 15 : Loss = 50.04957616329193\n",
      "Epoch 16 : Loss = 48.61212873458862\n",
      "Epoch 17 : Loss = 47.21941953897476\n",
      "Epoch 18 : Loss = 45.86977535486221\n",
      "Epoch 19 : Loss = 44.59262466430664\n",
      "Epoch 20 : Loss = 43.32969403266907\n",
      "Epoch 21 : Loss = 42.130806386470795\n",
      "Epoch 22 : Loss = 41.031650483608246\n",
      "Epoch 23 : Loss = 39.95331799983978\n",
      "Epoch 24 : Loss = 38.95661270618439\n",
      "Epoch 25 : Loss = 37.99318724870682\n",
      "Epoch 26 : Loss = 37.099294781684875\n",
      "Epoch 27 : Loss = 36.247111320495605\n",
      "Epoch 28 : Loss = 35.430638551712036\n",
      "Epoch 29 : Loss = 34.64480572938919\n",
      "Epoch 30 : Loss = 33.945850133895874\n",
      "Epoch 31 : Loss = 33.21570312976837\n",
      "Epoch 32 : Loss = 32.58810102939606\n",
      "Epoch 33 : Loss = 31.893023192882538\n",
      "Epoch 34 : Loss = 31.206787765026093\n",
      "Epoch 35 : Loss = 30.553720474243164\n",
      "Epoch 36 : Loss = 29.925238490104675\n",
      "Epoch 37 : Loss = 29.32196855545044\n",
      "Epoch 38 : Loss = 28.708794206380844\n",
      "Epoch 39 : Loss = 28.23389971256256\n",
      "Epoch 40 : Loss = 27.55191496014595\n",
      "Epoch 41 : Loss = 26.998370110988617\n",
      "Epoch 42 : Loss = 26.43870508670807\n",
      "Epoch 43 : Loss = 25.993393510580063\n",
      "Epoch 44 : Loss = 25.378377377986908\n",
      "Epoch 45 : Loss = 24.84514942765236\n",
      "Epoch 46 : Loss = 24.288298219442368\n",
      "Epoch 47 : Loss = 23.7801533639431\n",
      "Epoch 48 : Loss = 23.286970674991608\n",
      "Epoch 49 : Loss = 22.778356164693832\n",
      "Epoch 50 : Loss = 22.239710956811905\n",
      "Epoch 51 : Loss = 21.744594901800156\n",
      "Epoch 52 : Loss = 21.315116614103317\n",
      "Epoch 53 : Loss = 20.782095670700073\n",
      "Epoch 54 : Loss = 20.319570690393448\n",
      "Epoch 55 : Loss = 19.816647171974182\n",
      "Epoch 56 : Loss = 19.408869713544846\n",
      "Epoch 57 : Loss = 18.82126149535179\n",
      "Epoch 58 : Loss = 18.449767380952835\n",
      "Epoch 59 : Loss = 17.923964977264404\n",
      "Epoch 60 : Loss = 17.473882496356964\n",
      "Epoch 61 : Loss = 17.10556471347809\n",
      "Epoch 62 : Loss = 16.620612055063248\n",
      "Epoch 63 : Loss = 16.151596516370773\n",
      "Epoch 64 : Loss = 15.801096960902214\n",
      "Epoch 65 : Loss = 15.236268192529678\n",
      "Epoch 66 : Loss = 14.892164945602417\n",
      "Epoch 67 : Loss = 14.452141478657722\n",
      "Epoch 68 : Loss = 14.104583144187927\n",
      "Epoch 69 : Loss = 13.646344661712646\n",
      "Epoch 70 : Loss = 13.261612176895142\n",
      "Epoch 71 : Loss = 12.838833168148994\n",
      "Epoch 72 : Loss = 12.530343115329742\n",
      "Epoch 73 : Loss = 12.03810727596283\n",
      "Epoch 74 : Loss = 11.66402618587017\n",
      "Epoch 75 : Loss = 11.281682804226875\n",
      "Epoch 76 : Loss = 10.882180035114288\n",
      "Epoch 77 : Loss = 10.52275836467743\n",
      "Epoch 78 : Loss = 10.113409489393234\n",
      "Epoch 79 : Loss = 9.829811461269855\n",
      "Epoch 80 : Loss = 9.48700276017189\n",
      "Epoch 81 : Loss = 9.108286872506142\n",
      "Epoch 82 : Loss = 8.831117823719978\n",
      "Epoch 83 : Loss = 8.514782167971134\n",
      "Epoch 84 : Loss = 8.22907005995512\n",
      "Epoch 85 : Loss = 7.8261344358325005\n",
      "Epoch 86 : Loss = 7.593195736408234\n",
      "Epoch 87 : Loss = 7.335543781518936\n",
      "Epoch 88 : Loss = 6.9717739298939705\n",
      "Epoch 89 : Loss = 6.744030766189098\n",
      "Epoch 90 : Loss = 6.4851856380701065\n",
      "Epoch 91 : Loss = 6.2410514280200005\n",
      "Epoch 92 : Loss = 6.014139659702778\n",
      "Epoch 93 : Loss = 5.808861441910267\n",
      "Epoch 94 : Loss = 5.5623757764697075\n",
      "Epoch 95 : Loss = 5.265078492462635\n",
      "Epoch 96 : Loss = 5.054845288395882\n",
      "Epoch 97 : Loss = 4.8714990094304085\n",
      "Epoch 98 : Loss = 4.734832711517811\n",
      "Epoch 99 : Loss = 4.537749893963337\n",
      "Epoch 100 : Loss = 4.2997718676924706\n",
      "Epoch 101 : Loss = 4.146063506603241\n",
      "Epoch 102 : Loss = 4.017380021512508\n",
      "Epoch 103 : Loss = 3.863389030098915\n",
      "Epoch 104 : Loss = 3.6908840015530586\n",
      "Epoch 105 : Loss = 3.536502704024315\n",
      "Epoch 106 : Loss = 3.3894916027784348\n",
      "Epoch 107 : Loss = 3.3082547411322594\n",
      "Epoch 108 : Loss = 3.162179261445999\n",
      "Epoch 109 : Loss = 3.037194661796093\n",
      "Epoch 110 : Loss = 2.9313981160521507\n",
      "Epoch 111 : Loss = 2.8194657787680626\n",
      "Epoch 112 : Loss = 2.7241950780153275\n",
      "Epoch 113 : Loss = 2.633160226047039\n",
      "Epoch 114 : Loss = 2.531832493841648\n",
      "Epoch 115 : Loss = 2.439144253730774\n",
      "Epoch 116 : Loss = 2.3473392724990845\n",
      "Epoch 117 : Loss = 2.259643644094467\n",
      "Epoch 118 : Loss = 2.2017041072249413\n",
      "Epoch 119 : Loss = 2.133096344769001\n",
      "Epoch 120 : Loss = 2.0435408502817154\n",
      "Epoch 121 : Loss = 1.9829551726579666\n",
      "Epoch 122 : Loss = 1.9180644825100899\n",
      "Epoch 123 : Loss = 1.860669732093811\n",
      "Epoch 124 : Loss = 1.7924292460083961\n",
      "Epoch 125 : Loss = 1.7456390336155891\n",
      "Epoch 126 : Loss = 1.697987049818039\n",
      "Epoch 127 : Loss = 1.641437366604805\n",
      "Epoch 128 : Loss = 1.5963957086205482\n",
      "Epoch 129 : Loss = 1.5480981171131134\n",
      "Epoch 130 : Loss = 1.5023934543132782\n",
      "Epoch 131 : Loss = 1.4653990268707275\n",
      "Epoch 132 : Loss = 1.4224396273493767\n",
      "Epoch 133 : Loss = 1.3806231245398521\n",
      "Epoch 134 : Loss = 1.3494224101305008\n",
      "Epoch 135 : Loss = 1.3092556446790695\n",
      "Epoch 136 : Loss = 1.2780254855751991\n",
      "Epoch 137 : Loss = 1.2470659762620926\n",
      "Epoch 138 : Loss = 1.2144674956798553\n",
      "Epoch 139 : Loss = 1.1854434832930565\n",
      "Epoch 140 : Loss = 1.1599647030234337\n",
      "Epoch 141 : Loss = 1.1280299499630928\n",
      "Epoch 142 : Loss = 1.1016697064042091\n",
      "Epoch 143 : Loss = 1.0787317156791687\n",
      "Epoch 144 : Loss = 1.0536651760339737\n",
      "Epoch 145 : Loss = 1.0284256115555763\n",
      "Epoch 146 : Loss = 1.0067922174930573\n",
      "Epoch 147 : Loss = 0.9823343604803085\n",
      "Epoch 148 : Loss = 0.9642999693751335\n",
      "Epoch 149 : Loss = 0.9437460079789162\n",
      "Epoch 150 : Loss = 0.9246732443571091\n",
      "Epoch 151 : Loss = 0.9036552384495735\n",
      "Epoch 152 : Loss = 0.8867468014359474\n",
      "Epoch 153 : Loss = 0.8687799721956253\n",
      "Epoch 154 : Loss = 0.8523730412125587\n",
      "Epoch 155 : Loss = 0.8342209979891777\n",
      "Epoch 156 : Loss = 0.8182913437485695\n",
      "Epoch 157 : Loss = 0.8036307990550995\n",
      "Epoch 158 : Loss = 0.7872754260897636\n",
      "Epoch 159 : Loss = 0.7737183049321175\n",
      "Epoch 160 : Loss = 0.7581612020730972\n",
      "Epoch 161 : Loss = 0.7447209879755974\n",
      "Epoch 162 : Loss = 0.732728473842144\n",
      "Epoch 163 : Loss = 0.7193284556269646\n",
      "Epoch 164 : Loss = 0.707060419023037\n",
      "Epoch 165 : Loss = 0.696044810116291\n",
      "Epoch 166 : Loss = 0.6825889945030212\n",
      "Epoch 167 : Loss = 0.6693788021802902\n",
      "Epoch 168 : Loss = 0.6592937707901001\n",
      "Epoch 169 : Loss = 0.649251289665699\n",
      "Epoch 170 : Loss = 0.6379421129822731\n",
      "Epoch 171 : Loss = 0.6290237680077553\n",
      "Epoch 172 : Loss = 0.6184065341949463\n",
      "Epoch 173 : Loss = 0.607344888150692\n",
      "Epoch 174 : Loss = 0.5994042530655861\n",
      "Epoch 175 : Loss = 0.5898381397128105\n",
      "Epoch 176 : Loss = 0.5810472145676613\n",
      "Epoch 177 : Loss = 0.5728491470217705\n",
      "Epoch 178 : Loss = 0.5628597736358643\n",
      "Epoch 179 : Loss = 0.554708406329155\n",
      "Epoch 180 : Loss = 0.5469522923231125\n",
      "Epoch 181 : Loss = 0.5388201773166656\n",
      "Epoch 182 : Loss = 0.5311721861362457\n",
      "Epoch 183 : Loss = 0.5243403166532516\n",
      "Epoch 184 : Loss = 0.5169491097331047\n",
      "Epoch 185 : Loss = 0.5090040639042854\n",
      "Epoch 186 : Loss = 0.5039975270628929\n",
      "Epoch 187 : Loss = 0.4967700093984604\n",
      "Epoch 188 : Loss = 0.4896279498934746\n",
      "Epoch 189 : Loss = 0.48335372656583786\n",
      "Epoch 190 : Loss = 0.4766942709684372\n",
      "Epoch 191 : Loss = 0.47019222378730774\n",
      "Epoch 192 : Loss = 0.4653867408633232\n",
      "Epoch 193 : Loss = 0.4596114158630371\n",
      "Epoch 194 : Loss = 0.4535509720444679\n",
      "Epoch 195 : Loss = 0.44694167375564575\n",
      "Epoch 196 : Loss = 0.4419357478618622\n",
      "Epoch 197 : Loss = 0.436039537191391\n",
      "Epoch 198 : Loss = 0.43027058243751526\n",
      "Epoch 199 : Loss = 0.4256295934319496\n",
      "Epoch 200 : Loss = 0.4200408309698105\n",
      "Epoch 201 : Loss = 0.4159857854247093\n",
      "Epoch 202 : Loss = 0.41071760654449463\n",
      "Epoch 203 : Loss = 0.4059908464550972\n",
      "Epoch 204 : Loss = 0.4010622426867485\n",
      "Epoch 205 : Loss = 0.39641714096069336\n",
      "Epoch 206 : Loss = 0.39194630086421967\n",
      "Epoch 207 : Loss = 0.3871699273586273\n",
      "Epoch 208 : Loss = 0.38378095626831055\n",
      "Epoch 209 : Loss = 0.37909187376499176\n",
      "Epoch 210 : Loss = 0.37489044666290283\n",
      "Epoch 211 : Loss = 0.3711787983775139\n",
      "Epoch 212 : Loss = 0.36694182455539703\n",
      "Epoch 213 : Loss = 0.3632839247584343\n",
      "Epoch 214 : Loss = 0.3588903993368149\n",
      "Epoch 215 : Loss = 0.3551320284605026\n",
      "Epoch 216 : Loss = 0.3515361472964287\n",
      "Epoch 217 : Loss = 0.3479349687695503\n",
      "Epoch 218 : Loss = 0.3447866290807724\n",
      "Epoch 219 : Loss = 0.3407181352376938\n",
      "Epoch 220 : Loss = 0.33734511584043503\n",
      "Epoch 221 : Loss = 0.33408503234386444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222 : Loss = 0.3308044373989105\n",
      "Epoch 223 : Loss = 0.32822395861148834\n",
      "Epoch 224 : Loss = 0.32438911497592926\n",
      "Epoch 225 : Loss = 0.3215700536966324\n",
      "Epoch 226 : Loss = 0.31803208589553833\n",
      "Epoch 227 : Loss = 0.31501244008541107\n",
      "Epoch 228 : Loss = 0.3121921122074127\n",
      "Epoch 229 : Loss = 0.30918169021606445\n",
      "Epoch 230 : Loss = 0.3059910237789154\n",
      "Epoch 231 : Loss = 0.30343766510486603\n",
      "Epoch 232 : Loss = 0.300594687461853\n",
      "Epoch 233 : Loss = 0.297659695148468\n",
      "Epoch 234 : Loss = 0.2951820641756058\n",
      "Epoch 235 : Loss = 0.2928411066532135\n",
      "Epoch 236 : Loss = 0.28990454971790314\n",
      "Epoch 237 : Loss = 0.2870728373527527\n",
      "Epoch 238 : Loss = 0.28491948544979095\n",
      "Epoch 239 : Loss = 0.2825154811143875\n",
      "Epoch 240 : Loss = 0.27966585755348206\n",
      "Epoch 241 : Loss = 0.27732308208942413\n",
      "Epoch 242 : Loss = 0.27520811557769775\n",
      "Epoch 243 : Loss = 0.27278752624988556\n",
      "Epoch 244 : Loss = 0.27018335461616516\n",
      "Epoch 245 : Loss = 0.26811665296554565\n",
      "Epoch 246 : Loss = 0.26558053493499756\n",
      "Epoch 247 : Loss = 0.2635371685028076\n",
      "Epoch 248 : Loss = 0.2612340450286865\n",
      "Epoch 249 : Loss = 0.2591685503721237\n",
      "Epoch 250 : Loss = 0.25685445964336395\n",
      "Epoch 251 : Loss = 0.2548282593488693\n",
      "Epoch 252 : Loss = 0.2528361827135086\n",
      "Epoch 253 : Loss = 0.25101687014102936\n",
      "Epoch 254 : Loss = 0.24888768792152405\n",
      "Epoch 255 : Loss = 0.24715621769428253\n",
      "Epoch 256 : Loss = 0.24504567682743073\n",
      "Epoch 257 : Loss = 0.24303533136844635\n",
      "Epoch 258 : Loss = 0.2411595731973648\n",
      "Epoch 259 : Loss = 0.2395164966583252\n",
      "Epoch 260 : Loss = 0.23761510848999023\n",
      "Epoch 261 : Loss = 0.23572322726249695\n",
      "Epoch 262 : Loss = 0.23385846614837646\n",
      "Epoch 263 : Loss = 0.23200348019599915\n",
      "Epoch 264 : Loss = 0.23034028708934784\n",
      "Epoch 265 : Loss = 0.2286323457956314\n",
      "Epoch 266 : Loss = 0.22696618735790253\n",
      "Epoch 267 : Loss = 0.22515398263931274\n",
      "Epoch 268 : Loss = 0.22352217137813568\n",
      "Epoch 269 : Loss = 0.22184807062149048\n",
      "Epoch 270 : Loss = 0.22025887668132782\n",
      "Epoch 271 : Loss = 0.21878165006637573\n",
      "Epoch 272 : Loss = 0.21727053821086884\n",
      "Epoch 273 : Loss = 0.21555371582508087\n",
      "Epoch 274 : Loss = 0.21410834789276123\n",
      "Epoch 275 : Loss = 0.21265909075737\n",
      "Epoch 276 : Loss = 0.21111755073070526\n",
      "Epoch 277 : Loss = 0.20967082679271698\n",
      "Epoch 278 : Loss = 0.20818035304546356\n",
      "Epoch 279 : Loss = 0.20672202110290527\n",
      "Epoch 280 : Loss = 0.20538382232189178\n",
      "Epoch 281 : Loss = 0.2039617896080017\n",
      "Epoch 282 : Loss = 0.20253896713256836\n",
      "Epoch 283 : Loss = 0.20120812952518463\n",
      "Epoch 284 : Loss = 0.19984479248523712\n",
      "Epoch 285 : Loss = 0.19848690927028656\n",
      "Epoch 286 : Loss = 0.19716790318489075\n",
      "Epoch 287 : Loss = 0.19591833651065826\n",
      "Epoch 288 : Loss = 0.19481727480888367\n",
      "Epoch 289 : Loss = 0.19353049993515015\n",
      "Epoch 290 : Loss = 0.19223080575466156\n",
      "Epoch 291 : Loss = 0.1909487545490265\n",
      "Epoch 292 : Loss = 0.1896136850118637\n",
      "Epoch 293 : Loss = 0.18847595155239105\n",
      "Epoch 294 : Loss = 0.1872681975364685\n",
      "Epoch 295 : Loss = 0.18611101806163788\n",
      "Epoch 296 : Loss = 0.1848626732826233\n",
      "Epoch 297 : Loss = 0.18382716178894043\n",
      "Epoch 298 : Loss = 0.18262846767902374\n",
      "Epoch 299 : Loss = 0.18138845264911652\n",
      "Epoch 300 : Loss = 0.18027125298976898\n",
      "Epoch 301 : Loss = 0.1792333573102951\n",
      "Epoch 302 : Loss = 0.17811287939548492\n",
      "Epoch 303 : Loss = 0.17699947953224182\n",
      "Epoch 304 : Loss = 0.1759926676750183\n",
      "Epoch 305 : Loss = 0.17490054666996002\n",
      "Epoch 306 : Loss = 0.17376339435577393\n",
      "Epoch 307 : Loss = 0.17278704047203064\n",
      "Epoch 308 : Loss = 0.1717616617679596\n",
      "Epoch 309 : Loss = 0.17075350880622864\n",
      "Epoch 310 : Loss = 0.16978847980499268\n",
      "Epoch 311 : Loss = 0.1688583344221115\n",
      "Epoch 312 : Loss = 0.1678779274225235\n",
      "Epoch 313 : Loss = 0.16686893999576569\n",
      "Epoch 314 : Loss = 0.16615638136863708\n",
      "Epoch 315 : Loss = 0.1649962216615677\n",
      "Epoch 316 : Loss = 0.16401274502277374\n",
      "Epoch 317 : Loss = 0.16300129890441895\n",
      "Epoch 318 : Loss = 0.16216039657592773\n",
      "Epoch 319 : Loss = 0.1612575203180313\n",
      "Epoch 320 : Loss = 0.16034597158432007\n",
      "Epoch 321 : Loss = 0.15948835015296936\n",
      "Epoch 322 : Loss = 0.1585259586572647\n",
      "Epoch 323 : Loss = 0.15767009556293488\n",
      "Epoch 324 : Loss = 0.1568126529455185\n",
      "Epoch 325 : Loss = 0.15601585805416107\n",
      "Epoch 326 : Loss = 0.1550556719303131\n",
      "Epoch 327 : Loss = 0.1542716771364212\n",
      "Epoch 328 : Loss = 0.15337613224983215\n",
      "Epoch 329 : Loss = 0.1525440663099289\n",
      "Epoch 330 : Loss = 0.1517760306596756\n",
      "Epoch 331 : Loss = 0.15094760060310364\n",
      "Epoch 332 : Loss = 0.15006880462169647\n",
      "Epoch 333 : Loss = 0.14937086403369904\n",
      "Epoch 334 : Loss = 0.14860403537750244\n",
      "Epoch 335 : Loss = 0.1478278487920761\n",
      "Epoch 336 : Loss = 0.14694315195083618\n",
      "Epoch 337 : Loss = 0.14635057747364044\n",
      "Epoch 338 : Loss = 0.14562083780765533\n",
      "Epoch 339 : Loss = 0.14474539458751678\n",
      "Epoch 340 : Loss = 0.14396224915981293\n",
      "Epoch 341 : Loss = 0.14330199360847473\n",
      "Epoch 342 : Loss = 0.1425318866968155\n",
      "Epoch 343 : Loss = 0.1418849676847458\n",
      "Epoch 344 : Loss = 0.141159787774086\n",
      "Epoch 345 : Loss = 0.1403670758008957\n",
      "Epoch 346 : Loss = 0.1396453082561493\n",
      "Epoch 347 : Loss = 0.13900534808635712\n",
      "Epoch 348 : Loss = 0.1382455676794052\n",
      "Epoch 349 : Loss = 0.13758103549480438\n",
      "Epoch 350 : Loss = 0.1369267851114273\n",
      "Epoch 351 : Loss = 0.1362818330526352\n",
      "Epoch 352 : Loss = 0.13557374477386475\n",
      "Epoch 353 : Loss = 0.13493235409259796\n",
      "Epoch 354 : Loss = 0.13428634405136108\n",
      "Epoch 355 : Loss = 0.1336067169904709\n",
      "Epoch 356 : Loss = 0.13293702900409698\n",
      "Epoch 357 : Loss = 0.13235588371753693\n",
      "Epoch 358 : Loss = 0.13171590864658356\n",
      "Epoch 359 : Loss = 0.13111421465873718\n",
      "Epoch 360 : Loss = 0.1304301768541336\n",
      "Epoch 361 : Loss = 0.12982983887195587\n",
      "Epoch 362 : Loss = 0.12926219403743744\n",
      "Epoch 363 : Loss = 0.12861531972885132\n",
      "Epoch 364 : Loss = 0.1280696839094162\n",
      "Epoch 365 : Loss = 0.12740787863731384\n",
      "Epoch 366 : Loss = 0.12679724395275116\n",
      "Epoch 367 : Loss = 0.126344233751297\n",
      "Epoch 368 : Loss = 0.12567473948001862\n",
      "Epoch 369 : Loss = 0.1251455396413803\n",
      "Epoch 370 : Loss = 0.12454120814800262\n",
      "Epoch 371 : Loss = 0.12392240762710571\n",
      "Epoch 372 : Loss = 0.12342523038387299\n",
      "Epoch 373 : Loss = 0.12286891043186188\n",
      "Epoch 374 : Loss = 0.12230923771858215\n",
      "Epoch 375 : Loss = 0.12173742055892944\n",
      "Epoch 376 : Loss = 0.12117168307304382\n",
      "Epoch 377 : Loss = 0.1206548660993576\n",
      "Epoch 378 : Loss = 0.12012507021427155\n",
      "Epoch 379 : Loss = 0.11962486803531647\n",
      "Epoch 380 : Loss = 0.11904187500476837\n",
      "Epoch 381 : Loss = 0.11854086816310883\n",
      "Epoch 382 : Loss = 0.11802816390991211\n",
      "Epoch 383 : Loss = 0.11749085783958435\n",
      "Epoch 384 : Loss = 0.11700525879859924\n",
      "Epoch 385 : Loss = 0.11653706431388855\n",
      "Epoch 386 : Loss = 0.1160869300365448\n",
      "Epoch 387 : Loss = 0.11551298201084137\n",
      "Epoch 388 : Loss = 0.11500844359397888\n",
      "Epoch 389 : Loss = 0.11453969776630402\n",
      "Epoch 390 : Loss = 0.11407765746116638\n",
      "Epoch 391 : Loss = 0.11355908215045929\n",
      "Epoch 392 : Loss = 0.1130867749452591\n",
      "Epoch 393 : Loss = 0.11259514093399048\n",
      "Epoch 394 : Loss = 0.11210674047470093\n",
      "Epoch 395 : Loss = 0.1116517186164856\n",
      "Epoch 396 : Loss = 0.11126355826854706\n",
      "Epoch 397 : Loss = 0.11074838042259216\n",
      "Epoch 398 : Loss = 0.11026062071323395\n",
      "Epoch 399 : Loss = 0.1098054051399231\n",
      "Epoch 400 : Loss = 0.10939571261405945\n",
      "Epoch 401 : Loss = 0.10893711447715759\n",
      "Epoch 402 : Loss = 0.10851998627185822\n",
      "Epoch 403 : Loss = 0.10806994140148163\n",
      "Epoch 404 : Loss = 0.10761860013008118\n",
      "Epoch 405 : Loss = 0.10715338587760925\n",
      "Epoch 406 : Loss = 0.10672405362129211\n",
      "Epoch 407 : Loss = 0.10635422170162201\n",
      "Epoch 408 : Loss = 0.10592006146907806\n",
      "Epoch 409 : Loss = 0.10548485815525055\n",
      "Epoch 410 : Loss = 0.10500961542129517\n",
      "Epoch 411 : Loss = 0.10460735857486725\n",
      "Epoch 412 : Loss = 0.10426127910614014\n",
      "Epoch 413 : Loss = 0.10381186008453369\n",
      "Epoch 414 : Loss = 0.10340374708175659\n",
      "Epoch 415 : Loss = 0.10298749804496765\n",
      "Epoch 416 : Loss = 0.10259836912155151\n",
      "Epoch 417 : Loss = 0.10217282176017761\n",
      "Epoch 418 : Loss = 0.1018182635307312\n",
      "Epoch 419 : Loss = 0.10143080353736877\n",
      "Epoch 420 : Loss = 0.10102511942386627\n",
      "Epoch 421 : Loss = 0.10061688721179962\n",
      "Epoch 422 : Loss = 0.10023534297943115\n",
      "Epoch 423 : Loss = 0.09985487163066864\n",
      "Epoch 424 : Loss = 0.09953254461288452\n",
      "Epoch 425 : Loss = 0.0991254597902298\n",
      "Epoch 426 : Loss = 0.09877708554267883\n",
      "Epoch 427 : Loss = 0.09834285080432892\n",
      "Epoch 428 : Loss = 0.0980461835861206\n",
      "Epoch 429 : Loss = 0.0976390391588211\n",
      "Epoch 430 : Loss = 0.09729664027690887\n",
      "Epoch 431 : Loss = 0.09691984951496124\n",
      "Epoch 432 : Loss = 0.09654448926448822\n",
      "Epoch 433 : Loss = 0.09621666371822357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 434 : Loss = 0.09583932161331177\n",
      "Epoch 435 : Loss = 0.09549792110919952\n",
      "Epoch 436 : Loss = 0.09516498446464539\n",
      "Epoch 437 : Loss = 0.09478707611560822\n",
      "Epoch 438 : Loss = 0.09450791776180267\n",
      "Epoch 439 : Loss = 0.09412683546543121\n",
      "Epoch 440 : Loss = 0.09379677474498749\n",
      "Epoch 441 : Loss = 0.0934617817401886\n",
      "Epoch 442 : Loss = 0.09310056269168854\n",
      "Epoch 443 : Loss = 0.09276877343654633\n",
      "Epoch 444 : Loss = 0.09244254231452942\n",
      "Epoch 445 : Loss = 0.09211806952953339\n",
      "Epoch 446 : Loss = 0.09178347885608673\n",
      "Epoch 447 : Loss = 0.09147010743618011\n",
      "Epoch 448 : Loss = 0.09114730358123779\n",
      "Epoch 449 : Loss = 0.09081460535526276\n",
      "Epoch 450 : Loss = 0.09050148725509644\n",
      "Epoch 451 : Loss = 0.09020787477493286\n",
      "Epoch 452 : Loss = 0.08987978100776672\n",
      "Epoch 453 : Loss = 0.0895673930644989\n",
      "Epoch 454 : Loss = 0.08925142884254456\n",
      "Epoch 455 : Loss = 0.08897152543067932\n",
      "Epoch 456 : Loss = 0.0886412262916565\n",
      "Epoch 457 : Loss = 0.08833770453929901\n",
      "Epoch 458 : Loss = 0.08802661299705505\n",
      "Epoch 459 : Loss = 0.08773338794708252\n",
      "Epoch 460 : Loss = 0.08743971586227417\n",
      "Epoch 461 : Loss = 0.08712942898273468\n",
      "Epoch 462 : Loss = 0.08682943880558014\n",
      "Epoch 463 : Loss = 0.08653150498867035\n",
      "Epoch 464 : Loss = 0.08625800907611847\n",
      "Epoch 465 : Loss = 0.08596053719520569\n",
      "Epoch 466 : Loss = 0.08567161858081818\n",
      "Epoch 467 : Loss = 0.08538840711116791\n",
      "Epoch 468 : Loss = 0.0850808322429657\n",
      "Epoch 469 : Loss = 0.08487005531787872\n",
      "Epoch 470 : Loss = 0.08456942439079285\n",
      "Epoch 471 : Loss = 0.08427296578884125\n",
      "Epoch 472 : Loss = 0.08398783206939697\n",
      "Epoch 473 : Loss = 0.08369819819927216\n",
      "Epoch 474 : Loss = 0.08348444104194641\n",
      "Epoch 475 : Loss = 0.08315539360046387\n",
      "Epoch 476 : Loss = 0.08290517330169678\n",
      "Epoch 477 : Loss = 0.08264634013175964\n",
      "Epoch 478 : Loss = 0.0823754370212555\n",
      "Epoch 479 : Loss = 0.0820789784193039\n",
      "Epoch 480 : Loss = 0.08180408179759979\n",
      "Epoch 481 : Loss = 0.08155736327171326\n",
      "Epoch 482 : Loss = 0.08129525184631348\n",
      "Epoch 483 : Loss = 0.08103466033935547\n",
      "Epoch 484 : Loss = 0.08078490197658539\n",
      "Epoch 485 : Loss = 0.08051817119121552\n",
      "Epoch 486 : Loss = 0.08029274642467499\n",
      "Epoch 487 : Loss = 0.0800175815820694\n",
      "Epoch 488 : Loss = 0.07975293695926666\n",
      "Epoch 489 : Loss = 0.07949380576610565\n",
      "Epoch 490 : Loss = 0.07924686372280121\n",
      "Epoch 491 : Loss = 0.07901875674724579\n",
      "Epoch 492 : Loss = 0.07880447804927826\n",
      "Epoch 493 : Loss = 0.0785282701253891\n",
      "Epoch 494 : Loss = 0.07826793193817139\n",
      "Epoch 495 : Loss = 0.07803814113140106\n",
      "Epoch 496 : Loss = 0.07782292366027832\n",
      "Epoch 497 : Loss = 0.07756340503692627\n",
      "Epoch 498 : Loss = 0.07732939720153809\n",
      "Epoch 499 : Loss = 0.07707694172859192\n",
      "Epoch 500 : Loss = 0.07684719562530518\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "old_loss = np.inf\n",
    "\n",
    "max_epoch = 500\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        X, y = data[0], data[1].to(device)\n",
    "        \n",
    "        # applying PCA on the data to use as input to the MLFFNN\n",
    "        Z = torch.Tensor(PCA_model.transform(X)).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        y_hat = pca_clf(Z)\n",
    "        \n",
    "        # Calculate Loss (Cross Entropy)\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
    "    \n",
    "    if abs(running_loss-old_loss)/running_loss < 1e-4:\n",
    "        print('Converged')\n",
    "        break\n",
    "    \n",
    "    old_loss = running_loss\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.07644738256931305\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>331</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4\n",
       "0  290    0    0    0    0\n",
       "1    0  331    0    0    0\n",
       "2    0    0  288    0    0\n",
       "3    0    0    0  209    0\n",
       "4    0    0    0    0  290"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in trainloader:\n",
    "\n",
    "        X, y = data[0], data[1].to(device)\n",
    "        Z = torch.Tensor(PCA_model.transform(X)).to(device)\n",
    "        y_hat = pca_clf(Z)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.cpu().detach().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).cpu().detach().numpy()))\n",
    "\n",
    "print('Train Loss =', test_loss.item())\n",
    "pd.DataFrame(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 30.87776756286621\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4\n",
       "0  39   6  14   3   4\n",
       "1   4  46   4   3  22\n",
       "2  12  11  45   7  11\n",
       "3   5   4   5  32   5\n",
       "4   3  16  10   6  35"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_test_pred = []\n",
    "\n",
    "    for data in testloader:\n",
    "\n",
    "        X, y = data[0], data[1].to(device)\n",
    "        Z = torch.Tensor(PCA_model.transform(X)).to(device)\n",
    "        y_hat = pca_clf(Z)      \n",
    "        test_loss += criterion(y_hat, y)\n",
    "        \n",
    "        y_test.extend(list(y.cpu().detach().numpy()))\n",
    "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).cpu().detach().numpy()))\n",
    "\n",
    "print('Test Loss =', test_loss.item())\n",
    "pd.DataFrame(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.5596590909090909 Test Precision = 0.5664367996756374 Test F1 = 0.5649948683492221\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "prec = precision_score(y_test, y_test_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "print('Test Accuracy =', acc, 'Test Precision =', prec, 'Test F1 =', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
