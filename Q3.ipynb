{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Part 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIldeHYFipEN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "700ab7a7-68bc-4c63-c8ae-f01826619063"
      },
      "source": [
        "# !pip install pyunpack\n",
        "# !pip install patool"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyunpack in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: easyprocess in /usr/local/lib/python3.6/dist-packages (from pyunpack) (0.3)\n",
            "Requirement already satisfied: entrypoint2 in /usr/local/lib/python3.6/dist-packages (from pyunpack) (0.2.1)\n",
            "Requirement already satisfied: argparse in /usr/local/lib/python3.6/dist-packages (from entrypoint2->pyunpack) (1.4.0)\n",
            "Requirement already satisfied: patool in /usr/local/lib/python3.6/dist-packages (1.12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJD91Ep72TYz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "24321296-57ec-4f53-dab8-a4e910ed99a7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7xRvOOGcsL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pyunpack import Archive\n",
        "# Archive('/content/drive/My Drive/Data_Set_2(Black_and_white_images).rar').extractall('/content/drive/My Drive/Data_Set_2(Black_and_white_images)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTvZrj6Tb9CW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nIBihjGb9Cm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DatasetClass(Dataset):\n",
        "    \n",
        "    def __init__(self, folder, filename, label_dict):\n",
        "        \n",
        "        self.filename = filename\n",
        "        self.data = pd.read_csv(folder + '/' + filename, header=None)\n",
        "        self.y = torch.tensor(label_dict[self.filename.rstrip('.csv')], dtype=torch.long)\n",
        "    \n",
        "    def __getitem__(self, idx):     \n",
        "        \n",
        "        return torch.tensor(self.data.iloc[idx], dtype=torch.float), self.y\n",
        "      \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abtj_RwNb9Cw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_loader(directory, train_fraction=0.8, num_workers=2, batch_size=32):\n",
        "\n",
        "    files = list(filter(lambda x: x.endswith('.csv') and x[0].isupper(), os.listdir(directory)))\n",
        "    label_dict = {}\n",
        "\n",
        "    i = 0\n",
        "    for file in files:\n",
        "        label_dict[file.rstrip('.csv')] = i\n",
        "        i += 1\n",
        "\n",
        "    datasets = list(map(lambda x : DatasetClass(directory, x, label_dict), files))\n",
        "    dataset = ConcatDataset(datasets)\n",
        "    N = dataset.cumulative_sizes[-1]\n",
        "    \n",
        "    train_size = int(N*train_fraction)\n",
        "    test_size = N - train_size\n",
        "\n",
        "    train_data, test_data = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    \n",
        "    return trainloader, testloader, train_size, test_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRYvvEcXb9C5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainloader, testloader, train_size, test_size = train_test_loader('/content/drive/My Drive/Data_Set_2(Black_and_white_images)', train_fraction=0.8, num_workers=0, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hHs6vC6b9C-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_features, h_layer_sizes):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(n_features, h_layer_sizes[0])\n",
        "        self.fc2 = nn.Linear(h_layer_sizes[0], h_layer_sizes[1])\n",
        "        self.fc3 = nn.Linear(h_layer_sizes[1], h_layer_sizes[2])\n",
        "        self.out = nn.Linear(h_layer_sizes[2], n_features)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = torch.tanh(self.fc1(x)) # Hidden Layer 1 (Tanh)\n",
        "        x = self.fc2(x)    # Hidden Layer 2 (Linear)\n",
        "        x = torch.tanh(self.fc3(x)) # Hidden Layer 3 (Tanh)\n",
        "        x = self.out(x) # Output Layer (Linear)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def get_z(self, x):\n",
        "        \n",
        "        z = torch.tanh(self.fc1(x))\n",
        "        z = self.fc2(z)\n",
        "        \n",
        "        return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e_pNIyEb9DE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f711cf5-c149-40f5-d35a-808fe027eb77"
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru64o3f2b9DO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ae1 = AutoEncoder(784, [150, 300, 150])\n",
        "ae1 = ae1.to(device)\n",
        "optimizer1 = optim.SGD(ae1.parameters(), lr=0.001, momentum=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbHvIruCbcoL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "4fe44e9c-35fa-4487-b3c5-d0336211242c"
      },
      "source": [
        "nn.init.xavier_normal_(ae1.fc1.weight)\n",
        "nn.init.xavier_normal_(ae1.fc2.weight)\n",
        "nn.init.xavier_normal_(ae1.fc3.weight)\n",
        "nn.init.xavier_normal_(ae1.out.weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0473,  0.0143, -0.0267,  ...,  0.0494,  0.0549, -0.0022],\n",
              "        [-0.0727, -0.0053,  0.0683,  ...,  0.0236,  0.0278,  0.0068],\n",
              "        [ 0.0310,  0.0680,  0.0771,  ...,  0.0087,  0.0730, -0.0665],\n",
              "        ...,\n",
              "        [ 0.0289,  0.0029,  0.0511,  ..., -0.0339, -0.0300, -0.0316],\n",
              "        [ 0.0183,  0.0044,  0.0534,  ..., -0.0003,  0.0149,  0.0453],\n",
              "        [ 0.0992,  0.0698, -0.0617,  ...,  0.0015, -0.0276,  0.0216]],\n",
              "       device='cuda:0', requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trgn2rUeb9DY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "13f647a0-07f6-4e71-b5f3-9213a8ccfe86"
      },
      "source": [
        "old_loss = np.inf\n",
        "\n",
        "max_epoch = 50\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    len_ = 0\n",
        "    \n",
        "    for data in trainloader:\n",
        "        \n",
        "        X, _ = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        optimizer1.zero_grad()\n",
        "        \n",
        "        # Reconstructed Representation of X (forward)\n",
        "        X_hat = ae1(X)\n",
        "        \n",
        "        # Calculate Loss (MSE)\n",
        "        loss = criterion(X_hat, X)\n",
        "        \n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update Parameters\n",
        "        optimizer1.step()\n",
        "        \n",
        "        running_loss += loss.item()*len(X)\n",
        "    \n",
        "    print('Epoch', epoch+1, ': Loss =', running_loss/train_size)\n",
        "    \n",
        "    if (abs(running_loss-old_loss)/running_loss < 1e-5):\n",
        "        print('Converged')\n",
        "        break\n",
        "    \n",
        "    old_loss = running_loss\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Loss = 8262.5158515625\n",
            "Epoch 2 : Loss = 5816.47699609375\n",
            "Epoch 3 : Loss = 5684.198220052083\n",
            "Epoch 4 : Loss = 5665.770877604166\n",
            "Epoch 5 : Loss = 5672.217048828125\n",
            "Epoch 6 : Loss = 5679.772602213542\n",
            "Epoch 7 : Loss = 5656.240639973958\n",
            "Epoch 8 : Loss = 5656.17103125\n",
            "Epoch 9 : Loss = 5653.755246744791\n",
            "Epoch 10 : Loss = 5653.284908203125\n",
            "Epoch 11 : Loss = 5653.759967447917\n",
            "Epoch 12 : Loss = 5653.584149739583\n",
            "Epoch 13 : Loss = 5653.262468098958\n",
            "Epoch 14 : Loss = 5653.269096354166\n",
            "Converged\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzLhTvpq1zft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "532af516-a73d-4d6b-bec9-0a3a6f9f1b73"
      },
      "source": [
        "X_data_test = []\n",
        "X_data_test_pred = []\n",
        "\n",
        "test_loss = 0.0\n",
        "len_ = 0\n",
        "\n",
        "for data in testloader:\n",
        "        \n",
        "    X, _ = data[0].to(device), data[1].to(device)\n",
        "    with torch.no_grad():\n",
        "      X_pred = ae1(X)\n",
        "      loss = criterion(X_pred, X)\n",
        "\n",
        "    test_loss += loss.item()*len(X)\n",
        "\n",
        "test_loss = test_loss/test_size\n",
        "print(test_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5628.567348958333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQpvQyBK5CnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Z1_array = []\n",
        "\n",
        "for data in trainloader:\n",
        "        \n",
        "    X, _ = data[0].to(device), data[1].to(device)\n",
        "    with torch.no_grad():\n",
        "      Z1 = ae1.get_z(X)\n",
        "\n",
        "    if len(Z1_array) == 0:\n",
        "        Z1_array = Z1.cpu().numpy()\n",
        "    else:\n",
        "        Z1_array = np.append(Z1_array, Z1.cpu().numpy(), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-5C_u4uiyUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ff06c50-90d6-4167-d5c8-fea6f59db04b"
      },
      "source": [
        "Z1_array.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48000, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA2o4c0KivOE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "c899aa26-f92c-4bf3-f7fd-ed4b2589ae1d"
      },
      "source": [
        "pca2 = PCA(n_components=300)\n",
        "pca2.fit(Z1_array)\n",
        "\n",
        "explained_var = np.cumsum(pca2.explained_variance_ratio_)\n",
        "plt.plot(explained_var)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSpa66F-joQT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f52986d8-ada5-42cb-8dbd-811403fb937e"
      },
      "source": [
        "explained_var"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.95209974, 0.9623871 , 0.9701707 , 0.977094  , 0.9808555 ,\n",
              "       0.98345304, 0.98586243, 0.9879532 , 0.98965275, 0.99109817,\n",
              "       0.9923194 , 0.9934919 , 0.99452   , 0.9953611 , 0.9959334 ,\n",
              "       0.99637777, 0.99672544, 0.99694437, 0.9971484 , 0.99733377,\n",
              "       0.9975059 , 0.99767023, 0.99781525, 0.9979479 , 0.9980756 ,\n",
              "       0.99819076, 0.9983023 , 0.9984097 , 0.9985084 , 0.9986003 ,\n",
              "       0.9986875 , 0.9987706 , 0.99885154, 0.998927  , 0.9989986 ,\n",
              "       0.9990685 , 0.9991352 , 0.9991988 , 0.99925715, 0.99930656,\n",
              "       0.9993537 , 0.9993996 , 0.99944085, 0.99947876, 0.99951476,\n",
              "       0.999547  , 0.9995788 , 0.9996076 , 0.9996358 , 0.99966383,\n",
              "       0.99968946, 0.99971217, 0.99973357, 0.9997537 , 0.99977213,\n",
              "       0.9997887 , 0.9998047 , 0.99982   , 0.9998346 , 0.9998474 ,\n",
              "       0.9998596 , 0.99987096, 0.99988115, 0.99989104, 0.99990064,\n",
              "       0.9999094 , 0.9999174 , 0.99992436, 0.9999306 , 0.99993664,\n",
              "       0.9999421 , 0.99994725, 0.99995184, 0.99995595, 0.99995995,\n",
              "       0.9999638 , 0.9999672 , 0.9999706 , 0.99997395, 0.9999769 ,\n",
              "       0.9999796 , 0.99998224, 0.9999845 , 0.9999867 , 0.9999886 ,\n",
              "       0.99999034, 0.99999195, 0.9999935 , 0.9999948 , 0.99999595,\n",
              "       0.9999971 , 0.99999803, 0.9999988 , 0.9999994 , 0.99999976,\n",
              "       1.        , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
              "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwcvgUCVb9Df",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ae2 = AutoEncoder(300, [150, 100, 150])\n",
        "optimizer2 = optim.SGD(ae2.parameters(), lr=0.001, momentum=0.9)\n",
        "ae2 = ae2.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J144k8cub9Dn",
        "colab_type": "code",
        "colab": {},
        "outputId": "521076f1-f5a1-45b2-e9a4-bfe3dadc1c7a"
      },
      "source": [
        "old_loss = np.inf\n",
        "\n",
        "max_epoch = 500\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for data in trainloader:\n",
        "        \n",
        "        X, _ = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            Z = ae1.get_z(X)\n",
        "        \n",
        "        optimizer2.zero_grad()\n",
        "        \n",
        "        # Reconstructed Representation of X (forward)\n",
        "        Z_hat = ae2(Z)\n",
        "        \n",
        "        # Calculate Loss (MSE)\n",
        "        loss = criterion(Z_hat, Z)\n",
        "        \n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update Parameters\n",
        "        optimizer2.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
        "    \n",
        "    if abs(running_loss-old_loss)/running_loss < 1e-3:\n",
        "        print('Converged')\n",
        "        break\n",
        "        \n",
        "    old_loss = running_loss\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Loss = 980.7005815608427\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVEyxUUBb9Dt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ae3 = AutoEncoder(100, [75, 50, 75])\n",
        "optimizer3 = optim.SGD(ae3.parameters(), lr=0.001, momentum=0.9)\n",
        "ae3 = ae3.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV0pjnuNb9Dy",
        "colab_type": "code",
        "colab": {},
        "outputId": "e1f954d9-d141-46d9-c454-1f0f219c77b1"
      },
      "source": [
        "old_loss = np.inf\n",
        "\n",
        "max_epoch = 500\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for data in trainloader:\n",
        "        \n",
        "        X, _ = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            Z1 = ae1.get_z(X)\n",
        "            Z = ae2.get_z(Z1)\n",
        "        \n",
        "        optimizer3.zero_grad()\n",
        "        \n",
        "        # Reconstructed Representation of Z (forward)\n",
        "        Z_hat = ae3(Z)\n",
        "        \n",
        "        # Calculate Loss (MSE)\n",
        "        loss = criterion(Z_hat, Z)\n",
        "        \n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update Parameters\n",
        "        optimizer3.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
        "    \n",
        "    if abs(running_loss-old_loss)/running_loss < 1e-3:\n",
        "        print('Converged')\n",
        "        break\n",
        "        \n",
        "    old_loss = running_loss\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Loss = 410.4300510010253\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asYo4AYBb9EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FinalNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
        "        super(FinalNet, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "        self.fc4 = nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
        "        self.out = nn.Linear(hidden_sizes[3], num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        x = torch.tanh(self.fc4(x))\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            y_score = self.forward(X)\n",
        "            y_pred = torch.argmax(y_score, axis=1)\n",
        "            \n",
        "        return y_pred\n",
        "            \n",
        "    \n",
        "classifier = FinalNet(784, [300, 150, 75, 50], 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azVOLzPcb9EQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ae1_params = list(ae1.parameters())\n",
        "ae2_params = list(ae2.parameters())\n",
        "ae3_params = list(ae3.parameters())\n",
        "\n",
        "with torch.no_grad():\n",
        "    \n",
        "    classifier.fc1.weight.data = nn.Parameter(ae1_params[0])\n",
        "    classifier.fc1.bias.data = nn.Parameter(ae1_params[1])\n",
        "    \n",
        "    classifier.fc2.weight = nn.Parameter(torch.matmul(ae2_params[0], ae1_params[2]))\n",
        "    classifier.fc2.bias = nn.Parameter(torch.matmul(ae2_params[0], ae1_params[3]) + ae2_params[1])\n",
        "    \n",
        "    classifier.fc3.weight = nn.Parameter(torch.matmul(ae3_params[0], ae2_params[2]))\n",
        "    classifier.fc3.bias = nn.Parameter(torch.matmul(ae3_params[0], ae2_params[3]) + ae3_params[1])\n",
        "    \n",
        "    classifier.fc4.weight = nn.Parameter(ae3_params[2])\n",
        "    classifier.fc4.bias = nn.Parameter(ae3_params[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htcYHkZvb9EX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
        "classifier = classifier.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGqhu8MUb9Ed",
        "colab_type": "code",
        "colab": {},
        "outputId": "76d86061-cd1d-4840-91a7-4c7a5eee993b"
      },
      "source": [
        "old_loss = np.inf\n",
        "\n",
        "max_epoch = 500\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for data in trainloader:\n",
        "        \n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward\n",
        "        y_hat = classifier(X)\n",
        "        \n",
        "        # Calculate Loss (Cross Entropy)\n",
        "        loss = criterion(y_hat, y)\n",
        "        \n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update Parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
        "    \n",
        "    if abs(running_loss-old_loss)/running_loss < 1e-3:\n",
        "        print('Converged')\n",
        "        break\n",
        "    \n",
        "    old_loss = running_loss\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Loss = 3457.114572763443\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVYcIfixb9Eg",
        "colab_type": "code",
        "colab": {},
        "outputId": "ce1ac28f-dd62-4d23-cf23-403f3331e59a"
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    test_loss = 0.0\n",
        "    y_test = []\n",
        "    y_test_pred = []\n",
        "\n",
        "    for data in testloader:\n",
        "\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        y_hat = classifier(X)      \n",
        "        test_loss += criterion(y_hat, y)\n",
        "        \n",
        "        y_test.extend(list(y.detach().numpy()))\n",
        "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().numpy()))\n",
        "\n",
        "print('Test Loss =', test_loss.item())\n",
        "pd.DataFrame(confusion_matrix(y_test, y_test_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss = 864.0877075195312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1215</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1234</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1191</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1251</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1179</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1138</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1172</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1228</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1196</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1196</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1     2  3  4  5  6  7  8  9\n",
              "0  0  0  1215  0  0  0  0  0  0  0\n",
              "1  0  0  1234  0  0  0  0  0  0  0\n",
              "2  0  0  1191  0  0  0  0  0  0  0\n",
              "3  0  0  1251  0  0  0  0  0  0  0\n",
              "4  0  0  1179  0  0  0  0  0  0  0\n",
              "5  0  0  1138  0  0  0  0  0  0  0\n",
              "6  0  0  1172  0  0  0  0  0  0  0\n",
              "7  0  0  1228  0  0  0  0  0  0  0\n",
              "8  0  0  1196  0  0  0  0  0  0  0\n",
              "9  0  0  1196  0  0  0  0  0  0  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    }
  ]
}