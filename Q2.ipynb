{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Q2 (A2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIldeHYFipEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install pyunpack\n",
        "# !pip install patool"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJD91Ep72TYz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd0d6c68-034d-484d-c09b-4134c6bd7cf4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7xRvOOGcsL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pyunpack import Archive\n",
        "# Archive('/content/drive/My Drive/Data_Set_2(Black_and_white_images).rar').extractall('/content/drive/My Drive/Data_Set_2(Black_and_white_images)')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTvZrj6Tb9CW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from io import StringIO\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
        "import pickle"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nIBihjGb9Cm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DatasetClass(Dataset):\n",
        "    \n",
        "    def __init__(self, folder, filename, label_dict):\n",
        "        \n",
        "        self.data = []\n",
        "        self.filename = filename\n",
        "        tar = tarfile.open(folder + '/' + filename)\n",
        "        for file in tar.getmembers():\n",
        "            f = tar.extractfile(file)\n",
        "            if f != None:\n",
        "                content = pd.read_csv(StringIO(f.read().decode()), sep=' ', header=None).values.ravel()\n",
        "                self.data.append(content)\n",
        "            \n",
        "        self.y = torch.tensor(label_dict[self.filename[:-7]], dtype=torch.long)\n",
        "    \n",
        "    def __getitem__(self, idx):     \n",
        "        \n",
        "        return torch.tensor(self.data[idx], dtype=torch.float), self.y\n",
        "      \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.data)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abtj_RwNb9Cw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_loader(directory, label_dict, train_fraction=0.8, num_workers=2):\n",
        "\n",
        "    all_files = list(filter(lambda x: x.endswith('.tar.gz'), os.listdir(directory)))\n",
        "    files = [file for file in all_files if file[:-7] in label_dict.keys()]\n",
        "    \n",
        "    datasets = list(map(lambda x : DatasetClass(directory, x, label_dict), files))\n",
        "    dataset = ConcatDataset(datasets)\n",
        "    N = dataset.cumulative_sizes[-1]\n",
        "    \n",
        "    train_size = int(N*train_fraction)\n",
        "    test_size = N - train_size\n",
        "\n",
        "    train_data, test_data = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    trainloader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=num_workers)\n",
        "    testloader = DataLoader(test_data, batch_size=32, shuffle=True, num_workers=num_workers)\n",
        "    \n",
        "    return trainloader, testloader, train_size, test_size"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRYvvEcXb9C5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_dict = {\n",
        "    'tallbuilding': 0,\n",
        "    'mountain': 1,\n",
        "    'highway': 2,\n",
        "    'coast': 3, \n",
        "    'opencountry': 4}\n",
        "\n",
        "trainloader, testloader, train_size, test_size = train_test_loader('/content/drive/My Drive/Data_Set_1(Colored_Images)', label_dict, train_fraction=0.8, num_workers=0)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hHs6vC6b9C-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_features, h_layer_sizes):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(n_features, h_layer_sizes[0])\n",
        "        self.fc2 = nn.Linear(h_layer_sizes[0], h_layer_sizes[1])\n",
        "        self.fc3 = nn.Linear(h_layer_sizes[1], h_layer_sizes[2])\n",
        "        self.out = nn.Linear(h_layer_sizes[2], n_features)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = torch.tanh(self.fc1(x)) # Hidden Layer 1 (Tanh)\n",
        "        x = self.fc2(x)    # Hidden Layer 2 (Linear)\n",
        "        x = torch.tanh(self.fc3(x)) # Hidden Layer 3 (Tanh)\n",
        "        x = self.out(x) # Output Layer (Linear)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def get_z(self, x):\n",
        "        \n",
        "        z = torch.tanh(self.fc1(x))\n",
        "        z = self.fc2(z)\n",
        "        \n",
        "        return z"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e_pNIyEb9DE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c6730be-dede-40c2-d585-9c5cbca43b3f"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSWJgWEucK0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = []\n",
        "for X, y in trainloader:\n",
        "  X_train.extend(X.numpy())\n",
        "X_train = np.array(X_train)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEICm-tqcEBv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "a80c56d5-6da0-45a7-dc77-2638d14d6c90"
      },
      "source": [
        "pca1 = PCA(n_components=X_train.shape[1])\n",
        "pca1.fit(X_train)\n",
        "\n",
        "print('97% Variance Explained:', np.where(np.cumsum(pca1.explained_variance_ratio_)>=0.97)[0][0]+1)\n",
        "explained_var = np.cumsum(pca1.explained_variance_ratio_)\n",
        "plt.plot(explained_var)\n",
        "plt.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97% Variance Explained: 229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru64o3f2b9DO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "\n",
        "ae1 = AutoEncoder(828, [400, 200, 400])\n",
        "ae1 = ae1.to(device)\n",
        "optimizer1 = optim.SGD(ae1.parameters(), lr=0.0001, momentum=0.9)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trgn2rUeb9DY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c9f6453-039c-440e-9a40-cd26c1785f32"
      },
      "source": [
        "old_loss = np.inf\n",
        "\n",
        "max_epoch = 500\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    len_ = 0\n",
        "    \n",
        "    for data in trainloader:\n",
        "        \n",
        "        X, _ = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        optimizer1.zero_grad()\n",
        "        \n",
        "        # Reconstructed Representation of X (forward)\n",
        "        X_hat = ae1(X)\n",
        "        \n",
        "        # Calculate Loss (MSE)\n",
        "        loss = criterion(X_hat, X)\n",
        "        \n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update Parameters\n",
        "        optimizer1.step()\n",
        "        \n",
        "        running_loss += loss.item()*len(X)/train_size\n",
        "    \n",
        "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
        "    \n",
        "    if (abs(running_loss-old_loss)/running_loss < 1e-5):\n",
        "        print('Converged')\n",
        "        break\n",
        "    \n",
        "    old_loss = running_loss\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Loss = 1.9322659725492646\n",
            "Epoch 2 : Loss = 1.9119414172389289\n",
            "Epoch 3 : Loss = 1.8916960385712707\n",
            "Epoch 4 : Loss = 1.872071848674254\n",
            "Epoch 5 : Loss = 1.8526994206688627\n",
            "Epoch 6 : Loss = 1.833146650682796\n",
            "Epoch 7 : Loss = 1.8131206902590662\n",
            "Epoch 8 : Loss = 1.7923159165815872\n",
            "Epoch 9 : Loss = 1.7705351358110255\n",
            "Epoch 10 : Loss = 1.7475719343532214\n",
            "Epoch 11 : Loss = 1.7232755232941024\n",
            "Epoch 12 : Loss = 1.6975687092000793\n",
            "Epoch 13 : Loss = 1.6703512722795661\n",
            "Epoch 14 : Loss = 1.6416068293831565\n",
            "Epoch 15 : Loss = 1.6112863394347108\n",
            "Epoch 16 : Loss = 1.5794536227529696\n",
            "Epoch 17 : Loss = 1.5461519631472507\n",
            "Epoch 18 : Loss = 1.5114889063618397\n",
            "Epoch 19 : Loss = 1.4754880151965404\n",
            "Epoch 20 : Loss = 1.4383839260448108\n",
            "Epoch 21 : Loss = 1.400214138356122\n",
            "Epoch 22 : Loss = 1.3612529743801465\n",
            "Epoch 23 : Loss = 1.3216421089389103\n",
            "Epoch 24 : Loss = 1.281550922177055\n",
            "Epoch 25 : Loss = 1.2411584799939936\n",
            "Epoch 26 : Loss = 1.2006559344855219\n",
            "Epoch 27 : Loss = 1.1602380275726316\n",
            "Epoch 28 : Loss = 1.1200498613444243\n",
            "Epoch 29 : Loss = 1.0802710449153732\n",
            "Epoch 30 : Loss = 1.0410147905349727\n",
            "Epoch 31 : Loss = 1.0024574913761832\n",
            "Epoch 32 : Loss = 0.964658317240802\n",
            "Epoch 33 : Loss = 0.9277652366594837\n",
            "Epoch 34 : Loss = 0.8918538906357506\n",
            "Epoch 35 : Loss = 0.8570091548291122\n",
            "Epoch 36 : Loss = 0.8232068663293666\n",
            "Epoch 37 : Loss = 0.7905983220447194\n",
            "Epoch 38 : Loss = 0.7591978243806147\n",
            "Epoch 39 : Loss = 0.7289637543938376\n",
            "Epoch 40 : Loss = 0.6999395449053156\n",
            "Epoch 41 : Loss = 0.6722039187496361\n",
            "Epoch 42 : Loss = 0.6456224037842315\n",
            "Epoch 43 : Loss = 0.6202320090749045\n",
            "Epoch 44 : Loss = 0.5960550403053111\n",
            "Epoch 45 : Loss = 0.5730582719499414\n",
            "Epoch 46 : Loss = 0.5511540689251638\n",
            "Epoch 47 : Loss = 0.5303623920137231\n",
            "Epoch 48 : Loss = 0.5106559084220367\n",
            "Epoch 49 : Loss = 0.49195857955650857\n",
            "Epoch 50 : Loss = 0.4742906764149666\n",
            "Epoch 51 : Loss = 0.45757385072383017\n",
            "Epoch 52 : Loss = 0.4417578063227915\n",
            "Epoch 53 : Loss = 0.42683210562575946\n",
            "Epoch 54 : Loss = 0.4127526005560702\n",
            "Epoch 55 : Loss = 0.39949347214265307\n",
            "Epoch 56 : Loss = 0.38696804710409854\n",
            "Epoch 57 : Loss = 0.37522230635989795\n",
            "Epoch 58 : Loss = 0.36412269622087473\n",
            "Epoch 59 : Loss = 0.35369896346872515\n",
            "Epoch 60 : Loss = 0.343908655372533\n",
            "Epoch 61 : Loss = 0.33470798148350284\n",
            "Epoch 62 : Loss = 0.3260466788302768\n",
            "Epoch 63 : Loss = 0.3179446668787437\n",
            "Epoch 64 : Loss = 0.3103214183991605\n",
            "Epoch 65 : Loss = 0.30317146195606753\n",
            "Epoch 66 : Loss = 0.2964727492495017\n",
            "Epoch 67 : Loss = 0.2901967885819348\n",
            "Epoch 68 : Loss = 0.28431023318659177\n",
            "Epoch 69 : Loss = 0.27878934483636514\n",
            "Epoch 70 : Loss = 0.2736264372413808\n",
            "Epoch 71 : Loss = 0.268780896270817\n",
            "Epoch 72 : Loss = 0.2642573538151654\n",
            "Epoch 73 : Loss = 0.2600116289474747\n",
            "Epoch 74 : Loss = 0.2560491991991347\n",
            "Epoch 75 : Loss = 0.2523293417285789\n",
            "Epoch 76 : Loss = 0.24886368824677033\n",
            "Epoch 77 : Loss = 0.24561327628114007\n",
            "Epoch 78 : Loss = 0.24256828867576335\n",
            "Epoch 79 : Loss = 0.23973924598910584\n",
            "Epoch 80 : Loss = 0.2370676997710358\n",
            "Epoch 81 : Loss = 0.23460080000487243\n",
            "Epoch 82 : Loss = 0.2322665744207122\n",
            "Epoch 83 : Loss = 0.23010108450596983\n",
            "Epoch 84 : Loss = 0.2280742051926526\n",
            "Epoch 85 : Loss = 0.22617991031570867\n",
            "Epoch 86 : Loss = 0.22440948472781613\n",
            "Epoch 87 : Loss = 0.22275140746073283\n",
            "Epoch 88 : Loss = 0.22120672836899755\n",
            "Epoch 89 : Loss = 0.2197633422911167\n",
            "Epoch 90 : Loss = 0.21841661090200593\n",
            "Epoch 91 : Loss = 0.21715275198221207\n",
            "Epoch 92 : Loss = 0.2159773453392766\n",
            "Epoch 93 : Loss = 0.21487709676677524\n",
            "Epoch 94 : Loss = 0.21385068141601302\n",
            "Epoch 95 : Loss = 0.21289439092982895\n",
            "Epoch 96 : Loss = 0.21199391138824553\n",
            "Epoch 97 : Loss = 0.21115695617415692\n",
            "Epoch 98 : Loss = 0.2103761451488191\n",
            "Epoch 99 : Loss = 0.20964733782139688\n",
            "Epoch 100 : Loss = 0.2089615857059305\n",
            "Epoch 101 : Loss = 0.20832937481728467\n",
            "Epoch 102 : Loss = 0.2077316709540107\n",
            "Epoch 103 : Loss = 0.20717559077522965\n",
            "Epoch 104 : Loss = 0.20665722171014006\n",
            "Epoch 105 : Loss = 0.20617097311399193\n",
            "Epoch 106 : Loss = 0.20572017031637105\n",
            "Epoch 107 : Loss = 0.2052959129214287\n",
            "Epoch 108 : Loss = 0.20489981770515436\n",
            "Epoch 109 : Loss = 0.20453246818347415\n",
            "Epoch 110 : Loss = 0.20418589155782355\n",
            "Epoch 111 : Loss = 0.20386365191503006\n",
            "Epoch 112 : Loss = 0.2035644467581402\n",
            "Epoch 113 : Loss = 0.20328161357478663\n",
            "Epoch 114 : Loss = 0.20301808145913205\n",
            "Epoch 115 : Loss = 0.20277246189388357\n",
            "Epoch 116 : Loss = 0.2025446661493995\n",
            "Epoch 117 : Loss = 0.20232755894010712\n",
            "Epoch 118 : Loss = 0.20212797922166909\n",
            "Epoch 119 : Loss = 0.20194052566181528\n",
            "Epoch 120 : Loss = 0.20176380974325264\n",
            "Epoch 121 : Loss = 0.20160207965157245\n",
            "Epoch 122 : Loss = 0.2014465023848144\n",
            "Epoch 123 : Loss = 0.20130356028676039\n",
            "Epoch 124 : Loss = 0.2011687802997502\n",
            "Epoch 125 : Loss = 0.201043307103894\n",
            "Epoch 126 : Loss = 0.20092580704526464\n",
            "Epoch 127 : Loss = 0.20081595839424568\n",
            "Epoch 128 : Loss = 0.20071391354907642\n",
            "Epoch 129 : Loss = 0.20061660659584138\n",
            "Epoch 130 : Loss = 0.20052552798932247\n",
            "Epoch 131 : Loss = 0.20044156231663438\n",
            "Epoch 132 : Loss = 0.20036263492974363\n",
            "Epoch 133 : Loss = 0.20028853822838175\n",
            "Epoch 134 : Loss = 0.20021984421394085\n",
            "Epoch 135 : Loss = 0.20015371827916667\n",
            "Epoch 136 : Loss = 0.20009263300082905\n",
            "Epoch 137 : Loss = 0.20003603635863818\n",
            "Epoch 138 : Loss = 0.19998118518428368\n",
            "Epoch 139 : Loss = 0.19993083924055105\n",
            "Epoch 140 : Loss = 0.19988371364094992\n",
            "Epoch 141 : Loss = 0.1998390910977667\n",
            "Epoch 142 : Loss = 0.19979720562696454\n",
            "Epoch 143 : Loss = 0.19975880025462675\n",
            "Epoch 144 : Loss = 0.19972083480520686\n",
            "Epoch 145 : Loss = 0.19968682256611905\n",
            "Epoch 146 : Loss = 0.19965416836467656\n",
            "Epoch 147 : Loss = 0.19962341202930975\n",
            "Epoch 148 : Loss = 0.19959396327083762\n",
            "Epoch 149 : Loss = 0.19956654648889197\n",
            "Epoch 150 : Loss = 0.1995409300381487\n",
            "Epoch 151 : Loss = 0.19951655952767888\n",
            "Epoch 152 : Loss = 0.19949365915222603\n",
            "Epoch 153 : Loss = 0.199473212049766\n",
            "Epoch 154 : Loss = 0.199450979178602\n",
            "Epoch 155 : Loss = 0.19943178174170584\n",
            "Epoch 156 : Loss = 0.19941383261572235\n",
            "Epoch 157 : Loss = 0.19939618523825295\n",
            "Epoch 158 : Loss = 0.19937963343479412\n",
            "Epoch 159 : Loss = 0.19936388663270257\n",
            "Epoch 160 : Loss = 0.19934952428395097\n",
            "Epoch 161 : Loss = 0.1993351196023551\n",
            "Epoch 162 : Loss = 0.19932234354994519\n",
            "Epoch 163 : Loss = 0.19930891455574465\n",
            "Epoch 164 : Loss = 0.19929658858613536\n",
            "Epoch 165 : Loss = 0.19928500225598156\n",
            "Epoch 166 : Loss = 0.19927454231814906\n",
            "Epoch 167 : Loss = 0.19926286590370265\n",
            "Epoch 168 : Loss = 0.19925284927541562\n",
            "Epoch 169 : Loss = 0.19924317876046357\n",
            "Epoch 170 : Loss = 0.1992340406233614\n",
            "Epoch 171 : Loss = 0.19922515411268582\n",
            "Epoch 172 : Loss = 0.1992164579304782\n",
            "Epoch 173 : Loss = 0.19920774921774864\n",
            "Epoch 174 : Loss = 0.19919961318373683\n",
            "Epoch 175 : Loss = 0.19919218698685823\n",
            "Epoch 176 : Loss = 0.19918466156179257\n",
            "Epoch 177 : Loss = 0.19917734068903062\n",
            "Epoch 178 : Loss = 0.19917076148770074\n",
            "Epoch 179 : Loss = 0.1991632892326875\n",
            "Epoch 180 : Loss = 0.1991566985168241\n",
            "Epoch 181 : Loss = 0.19915027848698874\n",
            "Epoch 182 : Loss = 0.1991448422724551\n",
            "Epoch 183 : Loss = 0.1991383426568725\n",
            "Epoch 184 : Loss = 0.19913189045407564\n",
            "Epoch 185 : Loss = 0.1991258151829243\n",
            "Epoch 186 : Loss = 0.1991203329102559\n",
            "Epoch 187 : Loss = 0.19911547648635786\n",
            "Epoch 188 : Loss = 0.19910895621234717\n",
            "Epoch 189 : Loss = 0.1991046301343225\n",
            "Epoch 190 : Loss = 0.19909872758117586\n",
            "Epoch 191 : Loss = 0.19909387285059152\n",
            "Epoch 192 : Loss = 0.1990893137725917\n",
            "Epoch 193 : Loss = 0.19908314435319466\n",
            "Epoch 194 : Loss = 0.199078553440896\n",
            "Epoch 195 : Loss = 0.1990735110911456\n",
            "Epoch 196 : Loss = 0.19906835190274502\n",
            "Epoch 197 : Loss = 0.19906384091485624\n",
            "Epoch 198 : Loss = 0.1990593160417947\n",
            "Epoch 199 : Loss = 0.1990546936338598\n",
            "Epoch 200 : Loss = 0.19904984161257744\n",
            "Epoch 201 : Loss = 0.1990453313020142\n",
            "Epoch 202 : Loss = 0.19904103434898632\n",
            "Epoch 203 : Loss = 0.19903665611689747\n",
            "Epoch 204 : Loss = 0.19903169335289436\n",
            "Epoch 205 : Loss = 0.19902718236500563\n",
            "Epoch 206 : Loss = 0.19902300936254588\n",
            "Epoch 207 : Loss = 0.19901908967982637\n",
            "Epoch 208 : Loss = 0.19901370697400783\n",
            "Epoch 209 : Loss = 0.19900967011397533\n",
            "Epoch 210 : Loss = 0.199005478485064\n",
            "Epoch 211 : Loss = 0.19900114834308624\n",
            "Epoch 212 : Loss = 0.19899699159643872\n",
            "Epoch 213 : Loss = 0.19899252903732384\n",
            "Epoch 214 : Loss = 0.19898820329796188\n",
            "Epoch 215 : Loss = 0.19898367469960995\n",
            "Epoch 216 : Loss = 0.1989795413206924\n",
            "Epoch 217 : Loss = 0.19897566363215444\n",
            "Epoch 218 : Loss = 0.1989712522111156\n",
            "Epoch 219 : Loss = 0.19896715235981072\n",
            "Epoch 220 : Loss = 0.1989621455696496\n",
            "Epoch 221 : Loss = 0.19895806129683152\n",
            "Epoch 222 : Loss = 0.19895365834236142\n",
            "Epoch 223 : Loss = 0.1989494043995034\n",
            "Epoch 224 : Loss = 0.19894575395367367\n",
            "Epoch 225 : Loss = 0.198940822346644\n",
            "Epoch 226 : Loss = 0.19893730059266088\n",
            "Epoch 227 : Loss = 0.19893251867456874\n",
            "Epoch 228 : Loss = 0.19892845133488835\n",
            "Epoch 229 : Loss = 0.19892410730773752\n",
            "Epoch 230 : Loss = 0.19891986284743657\n",
            "Epoch 231 : Loss = 0.19891551882028582\n",
            "Epoch 232 : Loss = 0.19891204752705316\n",
            "Epoch 233 : Loss = 0.1989069933241064\n",
            "Epoch 234 : Loss = 0.1989028531719338\n",
            "Epoch 235 : Loss = 0.19889911670576446\n",
            "Epoch 236 : Loss = 0.19889411228624254\n",
            "Epoch 237 : Loss = 0.19888961078091097\n",
            "Epoch 238 : Loss = 0.19888597591356794\n",
            "Epoch 239 : Loss = 0.1988814496858554\n",
            "Epoch 240 : Loss = 0.19887697730552062\n",
            "Epoch 241 : Loss = 0.19887343319979583\n",
            "Epoch 242 : Loss = 0.19886827130209322\n",
            "Epoch 243 : Loss = 0.19886414842172104\n",
            "Epoch 244 : Loss = 0.198859552429481\n",
            "Epoch 245 : Loss = 0.19885592433539306\n",
            "Epoch 246 : Loss = 0.1988508725030855\n",
            "Epoch 247 : Loss = 0.19884702461687007\n",
            "Epoch 248 : Loss = 0.19884187694300304\n",
            "Epoch 249 : Loss = 0.1988380422646349\n",
            "Epoch 250 : Loss = 0.19883344390175553\n",
            "Epoch 251 : Loss = 0.19882855157960544\n",
            "Epoch 252 : Loss = 0.19882439754225986\n",
            "Epoch 253 : Loss = 0.19882030548019844\n",
            "Epoch 254 : Loss = 0.19881591133096002\n",
            "Epoch 255 : Loss = 0.19881147823550485\n",
            "Epoch 256 : Loss = 0.19880623742938042\n",
            "Epoch 257 : Loss = 0.19880197264931423\n",
            "Epoch 258 : Loss = 0.19879771904511884\n",
            "Epoch 259 : Loss = 0.19879357753829524\n",
            "Epoch 260 : Loss = 0.1987884193658829\n",
            "Epoch 261 : Loss = 0.19878435642881825\n",
            "Epoch 262 : Loss = 0.19877940314737236\n",
            "Epoch 263 : Loss = 0.19877515733242038\n",
            "Epoch 264 : Loss = 0.19877083497968587\n",
            "Epoch 265 : Loss = 0.19876590167934247\n",
            "Epoch 266 : Loss = 0.1987617848948999\n",
            "Epoch 267 : Loss = 0.19875638390129255\n",
            "Epoch 268 : Loss = 0.19875300506299193\n",
            "Epoch 269 : Loss = 0.19874742457812483\n",
            "Epoch 270 : Loss = 0.19874278049577362\n",
            "Epoch 271 : Loss = 0.19873821464451874\n",
            "Epoch 272 : Loss = 0.19873396950689234\n",
            "Epoch 273 : Loss = 0.19872896779667246\n",
            "Epoch 274 : Loss = 0.19872398877685718\n",
            "Epoch 275 : Loss = 0.19871959564360706\n",
            "Epoch 276 : Loss = 0.19871511174873877\n",
            "Epoch 277 : Loss = 0.1987095922231675\n",
            "Epoch 278 : Loss = 0.19870512763207612\n",
            "Epoch 279 : Loss = 0.19870040091601282\n",
            "Epoch 280 : Loss = 0.19869621199640364\n",
            "Epoch 281 : Loss = 0.19869098609144034\n",
            "Epoch 282 : Loss = 0.19868621467189354\n",
            "Epoch 283 : Loss = 0.19868189130317074\n",
            "Epoch 284 : Loss = 0.19867647439241407\n",
            "Epoch 285 : Loss = 0.19867239824750205\n",
            "Epoch 286 : Loss = 0.19866646792400972\n",
            "Epoch 287 : Loss = 0.1986625563691963\n",
            "Epoch 288 : Loss = 0.19865744357759302\n",
            "Epoch 289 : Loss = 0.1986524543978951\n",
            "Epoch 290 : Loss = 0.19864718345078555\n",
            "Epoch 291 : Loss = 0.1986426369033077\n",
            "Epoch 292 : Loss = 0.19863770936023098\n",
            "Epoch 293 : Loss = 0.19863268902355974\n",
            "Epoch 294 : Loss = 0.19862812859090895\n",
            "Epoch 295 : Loss = 0.19862296296791596\n",
            "Epoch 296 : Loss = 0.19861796870827675\n",
            "Epoch 297 : Loss = 0.1986129473556172\n",
            "Epoch 298 : Loss = 0.19860805774276905\n",
            "Epoch 299 : Loss = 0.19860318946567446\n",
            "Epoch 300 : Loss = 0.1985977149822495\n",
            "Epoch 301 : Loss = 0.19859292154962363\n",
            "Epoch 302 : Loss = 0.19858731498772442\n",
            "Epoch 303 : Loss = 0.19858201728625735\n",
            "Epoch 304 : Loss = 0.19857764548876072\n",
            "Epoch 305 : Loss = 0.19857232408090073\n",
            "Epoch 306 : Loss = 0.19856736267154868\n",
            "Epoch 307 : Loss = 0.19856208495118396\n",
            "Epoch 308 : Loss = 0.19855690307237886\n",
            "Epoch 309 : Loss = 0.1985517899421128\n",
            "Epoch 310 : Loss = 0.19854671948335387\n",
            "Epoch 311 : Loss = 0.19854155555367464\n",
            "Epoch 312 : Loss = 0.1985370201820677\n",
            "Epoch 313 : Loss = 0.198531222614375\n",
            "Epoch 314 : Loss = 0.19852573797106746\n",
            "Epoch 315 : Loss = 0.19852082261985002\n",
            "Epoch 316 : Loss = 0.19851530851288277\n",
            "Epoch 317 : Loss = 0.1985103193331848\n",
            "Epoch 318 : Loss = 0.1985048021782528\n",
            "Epoch 319 : Loss = 0.1984995925291018\n",
            "Epoch 320 : Loss = 0.19849379834803668\n",
            "Epoch 321 : Loss = 0.198488672348586\n",
            "Epoch 322 : Loss = 0.1984845948490229\n",
            "Epoch 323 : Loss = 0.1984778107567267\n",
            "Epoch 324 : Loss = 0.19847254861484873\n",
            "Epoch 325 : Loss = 0.19846706126223912\n",
            "Epoch 326 : Loss = 0.1984615264968439\n",
            "Epoch 327 : Loss = 0.19845664568922738\n",
            "Epoch 328 : Loss = 0.1984507969834588\n",
            "Epoch 329 : Loss = 0.19844539802182803\n",
            "Epoch 330 : Loss = 0.19843994385816835\n",
            "Epoch 331 : Loss = 0.19843456758694217\n",
            "Epoch 332 : Loss = 0.19842847639864136\n",
            "Epoch 333 : Loss = 0.19842317361723288\n",
            "Epoch 334 : Loss = 0.1984175836498087\n",
            "Epoch 335 : Loss = 0.19841218434951519\n",
            "Epoch 336 : Loss = 0.19840668107975612\n",
            "Epoch 337 : Loss = 0.1984008415178819\n",
            "Epoch 338 : Loss = 0.19839522242546076\n",
            "Epoch 339 : Loss = 0.19838968495076353\n",
            "Epoch 340 : Loss = 0.1983841142871163\n",
            "Epoch 341 : Loss = 0.19837875698100435\n",
            "Epoch 342 : Loss = 0.19837237691337414\n",
            "Epoch 343 : Loss = 0.198366835036061\n",
            "Epoch 344 : Loss = 0.19836139170960948\n",
            "Epoch 345 : Loss = 0.19835526563904501\n",
            "Epoch 346 : Loss = 0.19834936342456122\n",
            "Epoch 347 : Loss = 0.19834373654289683\n",
            "Epoch 348 : Loss = 0.19833751361478458\n",
            "Epoch 349 : Loss = 0.1983319385485216\n",
            "Epoch 350 : Loss = 0.19832622429186647\n",
            "Epoch 351 : Loss = 0.1983203302052888\n",
            "Epoch 352 : Loss = 0.1983144174922597\n",
            "Epoch 353 : Loss = 0.19830858842893076\n",
            "Epoch 354 : Loss = 0.19830300117080862\n",
            "Epoch 355 : Loss = 0.1982967680828138\n",
            "Epoch 356 : Loss = 0.19829089228402486\n",
            "Epoch 357 : Loss = 0.1982851505956866\n",
            "Epoch 358 : Loss = 0.19827928597276864\n",
            "Epoch 359 : Loss = 0.1982727606188167\n",
            "Epoch 360 : Loss = 0.19826678152788763\n",
            "Epoch 361 : Loss = 0.1982614895836873\n",
            "Epoch 362 : Loss = 0.19825466044924478\n",
            "Epoch 363 : Loss = 0.19824849204583603\n",
            "Epoch 364 : Loss = 0.19824342836033215\n",
            "Epoch 365 : Loss = 0.19823653826659376\n",
            "Epoch 366 : Loss = 0.1982301690361717\n",
            "Epoch 367 : Loss = 0.19822477109052916\n",
            "Epoch 368 : Loss = 0.1982190825722434\n",
            "Epoch 369 : Loss = 0.19821204245090487\n",
            "Epoch 370 : Loss = 0.19820574704896315\n",
            "Epoch 371 : Loss = 0.1981995390220122\n",
            "Epoch 372 : Loss = 0.19819401272318585\n",
            "Epoch 373 : Loss = 0.19818683679808272\n",
            "Epoch 374 : Loss = 0.19818082079291335\n",
            "Epoch 375 : Loss = 0.1981743469157002\n",
            "Epoch 376 : Loss = 0.19816849008202556\n",
            "Epoch 377 : Loss = 0.19816182215105407\n",
            "Epoch 378 : Loss = 0.1981552984904159\n",
            "Epoch 379 : Loss = 0.19814932312477723\n",
            "Epoch 380 : Loss = 0.19814274460077289\n",
            "Epoch 381 : Loss = 0.19813644208691336\n",
            "Epoch 382 : Loss = 0.19812990217046306\n",
            "Epoch 383 : Loss = 0.1981234885752201\n",
            "Epoch 384 : Loss = 0.19811709496107968\n",
            "Epoch 385 : Loss = 0.19811001961881464\n",
            "Epoch 386 : Loss = 0.1981039981950413\n",
            "Epoch 387 : Loss = 0.1980974989181216\n",
            "Epoch 388 : Loss = 0.19809076460925013\n",
            "Epoch 389 : Loss = 0.19808418235995554\n",
            "Epoch 390 : Loss = 0.19807740842754193\n",
            "Epoch 391 : Loss = 0.19807093556631694\n",
            "Epoch 392 : Loss = 0.19806394996968177\n",
            "Epoch 393 : Loss = 0.19805754484100768\n",
            "Epoch 394 : Loss = 0.19805083119056438\n",
            "Epoch 395 : Loss = 0.198044074868614\n",
            "Epoch 396 : Loss = 0.19803729958154934\n",
            "Epoch 397 : Loss = 0.19803074950521649\n",
            "Epoch 398 : Loss = 0.19802370091730898\n",
            "Epoch 399 : Loss = 0.19801724973050033\n",
            "Epoch 400 : Loss = 0.19801071049137553\n",
            "Epoch 401 : Loss = 0.19800368425520984\n",
            "Epoch 402 : Loss = 0.19799725440415464\n",
            "Epoch 403 : Loss = 0.1979901512915438\n",
            "Epoch 404 : Loss = 0.1979831165888092\n",
            "Epoch 405 : Loss = 0.19797656718980183\n",
            "Epoch 406 : Loss = 0.19796878505836835\n",
            "Epoch 407 : Loss = 0.19796276329593224\n",
            "Epoch 408 : Loss = 0.19795492867177178\n",
            "Epoch 409 : Loss = 0.19794817675243723\n",
            "Epoch 410 : Loss = 0.19794065268202266\n",
            "Epoch 411 : Loss = 0.19793350892988112\n",
            "Epoch 412 : Loss = 0.19792721894654358\n",
            "Epoch 413 : Loss = 0.1979209320111709\n",
            "Epoch 414 : Loss = 0.19791276380419734\n",
            "Epoch 415 : Loss = 0.19790555739944626\n",
            "Epoch 416 : Loss = 0.1978985399685123\n",
            "Epoch 417 : Loss = 0.19789083335887303\n",
            "Epoch 418 : Loss = 0.19788373160091316\n",
            "Epoch 419 : Loss = 0.19787675480950964\n",
            "Epoch 420 : Loss = 0.1978693905879151\n",
            "Epoch 421 : Loss = 0.1978621117093346\n",
            "Epoch 422 : Loss = 0.19785471396012744\n",
            "Epoch 423 : Loss = 0.19784732637080277\n",
            "Epoch 424 : Loss = 0.19784008745442727\n",
            "Epoch 425 : Loss = 0.19783265143632886\n",
            "Epoch 426 : Loss = 0.19782619550824163\n",
            "Epoch 427 : Loss = 0.19781817902218224\n",
            "Epoch 428 : Loss = 0.1978104497221383\n",
            "Epoch 429 : Loss = 0.1978030367331071\n",
            "Epoch 430 : Loss = 0.1977954804897308\n",
            "Epoch 431 : Loss = 0.1977879472754218\n",
            "Epoch 432 : Loss = 0.19777997210621837\n",
            "Epoch 433 : Loss = 0.1977726559747349\n",
            "Epoch 434 : Loss = 0.1977650221775878\n",
            "Epoch 435 : Loss = 0.19775720855051823\n",
            "Epoch 436 : Loss = 0.19774991036816078\n",
            "Epoch 437 : Loss = 0.19774245470762253\n",
            "Epoch 438 : Loss = 0.1977343383160504\n",
            "Epoch 439 : Loss = 0.1977270949970592\n",
            "Epoch 440 : Loss = 0.1977186964994127\n",
            "Epoch 441 : Loss = 0.1977112052792852\n",
            "Epoch 442 : Loss = 0.19770295579325076\n",
            "Epoch 443 : Loss = 0.19769528880715376\n",
            "Epoch 444 : Loss = 0.1976879090070725\n",
            "Epoch 445 : Loss = 0.19767997447739943\n",
            "Epoch 446 : Loss = 0.1976714046163993\n",
            "Epoch 447 : Loss = 0.19766353206201032\n",
            "Epoch 448 : Loss = 0.19765604592182423\n",
            "Epoch 449 : Loss = 0.19764824347062548\n",
            "Epoch 450 : Loss = 0.1976397565820001\n",
            "Epoch 451 : Loss = 0.19763166186484427\n",
            "Epoch 452 : Loss = 0.1976236027072777\n",
            "Epoch 453 : Loss = 0.1976152634756132\n",
            "Epoch 454 : Loss = 0.19760798425836998\n",
            "Epoch 455 : Loss = 0.19759899208491502\n",
            "Epoch 456 : Loss = 0.1975910721177404\n",
            "Epoch 457 : Loss = 0.1975828331302513\n",
            "Epoch 458 : Loss = 0.1975745751776479\n",
            "Epoch 459 : Loss = 0.19756688075986772\n",
            "Epoch 460 : Loss = 0.19755803150209508\n",
            "Epoch 461 : Loss = 0.19754937223412772\n",
            "Epoch 462 : Loss = 0.19754163446751508\n",
            "Epoch 463 : Loss = 0.19753278791904455\n",
            "Epoch 464 : Loss = 0.19752440567721027\n",
            "Epoch 465 : Loss = 0.1975169300355695\n",
            "Epoch 466 : Loss = 0.19750716063109308\n",
            "Epoch 467 : Loss = 0.19749902696772045\n",
            "Epoch 468 : Loss = 0.19749039309945973\n",
            "Epoch 469 : Loss = 0.19748197834600106\n",
            "Epoch 470 : Loss = 0.19747329672629185\n",
            "Epoch 471 : Loss = 0.19746478105133236\n",
            "Epoch 472 : Loss = 0.19745634530078282\n",
            "Epoch 473 : Loss = 0.1974473710764538\n",
            "Epoch 474 : Loss = 0.1974392489276149\n",
            "Epoch 475 : Loss = 0.1974304988980293\n",
            "Epoch 476 : Loss = 0.1974208947609771\n",
            "Epoch 477 : Loss = 0.19741297953508116\n",
            "Epoch 478 : Loss = 0.19740358367562288\n",
            "Epoch 479 : Loss = 0.1973950012841008\n",
            "Epoch 480 : Loss = 0.19738632101904263\n",
            "Epoch 481 : Loss = 0.197376913306388\n",
            "Epoch 482 : Loss = 0.19736779785969036\n",
            "Epoch 483 : Loss = 0.19735912098125982\n",
            "Epoch 484 : Loss = 0.19735019484704192\n",
            "Epoch 485 : Loss = 0.19734072143381293\n",
            "Epoch 486 : Loss = 0.1973318457603454\n",
            "Epoch 487 : Loss = 0.1973226602104577\n",
            "Epoch 488 : Loss = 0.1973135735500942\n",
            "Epoch 489 : Loss = 0.19730409675023766\n",
            "Epoch 490 : Loss = 0.19729552045464513\n",
            "Epoch 491 : Loss = 0.19728590310974553\n",
            "Epoch 492 : Loss = 0.19727693091739307\n",
            "Epoch 493 : Loss = 0.1972676525739106\n",
            "Epoch 494 : Loss = 0.19725862585685475\n",
            "Epoch 495 : Loss = 0.19724879684773347\n",
            "Epoch 496 : Loss = 0.19723920761184255\n",
            "Epoch 497 : Loss = 0.1972307081926953\n",
            "Epoch 498 : Loss = 0.19722079079259525\n",
            "Epoch 499 : Loss = 0.1972113857892427\n",
            "Epoch 500 : Loss = 0.19720142131501972\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzLhTvpq1zft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b283eaff-95c4-4f59-8143-2405800019d4"
      },
      "source": [
        "test_loss = 0.0\n",
        "\n",
        "for data in testloader:\n",
        "        \n",
        "    X, _ = data[0].to(device), data[1].to(device)\n",
        "    with torch.no_grad():\n",
        "      X_pred = ae1(X)\n",
        "      loss = criterion(X_pred, X)\n",
        "\n",
        "    test_loss += loss.item()*len(X)/test_size\n",
        "\n",
        "test_loss = test_loss\n",
        "print(test_loss)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.20566899261691352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQpvQyBK5CnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Z1_array = []\n",
        "\n",
        "for data in trainloader:\n",
        "        \n",
        "    X, _ = data[0].to(device), data[1].to(device)\n",
        "    with torch.no_grad():\n",
        "      Z1 = ae1.get_z(X)\n",
        "\n",
        "    if len(Z1_array) == 0:\n",
        "        Z1_array = Z1.cpu().numpy()\n",
        "    else:\n",
        "        Z1_array = np.append(Z1_array, Z1.cpu().numpy(), axis=0)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA2o4c0KivOE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "64a31b75-d0ef-4308-f3a9-c9994b1f747a"
      },
      "source": [
        "pca2 = PCA(n_components=Z1_array.shape[1])\n",
        "pca2.fit(Z1_array)\n",
        "print('97% Variance Explained:', np.where(np.cumsum(pca2.explained_variance_ratio_)>=0.97)[0][0]+1)\n",
        "\n",
        "explained_var = np.cumsum(pca2.explained_variance_ratio_)\n",
        "plt.plot(explained_var)\n",
        "plt.show()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97% Variance Explained: 85\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwcvgUCVb9Df",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ae2 = AutoEncoder(200, [150, 75, 150])\n",
        "optimizer2 = optim.SGD(ae2.parameters(), lr=0.0001, momentum=0.9)\n",
        "ae2 = ae2.to(device)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J144k8cub9Dn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a7639448-8479-4484-e133-3359bd34893f"
      },
      "source": [
        "old_loss = np.inf\n",
        "\n",
        "max_epoch = 500\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for data in trainloader:\n",
        "        \n",
        "        X, _ = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            Z = ae1.get_z(X)\n",
        "        \n",
        "        optimizer2.zero_grad()\n",
        "        \n",
        "        # Reconstructed Representation of X (forward)\n",
        "        Z_hat = ae2(Z)\n",
        "        \n",
        "        # Calculate Loss (MSE)\n",
        "        loss = criterion(Z_hat, Z)\n",
        "        \n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update Parameters\n",
        "        optimizer2.step()\n",
        "        \n",
        "        running_loss += loss.item()*len(X)/train_size\n",
        "    \n",
        "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
        "    \n",
        "    if abs(running_loss-old_loss)/running_loss < 1e-5:\n",
        "        print('Converged')\n",
        "        break\n",
        "        \n",
        "    old_loss = running_loss\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Loss = 2.108018495819786\n",
            "Epoch 2 : Loss = 2.075668237426065\n",
            "Epoch 3 : Loss = 2.0419576303525404\n",
            "Epoch 4 : Loss = 2.0073560151186856\n",
            "Epoch 5 : Loss = 1.9709010882811115\n",
            "Epoch 6 : Loss = 1.9318041720173573\n",
            "Epoch 7 : Loss = 1.889410598711534\n",
            "Epoch 8 : Loss = 1.8432131816040387\n",
            "Epoch 9 : Loss = 1.792929448864677\n",
            "Epoch 10 : Loss = 1.738418703729457\n",
            "Epoch 11 : Loss = 1.6797663461078294\n",
            "Epoch 12 : Loss = 1.617145549167286\n",
            "Epoch 13 : Loss = 1.550958955829794\n",
            "Epoch 14 : Loss = 1.481752287257801\n",
            "Epoch 15 : Loss = 1.4101453504779122\n",
            "Epoch 16 : Loss = 1.3368813801895487\n",
            "Epoch 17 : Loss = 1.2626744481650265\n",
            "Epoch 18 : Loss = 1.188277060335333\n",
            "Epoch 19 : Loss = 1.1144099100069567\n",
            "Epoch 20 : Loss = 1.041731755841862\n",
            "Epoch 21 : Loss = 0.9707777554338629\n",
            "Epoch 22 : Loss = 0.9021019542759113\n",
            "Epoch 23 : Loss = 0.8359965085983275\n",
            "Epoch 24 : Loss = 0.772885552861474\n",
            "Epoch 25 : Loss = 0.7128673385490073\n",
            "Epoch 26 : Loss = 0.6561448154124346\n",
            "Epoch 27 : Loss = 0.6027738831259988\n",
            "Epoch 28 : Loss = 0.5528104156255722\n",
            "Epoch 29 : Loss = 0.5061306262558157\n",
            "Epoch 30 : Loss = 0.4627394168214365\n",
            "Epoch 31 : Loss = 0.4225169196724893\n",
            "Epoch 32 : Loss = 0.38534302399917075\n",
            "Epoch 33 : Loss = 0.351044666360725\n",
            "Epoch 34 : Loss = 0.3195462775501339\n",
            "Epoch 35 : Loss = 0.2905947003852238\n",
            "Epoch 36 : Loss = 0.2641205625100569\n",
            "Epoch 37 : Loss = 0.23987687277522962\n",
            "Epoch 38 : Loss = 0.21778657253492958\n",
            "Epoch 39 : Loss = 0.19765431230718436\n",
            "Epoch 40 : Loss = 0.1793233369561759\n",
            "Epoch 41 : Loss = 0.16267765550450844\n",
            "Epoch 42 : Loss = 0.14755487374284046\n",
            "Epoch 43 : Loss = 0.13386127877641807\n",
            "Epoch 44 : Loss = 0.1214539511975917\n",
            "Epoch 45 : Loss = 0.11022837561639871\n",
            "Epoch 46 : Loss = 0.10007091391492974\n",
            "Epoch 47 : Loss = 0.09090437960218299\n",
            "Epoch 48 : Loss = 0.08262076127258217\n",
            "Epoch 49 : Loss = 0.07515383528714831\n",
            "Epoch 50 : Loss = 0.0684152107516473\n",
            "Epoch 51 : Loss = 0.062340915287760174\n",
            "Epoch 52 : Loss = 0.056875028816813764\n",
            "Epoch 53 : Loss = 0.05194965648380192\n",
            "Epoch 54 : Loss = 0.047515169124711654\n",
            "Epoch 55 : Loss = 0.043523266657509586\n",
            "Epoch 56 : Loss = 0.039934768002818914\n",
            "Epoch 57 : Loss = 0.03670954509553584\n",
            "Epoch 58 : Loss = 0.0338067919947207\n",
            "Epoch 59 : Loss = 0.031197639605538414\n",
            "Epoch 60 : Loss = 0.02885404804890806\n",
            "Epoch 61 : Loss = 0.026746914849023935\n",
            "Epoch 62 : Loss = 0.024854959877715868\n",
            "Epoch 63 : Loss = 0.023150399750606582\n",
            "Epoch 64 : Loss = 0.02162568101828749\n",
            "Epoch 65 : Loss = 0.02025217914276502\n",
            "Epoch 66 : Loss = 0.019020112506537273\n",
            "Epoch 67 : Loss = 0.017911355349827893\n",
            "Epoch 68 : Loss = 0.016918382501568307\n",
            "Epoch 69 : Loss = 0.016023577910593962\n",
            "Epoch 70 : Loss = 0.015221611236814751\n",
            "Epoch 71 : Loss = 0.01450066806071184\n",
            "Epoch 72 : Loss = 0.013852426062592052\n",
            "Epoch 73 : Loss = 0.013271515617485751\n",
            "Epoch 74 : Loss = 0.012749906290661202\n",
            "Epoch 75 : Loss = 0.012278687780384314\n",
            "Epoch 76 : Loss = 0.01185701884837313\n",
            "Epoch 77 : Loss = 0.01147826210680333\n",
            "Epoch 78 : Loss = 0.011138532120226455\n",
            "Epoch 79 : Loss = 0.010831325738267466\n",
            "Epoch 80 : Loss = 0.010556902397762646\n",
            "Epoch 81 : Loss = 0.0103089886251837\n",
            "Epoch 82 : Loss = 0.010087701280347326\n",
            "Epoch 83 : Loss = 0.009887706086208876\n",
            "Epoch 84 : Loss = 0.009708092209290378\n",
            "Epoch 85 : Loss = 0.009545947276902469\n",
            "Epoch 86 : Loss = 0.009401408331045375\n",
            "Epoch 87 : Loss = 0.00927029302428392\n",
            "Epoch 88 : Loss = 0.009152993951416149\n",
            "Epoch 89 : Loss = 0.009047472034581006\n",
            "Epoch 90 : Loss = 0.008951509254984558\n",
            "Epoch 91 : Loss = 0.008866726092740215\n",
            "Epoch 92 : Loss = 0.008789304739118299\n",
            "Epoch 93 : Loss = 0.008720492044548419\n",
            "Epoch 94 : Loss = 0.008657532384280454\n",
            "Epoch 95 : Loss = 0.008601825049316341\n",
            "Epoch 96 : Loss = 0.008550622489896013\n",
            "Epoch 97 : Loss = 0.008505766883238473\n",
            "Epoch 98 : Loss = 0.008464286347258498\n",
            "Epoch 99 : Loss = 0.008428091246804053\n",
            "Epoch 100 : Loss = 0.008393884498879992\n",
            "Epoch 101 : Loss = 0.008364044828340413\n",
            "Epoch 102 : Loss = 0.008337493837726386\n",
            "Epoch 103 : Loss = 0.008312583699907092\n",
            "Epoch 104 : Loss = 0.008290668483823538\n",
            "Epoch 105 : Loss = 0.008271172833205625\n",
            "Epoch 106 : Loss = 0.00825303641613573\n",
            "Epoch 107 : Loss = 0.008236721337942237\n",
            "Epoch 108 : Loss = 0.0082224291770465\n",
            "Epoch 109 : Loss = 0.008209121604026719\n",
            "Epoch 110 : Loss = 0.008197548143057658\n",
            "Epoch 111 : Loss = 0.008186534195291724\n",
            "Epoch 112 : Loss = 0.008176834132014352\n",
            "Epoch 113 : Loss = 0.00816807719159194\n",
            "Epoch 114 : Loss = 0.008160229687663643\n",
            "Epoch 115 : Loss = 0.008152997724457897\n",
            "Epoch 116 : Loss = 0.008146714937703857\n",
            "Epoch 117 : Loss = 0.008140570346520031\n",
            "Epoch 118 : Loss = 0.008135316480713134\n",
            "Epoch 119 : Loss = 0.008130604528229345\n",
            "Epoch 120 : Loss = 0.00812614795921201\n",
            "Epoch 121 : Loss = 0.008122262417931446\n",
            "Epoch 122 : Loss = 0.008118600415235216\n",
            "Epoch 123 : Loss = 0.008115339812568642\n",
            "Epoch 124 : Loss = 0.008112329756841065\n",
            "Epoch 125 : Loss = 0.008110011758452114\n",
            "Epoch 126 : Loss = 0.00810717917259105\n",
            "Epoch 127 : Loss = 0.008104942971840503\n",
            "Epoch 128 : Loss = 0.00810295414306562\n",
            "Epoch 129 : Loss = 0.008101151695220982\n",
            "Epoch 130 : Loss = 0.00809939037373459\n",
            "Epoch 131 : Loss = 0.008097963217137889\n",
            "Epoch 132 : Loss = 0.00809651668268171\n",
            "Epoch 133 : Loss = 0.008095132229341702\n",
            "Epoch 134 : Loss = 0.00809391598555852\n",
            "Epoch 135 : Loss = 0.008092801689847627\n",
            "Epoch 136 : Loss = 0.0080917555923489\n",
            "Epoch 137 : Loss = 0.008090834429656918\n",
            "Epoch 138 : Loss = 0.00809019170066511\n",
            "Epoch 139 : Loss = 0.008089302277023142\n",
            "Epoch 140 : Loss = 0.008088395539247855\n",
            "Epoch 141 : Loss = 0.008087713504210118\n",
            "Epoch 142 : Loss = 0.008087228443896905\n",
            "Epoch 143 : Loss = 0.008086521771143784\n",
            "Epoch 144 : Loss = 0.008085849324495279\n",
            "Epoch 145 : Loss = 0.008085510323078117\n",
            "Epoch 146 : Loss = 0.008084824901412834\n",
            "Epoch 147 : Loss = 0.00808436245742169\n",
            "Epoch 148 : Loss = 0.00808389988643202\n",
            "Epoch 149 : Loss = 0.008083508709784259\n",
            "Epoch 150 : Loss = 0.008083097816614263\n",
            "Epoch 151 : Loss = 0.008082699623297563\n",
            "Epoch 152 : Loss = 0.008082317124882884\n",
            "Epoch 153 : Loss = 0.008082036151212047\n",
            "Epoch 154 : Loss = 0.008081627734513446\n",
            "Epoch 155 : Loss = 0.008081329859454525\n",
            "Epoch 156 : Loss = 0.00808093000457368\n",
            "Epoch 157 : Loss = 0.00808074050159617\n",
            "Epoch 158 : Loss = 0.008080365136265755\n",
            "Epoch 159 : Loss = 0.00808009616395628\n",
            "Epoch 160 : Loss = 0.008079790097491985\n",
            "Epoch 161 : Loss = 0.008079639805311508\n",
            "Epoch 162 : Loss = 0.008079228975640777\n",
            "Epoch 163 : Loss = 0.008079050373370677\n",
            "Epoch 164 : Loss = 0.008078917691653423\n",
            "Epoch 165 : Loss = 0.008078702291558413\n",
            "Epoch 166 : Loss = 0.008078181449408556\n",
            "Epoch 167 : Loss = 0.008077913577753034\n",
            "Epoch 168 : Loss = 0.008077726371332325\n",
            "Epoch 169 : Loss = 0.008077472638846799\n",
            "Epoch 170 : Loss = 0.008077231743796305\n",
            "Epoch 171 : Loss = 0.00807695611464706\n",
            "Epoch 172 : Loss = 0.008077091886661947\n",
            "Epoch 173 : Loss = 0.008076591639440845\n",
            "Epoch 174 : Loss = 0.008076248309490356\n",
            "Epoch 175 : Loss = 0.008076042413118888\n",
            "Epoch 176 : Loss = 0.008075951408086854\n",
            "Epoch 177 : Loss = 0.00807580363471061\n",
            "Epoch 178 : Loss = 0.008075452653098513\n",
            "Epoch 179 : Loss = 0.008075226585126735\n",
            "Epoch 180 : Loss = 0.00807507580611855\n",
            "Epoch 181 : Loss = 0.008074764035303484\n",
            "Epoch 182 : Loss = 0.00807448404587128\n",
            "Epoch 183 : Loss = 0.008074309010143306\n",
            "Epoch 184 : Loss = 0.008074078931134532\n",
            "Epoch 185 : Loss = 0.008073802010833542\n",
            "Epoch 186 : Loss = 0.00807369463357397\n",
            "Epoch 187 : Loss = 0.00807334363079545\n",
            "Epoch 188 : Loss = 0.00807324723890898\n",
            "Epoch 189 : Loss = 0.008072894627482376\n",
            "Epoch 190 : Loss = 0.008072746102698149\n",
            "Epoch 191 : Loss = 0.008072498709556054\n",
            "Epoch 192 : Loss = 0.008072365488095042\n",
            "Epoch 193 : Loss = 0.008072220349938354\n",
            "Epoch 194 : Loss = 0.008071909732693299\n",
            "Epoch 195 : Loss = 0.008071738549254158\n",
            "Epoch 196 : Loss = 0.008071398817595435\n",
            "Epoch 197 : Loss = 0.008071125135757029\n",
            "Epoch 198 : Loss = 0.008070898824371396\n",
            "Epoch 199 : Loss = 0.008070829959416931\n",
            "Converged\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VHpK73lgdGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ab50435-f2e7-4015-b005-a8f384e671ea"
      },
      "source": [
        "test_loss = 0.0\n",
        "\n",
        "for data in testloader:\n",
        "        \n",
        "    X, _ = data[0].to(device), data[1].to(device)\n",
        "    with torch.no_grad():\n",
        "      Z = ae1.get_z(X)\n",
        "      Z_pred = ae2(Z)\n",
        "      loss = criterion(Z_pred, Z)\n",
        "\n",
        "    test_loss += loss.item()*len(X)/test_size\n",
        "\n",
        "test_loss = test_loss\n",
        "print(test_loss)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.009037855123593046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie7JQXLshlWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Z2_array = []\n",
        "\n",
        "for data in trainloader:\n",
        "        \n",
        "    X, _ = data[0].to(device), data[1].to(device)\n",
        "    with torch.no_grad():\n",
        "      Z1 = ae1.get_z(X)\n",
        "      Z2 = ae2.get_z(Z1)\n",
        "\n",
        "    if len(Z2_array) == 0:\n",
        "        Z2_array = Z2.cpu().numpy()\n",
        "    else:\n",
        "        Z2_array = np.append(Z2_array, Z2.cpu().numpy(), axis=0)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySCjdA11h5nq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "9023ef16-1214-4fba-d641-92ba50c74254"
      },
      "source": [
        "pca3 = PCA(n_components=Z2_array.shape[1])\n",
        "pca3.fit(Z2_array)\n",
        "print('97% Variance Explained:', np.where(np.cumsum(pca3.explained_variance_ratio_)>=0.97)[0][0]+1)\n",
        "\n",
        "explained_var = np.cumsum(pca3.explained_variance_ratio_)\n",
        "plt.plot(explained_var)\n",
        "plt.show()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97% Variance Explained: 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVEyxUUBb9Dt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ae3 = AutoEncoder(75, [50, 15, 50])\n",
        "optimizer3 = optim.SGD(ae3.parameters(), lr=0.0001, momentum=0.9)\n",
        "ae3 = ae3.to(device)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV0pjnuNb9Dy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "756db0b6-3e20-4ab0-fe10-6d3b1fefe80e"
      },
      "source": [
        "old_loss = np.inf\n",
        "\n",
        "max_epoch = 500\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for data in trainloader:\n",
        "        \n",
        "        X, _ = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            Z1 = ae1.get_z(X)\n",
        "            Z = ae2.get_z(Z1)\n",
        "        \n",
        "        optimizer3.zero_grad()\n",
        "        \n",
        "        # Reconstructed Representation of Z (forward)\n",
        "        Z_hat = ae3(Z)\n",
        "        \n",
        "        # Calculate Loss (MSE)\n",
        "        loss = criterion(Z_hat, Z)\n",
        "        \n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update Parameters\n",
        "        optimizer3.step()\n",
        "        \n",
        "        running_loss += loss.item()*len(X)/train_size\n",
        "    \n",
        "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
        "    \n",
        "    if abs(running_loss-old_loss)/running_loss < 1e-5:\n",
        "        print('Converged')\n",
        "        break\n",
        "        \n",
        "    old_loss = running_loss\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Loss = 2.3820965777743948\n",
            "Epoch 2 : Loss = 2.3099342963912277\n",
            "Epoch 3 : Loss = 2.240768606012518\n",
            "Epoch 4 : Loss = 2.17479775168679\n",
            "Epoch 5 : Loss = 2.1094413670626553\n",
            "Epoch 6 : Loss = 2.0430152958089653\n",
            "Epoch 7 : Loss = 1.9745480147275063\n",
            "Epoch 8 : Loss = 1.9036514433947473\n",
            "Epoch 9 : Loss = 1.8302678547122266\n",
            "Epoch 10 : Loss = 1.754635645584627\n",
            "Epoch 11 : Loss = 1.6771458062258628\n",
            "Epoch 12 : Loss = 1.5982784574682065\n",
            "Epoch 13 : Loss = 1.518536239862442\n",
            "Epoch 14 : Loss = 1.4384969174861908\n",
            "Epoch 15 : Loss = 1.3586993921886783\n",
            "Epoch 16 : Loss = 1.2796725820411334\n",
            "Epoch 17 : Loss = 1.201946513219313\n",
            "Epoch 18 : Loss = 1.1259492039680483\n",
            "Epoch 19 : Loss = 1.0521085451949728\n",
            "Epoch 20 : Loss = 0.9807622825557535\n",
            "Epoch 21 : Loss = 0.9121949726885015\n",
            "Epoch 22 : Loss = 0.8466222272677854\n",
            "Epoch 23 : Loss = 0.7842071015726437\n",
            "Epoch 24 : Loss = 0.7250346216288482\n",
            "Epoch 25 : Loss = 0.6691761829636315\n",
            "Epoch 26 : Loss = 0.616626956246116\n",
            "Epoch 27 : Loss = 0.5673402168534019\n",
            "Epoch 28 : Loss = 0.5212812240828166\n",
            "Epoch 29 : Loss = 0.47833237051963795\n",
            "Epoch 30 : Loss = 0.4384097240187904\n",
            "Epoch 31 : Loss = 0.4013732678510925\n",
            "Epoch 32 : Loss = 0.3670932677659121\n",
            "Epoch 33 : Loss = 0.3354282839731737\n",
            "Epoch 34 : Loss = 0.3062294470992955\n",
            "Epoch 35 : Loss = 0.27935254573822016\n",
            "Epoch 36 : Loss = 0.254660213874145\n",
            "Epoch 37 : Loss = 0.23198714310472665\n",
            "Epoch 38 : Loss = 0.21120763637802817\n",
            "Epoch 39 : Loss = 0.19219064035198907\n",
            "Epoch 40 : Loss = 0.17479239031672472\n",
            "Epoch 41 : Loss = 0.1589052606035362\n",
            "Epoch 42 : Loss = 0.14440140974792567\n",
            "Epoch 43 : Loss = 0.13117394308474922\n",
            "Epoch 44 : Loss = 0.11911837350238454\n",
            "Epoch 45 : Loss = 0.10814446244727483\n",
            "Epoch 46 : Loss = 0.0981558712029999\n",
            "Epoch 47 : Loss = 0.08906620334495198\n",
            "Epoch 48 : Loss = 0.08081145652315834\n",
            "Epoch 49 : Loss = 0.07330827093259853\n",
            "Epoch 50 : Loss = 0.06649129744619132\n",
            "Epoch 51 : Loss = 0.06030628144402397\n",
            "Epoch 52 : Loss = 0.05468938694420185\n",
            "Epoch 53 : Loss = 0.04959839455444705\n",
            "Epoch 54 : Loss = 0.044977342286570486\n",
            "Epoch 55 : Loss = 0.040788232687522054\n",
            "Epoch 56 : Loss = 0.036992263726212775\n",
            "Epoch 57 : Loss = 0.033552119254388585\n",
            "Epoch 58 : Loss = 0.03043339532715353\n",
            "Epoch 59 : Loss = 0.027610170273956937\n",
            "Epoch 60 : Loss = 0.025050723036243158\n",
            "Epoch 61 : Loss = 0.02273408619856293\n",
            "Epoch 62 : Loss = 0.020637770860709923\n",
            "Epoch 63 : Loss = 0.01873653373596343\n",
            "Epoch 64 : Loss = 0.01701680168678815\n",
            "Epoch 65 : Loss = 0.015460796357894484\n",
            "Epoch 66 : Loss = 0.01405142381025309\n",
            "Epoch 67 : Loss = 0.012774435760961338\n",
            "Epoch 68 : Loss = 0.011620447230101987\n",
            "Epoch 69 : Loss = 0.010575656821443275\n",
            "Epoch 70 : Loss = 0.009629599453712055\n",
            "Epoch 71 : Loss = 0.008773042883893307\n",
            "Epoch 72 : Loss = 0.007998538277619942\n",
            "Epoch 73 : Loss = 0.007296943529085679\n",
            "Epoch 74 : Loss = 0.006663077394478024\n",
            "Epoch 75 : Loss = 0.006087867405519566\n",
            "Epoch 76 : Loss = 0.005568210162561049\n",
            "Epoch 77 : Loss = 0.0050976255337115035\n",
            "Epoch 78 : Loss = 0.004671369446441531\n",
            "Epoch 79 : Loss = 0.004285931650718505\n",
            "Epoch 80 : Loss = 0.003937308655374429\n",
            "Epoch 81 : Loss = 0.0036211971944960014\n",
            "Epoch 82 : Loss = 0.0033351548111320213\n",
            "Epoch 83 : Loss = 0.003075983608141541\n",
            "Epoch 84 : Loss = 0.002841713780071587\n",
            "Epoch 85 : Loss = 0.002629710197321732\n",
            "Epoch 86 : Loss = 0.002437217626720667\n",
            "Epoch 87 : Loss = 0.0022633467266463085\n",
            "Epoch 88 : Loss = 0.0021055191728836776\n",
            "Epoch 89 : Loss = 0.0019630555814894087\n",
            "Epoch 90 : Loss = 0.001833512262568217\n",
            "Epoch 91 : Loss = 0.0017165554798504513\n",
            "Epoch 92 : Loss = 0.0016107829023067925\n",
            "Epoch 93 : Loss = 0.0015143305169079788\n",
            "Epoch 94 : Loss = 0.0014273769709027624\n",
            "Epoch 95 : Loss = 0.0013484776110007338\n",
            "Epoch 96 : Loss = 0.0012771067532329735\n",
            "Epoch 97 : Loss = 0.0012123399369143458\n",
            "Epoch 98 : Loss = 0.0011536718838297843\n",
            "Epoch 99 : Loss = 0.001100563535477373\n",
            "Epoch 100 : Loss = 0.00105232393914114\n",
            "Epoch 101 : Loss = 0.0010086742510214788\n",
            "Epoch 102 : Loss = 0.000969193992204964\n",
            "Epoch 103 : Loss = 0.0009330962043763561\n",
            "Epoch 104 : Loss = 0.000900639661598358\n",
            "Epoch 105 : Loss = 0.0008711873868543822\n",
            "Epoch 106 : Loss = 0.0008444177166728132\n",
            "Epoch 107 : Loss = 0.0008201205485172316\n",
            "Epoch 108 : Loss = 0.0007981132776793942\n",
            "Epoch 109 : Loss = 0.0007782313630080106\n",
            "Epoch 110 : Loss = 0.0007600326895375144\n",
            "Epoch 111 : Loss = 0.0007437597642737359\n",
            "Epoch 112 : Loss = 0.0007287263076498427\n",
            "Epoch 113 : Loss = 0.0007152373767563735\n",
            "Epoch 114 : Loss = 0.0007030040889565663\n",
            "Epoch 115 : Loss = 0.0006918949240405876\n",
            "Epoch 116 : Loss = 0.0006817989507742989\n",
            "Epoch 117 : Loss = 0.0006726449236421929\n",
            "Epoch 118 : Loss = 0.0006643073667708613\n",
            "Epoch 119 : Loss = 0.0006567890020299025\n",
            "Epoch 120 : Loss = 0.0006499246235100806\n",
            "Epoch 121 : Loss = 0.0006437235927230425\n",
            "Epoch 122 : Loss = 0.000638066935194233\n",
            "Epoch 123 : Loss = 0.0006329286909683353\n",
            "Epoch 124 : Loss = 0.0006283450043569742\n",
            "Epoch 125 : Loss = 0.0006240593852866863\n",
            "Epoch 126 : Loss = 0.0006203250767694872\n",
            "Epoch 127 : Loss = 0.0006167550540836105\n",
            "Epoch 128 : Loss = 0.0006135862839238889\n",
            "Epoch 129 : Loss = 0.0006107407528113319\n",
            "Epoch 130 : Loss = 0.0006081237661419437\n",
            "Epoch 131 : Loss = 0.000605756866703318\n",
            "Epoch 132 : Loss = 0.0006035950407914987\n",
            "Epoch 133 : Loss = 0.0006016461927422575\n",
            "Epoch 134 : Loss = 0.0005998751366860233\n",
            "Epoch 135 : Loss = 0.0005983027532982469\n",
            "Epoch 136 : Loss = 0.0005967747301011431\n",
            "Epoch 137 : Loss = 0.0005954559108596932\n",
            "Epoch 138 : Loss = 0.0005942496129798451\n",
            "Epoch 139 : Loss = 0.0005931450359639712\n",
            "Epoch 140 : Loss = 0.000592175330067138\n",
            "Epoch 141 : Loss = 0.0005912307341614821\n",
            "Epoch 142 : Loss = 0.0005904248920904304\n",
            "Epoch 143 : Loss = 0.0005896875445614568\n",
            "Epoch 144 : Loss = 0.0005889925351683896\n",
            "Epoch 145 : Loss = 0.0005883681045485321\n",
            "Epoch 146 : Loss = 0.0005877958961222745\n",
            "Epoch 147 : Loss = 0.0005872853798791765\n",
            "Epoch 148 : Loss = 0.0005868103425283069\n",
            "Epoch 149 : Loss = 0.0005863854074215687\n",
            "Epoch 150 : Loss = 0.0005860019895904275\n",
            "Epoch 151 : Loss = 0.0005856612452506934\n",
            "Epoch 152 : Loss = 0.0005853231830935164\n",
            "Epoch 153 : Loss = 0.0005850396598609884\n",
            "Epoch 154 : Loss = 0.0005847960019309538\n",
            "Epoch 155 : Loss = 0.0005845278025266121\n",
            "Epoch 156 : Loss = 0.0005843137967696583\n",
            "Epoch 157 : Loss = 0.0005841355252249\n",
            "Epoch 158 : Loss = 0.0005839380901306869\n",
            "Epoch 159 : Loss = 0.0005838068569904531\n",
            "Epoch 160 : Loss = 0.0005836417033358223\n",
            "Epoch 161 : Loss = 0.0005834811699142761\n",
            "Epoch 162 : Loss = 0.0005833619933665349\n",
            "Epoch 163 : Loss = 0.0005832630267832427\n",
            "Epoch 164 : Loss = 0.0005831507005495952\n",
            "Epoch 165 : Loss = 0.0005830604085614058\n",
            "Epoch 166 : Loss = 0.00058296768309612\n",
            "Epoch 167 : Loss = 0.0005829109028458003\n",
            "Epoch 168 : Loss = 0.0005828345337332312\n",
            "Epoch 169 : Loss = 0.0005827524275280831\n",
            "Epoch 170 : Loss = 0.0005826909671833906\n",
            "Epoch 171 : Loss = 0.0005826349390025084\n",
            "Epoch 172 : Loss = 0.0005825984593353827\n",
            "Epoch 173 : Loss = 0.0005825484192676164\n",
            "Epoch 174 : Loss = 0.0005825006489679007\n",
            "Epoch 175 : Loss = 0.0005824827001726425\n",
            "Epoch 176 : Loss = 0.0005824301862792875\n",
            "Epoch 177 : Loss = 0.0005823918743937446\n",
            "Epoch 178 : Loss = 0.0005823633073305247\n",
            "Epoch 179 : Loss = 0.000582345200448551\n",
            "Epoch 180 : Loss = 0.0005823297397000714\n",
            "Epoch 181 : Loss = 0.0005822963781115091\n",
            "Epoch 182 : Loss = 0.0005822804080459967\n",
            "Epoch 183 : Loss = 0.0005822770568060645\n",
            "Converged\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el7d7FcLMTtr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2efab482-634c-4dca-f670-0bb47b6ae4bc"
      },
      "source": [
        "test_loss = 0.0\n",
        "\n",
        "for data in testloader:\n",
        "        \n",
        "    X, _ = data[0].to(device), data[1].to(device)\n",
        "    with torch.no_grad():\n",
        "      Z1 = ae1.get_z(X)\n",
        "      Z = ae2.get_z(Z1)\n",
        "      Z_pred = ae3(Z)\n",
        "      loss = criterion(Z_pred, Z)\n",
        "\n",
        "    test_loss += loss.item()*len(X)/test_size\n",
        "\n",
        "test_loss = test_loss\n",
        "print(test_loss)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0007135829628995534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asYo4AYBb9EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FinalNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
        "        super(FinalNet, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "        self.fc4 = nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
        "        self.out = nn.Linear(hidden_sizes[3], num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        x = torch.tanh(self.fc4(x))\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            y_score = self.forward(X)\n",
        "            y_pred = torch.argmax(y_score, axis=1)\n",
        "            \n",
        "        return y_pred\n",
        "            \n",
        "    \n",
        "classifier = FinalNet(828, [400, 150, 50, 15], 5)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azVOLzPcb9EQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ae1_params = list(ae1.parameters())\n",
        "# ae2_params = list(ae2.parameters())\n",
        "# ae3_params = list(ae3.parameters())\n",
        "# pickle.dump((ae1_params, ae2_params, ae3_params), open('/content/drive/My Drive/Ass1Q2_init.sav', 'wb'))\n",
        "\n",
        "(ae1_params, ae2_params, ae3_params) = pickle.load(open('/content/drive/My Drive/Ass1Q2_init.sav', 'rb'))\n",
        "\n",
        "with torch.no_grad():\n",
        "    \n",
        "    classifier.fc1.weight.data = nn.Parameter(ae1_params[0])\n",
        "    classifier.fc1.bias.data = nn.Parameter(ae1_params[1])\n",
        "    \n",
        "    classifier.fc2.weight = nn.Parameter(torch.matmul(ae2_params[0], ae1_params[2]))\n",
        "    classifier.fc2.bias = nn.Parameter(torch.matmul(ae2_params[0], ae1_params[3]) + ae2_params[1])\n",
        "    \n",
        "    classifier.fc3.weight = nn.Parameter(torch.matmul(ae3_params[0], ae2_params[2]))\n",
        "    classifier.fc3.bias = nn.Parameter(torch.matmul(ae3_params[0], ae2_params[3]) + ae3_params[1])\n",
        "    \n",
        "    classifier.fc4.weight = nn.Parameter(ae3_params[2])\n",
        "    classifier.fc4.bias = nn.Parameter(ae3_params[3])"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htcYHkZvb9EX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
        "classifier = classifier.to(device)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGqhu8MUb9Ed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd12ddb8-9c28-478b-8915-39c35a0149d6"
      },
      "source": [
        "old_loss = np.inf\n",
        "\n",
        "max_epoch = 200\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for data in trainloader:\n",
        "        \n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward\n",
        "        y_hat = classifier(X)\n",
        "        \n",
        "        # Calculate Loss (Cross Entropy)\n",
        "        loss = criterion(y_hat, y)\n",
        "        \n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update Parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()*len(X)/train_size\n",
        "    \n",
        "    print('Epoch', epoch+1, ': Loss =', running_loss)\n",
        "    \n",
        "    if abs(running_loss-old_loss)/running_loss < 1e-3:\n",
        "        print('Converged')\n",
        "        break\n",
        "    \n",
        "    old_loss = running_loss\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 : Loss = 1.7094283212314947\n",
            "Epoch 2 : Loss = 0.8871442418206821\n",
            "Epoch 3 : Loss = 0.618906164711172\n",
            "Epoch 4 : Loss = 0.41890863363038405\n",
            "Epoch 5 : Loss = 0.3108258223669096\n",
            "Epoch 6 : Loss = 0.2721154168248176\n",
            "Epoch 7 : Loss = 0.2220215846530416\n",
            "Epoch 8 : Loss = 0.2504797132516449\n",
            "Epoch 9 : Loss = 0.2067782597785647\n",
            "Epoch 10 : Loss = 0.16732645796781243\n",
            "Epoch 11 : Loss = 0.15982529961250047\n",
            "Epoch 12 : Loss = 0.1701343020593578\n",
            "Epoch 13 : Loss = 0.1697550485418601\n",
            "Epoch 14 : Loss = 0.1454118038104339\n",
            "Epoch 15 : Loss = 0.22705053741281678\n",
            "Epoch 16 : Loss = 0.16264148615300658\n",
            "Epoch 17 : Loss = 0.14003813080489638\n",
            "Epoch 18 : Loss = 0.15102741325443444\n",
            "Epoch 19 : Loss = 0.1318457814102823\n",
            "Epoch 20 : Loss = 0.14038881659507754\n",
            "Epoch 21 : Loss = 0.13939036699858576\n",
            "Epoch 22 : Loss = 0.1377106820317832\n",
            "Epoch 23 : Loss = 0.11846282180737366\n",
            "Epoch 24 : Loss = 0.13147930919446732\n",
            "Epoch 25 : Loss = 0.15264644558456808\n",
            "Epoch 26 : Loss = 0.20466907051476568\n",
            "Epoch 27 : Loss = 0.1173877367241816\n",
            "Epoch 28 : Loss = 0.12037531675940212\n",
            "Epoch 29 : Loss = 0.11528817547315896\n",
            "Epoch 30 : Loss = 0.1112499615516175\n",
            "Epoch 31 : Loss = 0.11916739646006713\n",
            "Epoch 32 : Loss = 0.13092972947792572\n",
            "Epoch 33 : Loss = 0.1194868323139169\n",
            "Epoch 34 : Loss = 0.10801943239163271\n",
            "Epoch 35 : Loss = 0.11107321100478822\n",
            "Epoch 36 : Loss = 0.11826297285204584\n",
            "Epoch 37 : Loss = 0.11491750561716885\n",
            "Epoch 38 : Loss = 0.10203228264369749\n",
            "Epoch 39 : Loss = 0.10816259817643599\n",
            "Epoch 40 : Loss = 0.13207424838434567\n",
            "Epoch 41 : Loss = 0.1319264343685725\n",
            "Epoch 42 : Loss = 0.1357143253765323\n",
            "Epoch 43 : Loss = 0.14516897211697968\n",
            "Epoch 44 : Loss = 0.11154064883223992\n",
            "Epoch 45 : Loss = 0.10680983740497722\n",
            "Epoch 46 : Loss = 0.09933548437600787\n",
            "Epoch 47 : Loss = 0.09729652022096245\n",
            "Epoch 48 : Loss = 0.11422437920489092\n",
            "Epoch 49 : Loss = 0.15753573077646166\n",
            "Epoch 50 : Loss = 0.3695269059389829\n",
            "Epoch 51 : Loss = 0.23437131382524964\n",
            "Epoch 52 : Loss = 0.12248560091988608\n",
            "Epoch 53 : Loss = 0.11454178003424945\n",
            "Epoch 54 : Loss = 0.12244952097535132\n",
            "Epoch 55 : Loss = 0.1218135263770819\n",
            "Epoch 56 : Loss = 0.12450057081878187\n",
            "Epoch 57 : Loss = 0.2583880107849837\n",
            "Epoch 58 : Loss = 0.12666873193599962\n",
            "Epoch 59 : Loss = 0.12712356295775284\n",
            "Epoch 60 : Loss = 0.10312363183633846\n",
            "Epoch 61 : Loss = 0.09354188293218613\n",
            "Epoch 62 : Loss = 0.10093827223913238\n",
            "Epoch 63 : Loss = 0.09261503832584081\n",
            "Epoch 64 : Loss = 0.09241190493445504\n",
            "Epoch 65 : Loss = 0.10085860910740767\n",
            "Epoch 66 : Loss = 0.10019657066599892\n",
            "Epoch 67 : Loss = 0.09315507858991622\n",
            "Epoch 68 : Loss = 0.10136880048296672\n",
            "Epoch 69 : Loss = 0.09926859704269608\n",
            "Epoch 70 : Loss = 0.09509922428564595\n",
            "Epoch 71 : Loss = 0.09879792464727706\n",
            "Epoch 72 : Loss = 0.08996569949456236\n",
            "Epoch 73 : Loss = 0.09242618134753272\n",
            "Epoch 74 : Loss = 0.09548870152370495\n",
            "Epoch 75 : Loss = 0.0923201416365125\n",
            "Epoch 76 : Loss = 0.40902778946540574\n",
            "Epoch 77 : Loss = 0.26167085986923083\n",
            "Epoch 78 : Loss = 0.32812094061889424\n",
            "Epoch 79 : Loss = 0.11173054533587264\n",
            "Epoch 80 : Loss = 0.08319089138372379\n",
            "Epoch 81 : Loss = 0.08252035530114718\n",
            "Epoch 82 : Loss = 0.08318714560432867\n",
            "Epoch 83 : Loss = 0.08921891704879023\n",
            "Epoch 84 : Loss = 0.08825613439760426\n",
            "Epoch 85 : Loss = 0.08332169936461883\n",
            "Epoch 86 : Loss = 0.09252964214167812\n",
            "Epoch 87 : Loss = 0.08289161510765553\n",
            "Epoch 88 : Loss = 0.08793167981573127\n",
            "Epoch 89 : Loss = 0.08204320530322463\n",
            "Epoch 90 : Loss = 0.0804590244184841\n",
            "Epoch 91 : Loss = 0.08402807993645017\n",
            "Epoch 92 : Loss = 0.10033476234159687\n",
            "Epoch 93 : Loss = 0.07853287814015694\n",
            "Epoch 94 : Loss = 0.07354091852903366\n",
            "Epoch 95 : Loss = 0.0810380427336151\n",
            "Epoch 96 : Loss = 0.07595224255187945\n",
            "Epoch 97 : Loss = 0.07646230303428389\n",
            "Epoch 98 : Loss = 0.07775311036543411\n",
            "Epoch 99 : Loss = 0.09261188994754446\n",
            "Epoch 100 : Loss = 0.08072442857717925\n",
            "Epoch 101 : Loss = 0.08681454458697278\n",
            "Epoch 102 : Loss = 0.08489119363102046\n",
            "Epoch 103 : Loss = 0.08549081906676287\n",
            "Epoch 104 : Loss = 0.07522970344871284\n",
            "Epoch 105 : Loss = 0.08064793355085632\n",
            "Epoch 106 : Loss = 0.08508725803006781\n",
            "Epoch 107 : Loss = 0.08295420125465501\n",
            "Epoch 108 : Loss = 0.07514140653339302\n",
            "Epoch 109 : Loss = 0.07342398217455907\n",
            "Epoch 110 : Loss = 0.07948259331963278\n",
            "Epoch 111 : Loss = 0.07275837439705027\n",
            "Epoch 112 : Loss = 0.07835700896314599\n",
            "Epoch 113 : Loss = 0.07725206390023232\n",
            "Epoch 114 : Loss = 0.09230641851370981\n",
            "Epoch 115 : Loss = 0.07574848424304614\n",
            "Epoch 116 : Loss = 0.07349650223146785\n",
            "Epoch 117 : Loss = 0.07232072331349956\n",
            "Epoch 118 : Loss = 0.0708476956933737\n",
            "Epoch 119 : Loss = 0.07457314262335951\n",
            "Epoch 120 : Loss = 0.11152646326544609\n",
            "Epoch 121 : Loss = 0.07946918515319173\n",
            "Epoch 122 : Loss = 0.09571462725712493\n",
            "Epoch 123 : Loss = 0.09545092479410495\n",
            "Epoch 124 : Loss = 0.07873940078372305\n",
            "Epoch 125 : Loss = 0.06677418100562962\n",
            "Epoch 126 : Loss = 0.07776334009725937\n",
            "Epoch 127 : Loss = 0.07033791155977681\n",
            "Epoch 128 : Loss = 0.06676643409512258\n",
            "Epoch 129 : Loss = 0.06622572327879342\n",
            "Epoch 130 : Loss = 0.06321955844759942\n",
            "Epoch 131 : Loss = 0.0734474846923893\n",
            "Epoch 132 : Loss = 0.07069402526725423\n",
            "Epoch 133 : Loss = 0.07358007471669804\n",
            "Epoch 134 : Loss = 0.06176482826809992\n",
            "Epoch 135 : Loss = 0.06358695301142604\n",
            "Epoch 136 : Loss = 0.08187396807426757\n",
            "Epoch 137 : Loss = 0.0869749422100457\n",
            "Epoch 138 : Loss = 0.0849907643754374\n",
            "Epoch 139 : Loss = 0.19868114649910815\n",
            "Epoch 140 : Loss = 0.21826874701814217\n",
            "Epoch 141 : Loss = 0.07154053458097305\n",
            "Epoch 142 : Loss = 0.06896549590270626\n",
            "Epoch 143 : Loss = 0.06340016204525123\n",
            "Epoch 144 : Loss = 0.0745491860434413\n",
            "Epoch 145 : Loss = 0.08480951799587769\n",
            "Epoch 146 : Loss = 0.06313683532855727\n",
            "Epoch 147 : Loss = 0.06241557747125627\n",
            "Epoch 148 : Loss = 0.0613096473230557\n",
            "Epoch 149 : Loss = 0.06779881515963511\n",
            "Epoch 150 : Loss = 0.06437399492345074\n",
            "Epoch 151 : Loss = 0.07033149677921424\n",
            "Epoch 152 : Loss = 0.060077697885307396\n",
            "Epoch 153 : Loss = 0.06327012249014595\n",
            "Epoch 154 : Loss = 0.06955793398347769\n",
            "Epoch 155 : Loss = 0.05728013220835816\n",
            "Epoch 156 : Loss = 0.10470915162427859\n",
            "Epoch 157 : Loss = 0.29987101739441807\n",
            "Epoch 158 : Loss = 0.369484678080136\n",
            "Epoch 159 : Loss = 0.19155831245536156\n",
            "Epoch 160 : Loss = 0.09714052148840645\n",
            "Epoch 161 : Loss = 0.16115256822244683\n",
            "Epoch 162 : Loss = 0.08085776509886437\n",
            "Epoch 163 : Loss = 0.07168686677786437\n",
            "Epoch 164 : Loss = 0.06815377360379155\n",
            "Epoch 165 : Loss = 0.08418846011839133\n",
            "Epoch 166 : Loss = 0.06395564363761382\n",
            "Epoch 167 : Loss = 0.06010607460683041\n",
            "Epoch 168 : Loss = 0.06551787037063729\n",
            "Epoch 169 : Loss = 0.05857608670538122\n",
            "Epoch 170 : Loss = 0.057407062331383873\n",
            "Epoch 171 : Loss = 0.054508343338966356\n",
            "Epoch 172 : Loss = 0.06576656160706824\n",
            "Epoch 173 : Loss = 0.05872865478423509\n",
            "Epoch 174 : Loss = 0.057375906119969754\n",
            "Epoch 175 : Loss = 0.057281002063642866\n",
            "Epoch 176 : Loss = 0.05657760367136112\n",
            "Epoch 177 : Loss = 0.060147950510409755\n",
            "Epoch 178 : Loss = 0.0631820910017599\n",
            "Epoch 179 : Loss = 0.06206876547499138\n",
            "Epoch 180 : Loss = 0.05419041555036196\n",
            "Epoch 181 : Loss = 0.05914613994007759\n",
            "Epoch 182 : Loss = 0.05499353276735002\n",
            "Epoch 183 : Loss = 0.058533403768458156\n",
            "Epoch 184 : Loss = 0.06331365614790808\n",
            "Epoch 185 : Loss = 0.05850904824381526\n",
            "Epoch 186 : Loss = 0.05960336488417605\n",
            "Epoch 187 : Loss = 0.06813395675271751\n",
            "Epoch 188 : Loss = 0.056941201910376535\n",
            "Epoch 189 : Loss = 0.05134677421301604\n",
            "Epoch 190 : Loss = 0.05755091009830888\n",
            "Epoch 191 : Loss = 0.054715182970870635\n",
            "Epoch 192 : Loss = 0.05214754271913658\n",
            "Epoch 193 : Loss = 0.056499209254980094\n",
            "Epoch 194 : Loss = 0.056370552121238274\n",
            "Epoch 195 : Loss = 0.05394885274158283\n",
            "Epoch 196 : Loss = 0.057186121459711685\n",
            "Epoch 197 : Loss = 0.06007034102962776\n",
            "Epoch 198 : Loss = 0.05509245141663335\n",
            "Epoch 199 : Loss = 0.05753853798589921\n",
            "Epoch 200 : Loss = 0.04977694162252272\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-5XLFwwMwXd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "c66c660d-6a95-482e-b764-e2b9cc4b492c"
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    train_loss = 0.0\n",
        "    y_train = []\n",
        "    y_train_pred = []\n",
        "\n",
        "    for data in trainloader:\n",
        "\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        y_hat = classifier(X)      \n",
        "        train_loss += criterion(y_hat, y)*len(X)/train_size\n",
        "        \n",
        "        y_train.extend(list(y.detach().cpu().numpy()))\n",
        "        y_train_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
        "\n",
        "print('Train Loss =', train_loss.item())\n",
        "print(pd.DataFrame(confusion_matrix(y_train, y_train_pred)))\n",
        "\n",
        "acc = accuracy_score(y_train, y_train_pred)\n",
        "prec = precision_score(y_train, y_train_pred, average='macro')\n",
        "f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "\n",
        "print('Train Accuracy =', acc, 'Train Precision =', prec, 'Train F1 =', f1)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Loss = 0.04588083177804947\n",
            "     0    1    2    3    4\n",
            "0  265    2    0    3    0\n",
            "1    0  295    0    0    1\n",
            "2    0    0  213    0    0\n",
            "3    1    0    0  282    5\n",
            "4    0    0    0    3  338\n",
            "Train Accuracy = 0.9893465909090909 Train Precision = 0.9902462801942604 Train F1 = 0.989954985752421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVYcIfixb9Eg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "f08c03c4-79b1-4b5e-97cc-31d7d820fb62"
      },
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    test_loss = 0.0\n",
        "    y_test = []\n",
        "    y_test_pred = []\n",
        "\n",
        "    for data in testloader:\n",
        "\n",
        "        X, y = data[0].to(device), data[1].to(device)\n",
        "        y_hat = classifier(X)      \n",
        "        test_loss += criterion(y_hat, y)*len(X)/test_size\n",
        "        \n",
        "        y_test.extend(list(y.detach().cpu().numpy()))\n",
        "        y_test_pred.extend(list(torch.argmax(y_hat, axis=1).detach().cpu().numpy()))\n",
        "\n",
        "print('Test Loss =', test_loss.item())\n",
        "pd.DataFrame(confusion_matrix(y_test, y_test_pred))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss = 2.5016613006591797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16</td>\n",
              "      <td>28</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>31</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>34</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    0   1   2   3   4\n",
              "0  39  28   0  10   9\n",
              "1  16  28   3   9  22\n",
              "2   2   3  31   7   4\n",
              "3   7   4   7  34  20\n",
              "4   3   9   2   6  49"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr5SF1L7Tmvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31922360-3a8a-43bb-f629-8c9b506ea4e9"
      },
      "source": [
        "acc = accuracy_score(y_test, y_test_pred)\n",
        "prec = precision_score(y_test, y_test_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "print('Test Accuracy =', acc, 'Test Precision =', prec, 'Test F1 =', f1)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy = 0.5142045454545454 Test Precision = 0.5356428069982392 Test F1 = 0.5262507510837123\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}